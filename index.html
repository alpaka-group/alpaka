<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>alpaka: **alpaka** - Abstraction Library for Parallel Kernel Acceleration</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="alpaka_doxygen.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">alpaka
   </div>
   <div id="projectbrief">Abstraction Library for Parallel Kernel Acceleration</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('index.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">**alpaka** - Abstraction Library for Parallel Kernel Acceleration </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a href="https://github.com/ComputationalRadiationPhysics/alpaka/actions?query=workflow%3A%22Continuous+Integration%22"></a> <a href="https://travis-ci.org/alpaka-group/alpaka"></a> <a href="https://isocpp.org/"></a> <a href="https://github.com/alpaka-group/alpaka"></a> <a href="https://www.mozilla.org/en-US/MPL/2.0/"></a></p>
<div class="image">
<img src="doc/images/alpaka_401x135.png" alt="Alpaka"/>
</div>
<p>The <b>alpaka</b> library is a header-only C++14 abstraction library for accelerator development.</p>
<p>Its aim is to provide performance portability across accelerators through the abstraction (not hiding!) of the underlying levels of parallelism.</p>
<p>It is platform independent and supports the concurrent and cooperative use of multiple devices such as the hosts CPU as well as attached accelerators as for instance CUDA GPUs and Xeon Phis (currently native execution only). A multitude of accelerator back-end variants using CUDA, OpenMP (2.0/4.0), Boost.Fiber, std::thread and also serial execution is provided and can be selected depending on the device. Only one implementation of the user kernel is required by representing them as function objects with a special interface. There is no need to write special CUDA, OpenMP or custom threading code. Accelerator back-ends can be mixed within a device queue. The decision which accelerator back-end executes which kernel can be made at runtime.</p>
<p>The abstraction used is very similar to the CUDA grid-blocks-threads division strategy. Algorithms that should be parallelized have to be divided into a multi-dimensional grid consisting of small uniform work items. These functions are called kernels and are executed in parallel threads. The threads in the grid are organized in blocks. All threads in a block are executed in parallel and can interact via fast shared memory. Blocks are executed independently and can not interact in any way. The block execution order is unspecified and depends on the accelerator in use. By using this abstraction the execution can be optimally adapted to the available hardware.</p>
<h2>Software License </h2>
<p><b>alpaka</b> is licensed under <b>MPL-2.0</b>.</p>
<h2>Documentation </h2>
<p>The general documentation is located within the <code>doc/markdown</code> subfolder of the repository. The <a href="http://alpaka-group.github.io/alpaka/">source code documentation</a> is generated with <a href="http://www.doxygen.org">doxygen</a>.</p>
<h2>Accelerator Back-ends </h2>
<table class="doxtable">
<tr>
<th>Accelerator Back-end</th><th>Lib/API</th><th>Devices</th><th>Execution strategy grid-blocks</th><th>Execution strategy block-threads  </th></tr>
<tr>
<td>Serial</td><td>n/a</td><td>Host CPU (single core)</td><td>sequential</td><td>sequential (only 1 thread per block) </td></tr>
<tr>
<td>OpenMP 2.0+ blocks</td><td>OpenMP 2.0+</td><td>Host CPU (multi core)</td><td>parallel (preemptive multitasking)</td><td>sequential (only 1 thread per block) </td></tr>
<tr>
<td>OpenMP 2.0+ threads</td><td>OpenMP 2.0+</td><td>Host CPU (multi core)</td><td>sequential</td><td>parallel (preemptive multitasking) </td></tr>
<tr>
<td>OpenMP 4.0+ (CPU)</td><td>OpenMP 4.0+</td><td>Host CPU (multi core)</td><td>parallel (undefined)</td><td>parallel (preemptive multitasking) </td></tr>
<tr>
<td>std::thread </td><td>std::thread </td><td>Host CPU (multi core)</td><td>sequential</td><td>parallel (preemptive multitasking) </td></tr>
<tr>
<td>Boost.Fiber </td><td>boost::fibers::fiber </td><td>Host CPU (single core)</td><td>sequential</td><td>parallel (cooperative multitasking) </td></tr>
<tr>
<td>TBB</td><td>TBB 2.2+</td><td>Host CPU (multi core)</td><td>parallel (preemptive multitasking)</td><td>sequential (only 1 thread per block) </td></tr>
<tr>
<td>CUDA</td><td>CUDA 9.0-10.2</td><td>NVIDIA GPUs</td><td>parallel (undefined)</td><td>parallel (lock-step within warps) </td></tr>
<tr>
<td>HIP(nvcc)</td><td><a href="https://github.com/ROCm-Developer-Tools/HIP">HIP 3.1+</a></td><td>NVIDIA GPUs SM 2.0+</td><td>parallel (undefined)</td><td>parallel (lock-step within warps) </td></tr>
</table>
<h2>Supported Compilers </h2>
<p>This library uses C++14 (or newer when available).</p>
<table class="doxtable">
<tr>
<th>Accelerator Back-end</th><th>gcc 5.5 <br />
 (Linux)</th><th>gcc 6.4/7.3 <br />
 (Linux)</th><th>gcc 8.1 <br />
 (Linux)</th><th>gcc 9.1 <br />
 (Linux)</th><th>clang 4 <br />
 (Linux)</th><th>clang 5 <br />
 (Linux)</th><th>clang 6 <br />
 (Linux)</th><th>clang 7 <br />
 (Linux)</th><th>clang 8 <br />
 (Linux)</th><th>clang 9 <br />
 (Linux)</th><th>Apple LLVM 11.0-11.4 <br />
 (macOS)</th><th>MSVC 2017/2019 <br />
 (Windows)  </th></tr>
<tr>
<td>Serial</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark: </td></tr>
<tr>
<td>OpenMP 2.0+ blocks</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:x:</td><td>:white_check_mark: </td></tr>
<tr>
<td>OpenMP 2.0+ threads</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:x:</td><td>:white_check_mark: </td></tr>
<tr>
<td>OpenMP 4.0+ (CPU)</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:x:</td><td>:x: </td></tr>
<tr>
<td>std::thread </td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark: </td></tr>
<tr>
<td>Boost.Fiber </td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:x:</td><td>:white_check_mark: </td></tr>
<tr>
<td>TBB</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark:</td><td>:white_check_mark: </td></tr>
<tr>
<td>CUDA (nvcc)</td><td>:white_check_mark: <br />
 (CUDA 9.0-10.2)</td><td>:white_check_mark: <br />
 (CUDA 9.2-10.2) </td><td>:white_check_mark: <br />
 (CUDA 10.1-10.2) </td><td>:x:</td><td>:white_check_mark: <br />
 (CUDA 9.1-10.2)</td><td>:white_check_mark: <br />
 (CUDA 10.1-10.2)</td><td>:white_check_mark: <br />
 (CUDA 10.1-10.2)</td><td>:white_check_mark: <br />
 (CUDA 10.1-10.2)</td><td>:white_check_mark: <br />
 (CUDA 10.1-10.2)</td><td>:x:</td><td>:x:</td><td>:white_check_mark: <br />
 (CUDA 10.0-10.2) </td></tr>
<tr>
<td>CUDA (clang) </td><td>- </td><td>- </td><td>- </td><td>- </td><td>- </td><td>- </td><td>:white_check_mark: <br />
 (CUDA 9.0) </td><td>:white_check_mark: <br />
 (CUDA 9.0-9.2) </td><td>:white_check_mark: <br />
 (CUDA 9.0-10.0) </td><td>:white_check_mark: <br />
 (CUDA 9.2-10.1) </td><td>- </td><td>- </td></tr>
<tr>
<td>HIP (nvcc)</td><td>:white_check_mark: <br />
 (nvcc 9.0+)</td><td>:x:</td><td>:x:</td><td>:x:</td><td>:x:</td><td>:x:</td><td>:x:</td><td>:x:</td><td>:x:</td><td>:x:</td><td>:x:</td><td>:x: </td></tr>
</table>
<p>Other compilers or combinations marked with :x: in the table above may work but are not tested in CI and are therefore not explicitly supported.</p>
<h2>Dependencies </h2>
<p><a href="https://boost.org/">Boost</a> 1.65.1+ is the only mandatory external dependency. The <b>alpaka</b> library itself just requires header-only libraries. However some of the accelerator back-end implementations require different boost libraries to be built.</p>
<p>When an accelerator back-end using <em>Boost.Fiber</em> is enabled, <code>boost-fiber</code> and all of its dependencies are required to be built in C++11 mode <code>./b2 cxxflags="-std=c++11"</code>. When <em>Boost.Fiber</em> is enabled and alpaka is built in C++17 mode with clang and libstc++, Boost &gt;= 1.67.0 is required.</p>
<p>When an accelerator back-end using <em>CUDA</em> is enabled, version <em>9.0</em> of the <em>CUDA SDK</em> is the minimum requirement. <em>NOTE</em>: When using nvcc as <em>CUDA</em> compiler, the <em>CUDA accelerator back-end</em> can not be enabled together with the <em>Boost.Fiber accelerator back-end</em> due to bugs in the nvcc compiler. <em>NOTE</em>: When using clang as a native <em>CUDA</em> compiler, the <em>CUDA accelerator back-end</em> can not be enabled together with any <em>OpenMP accelerator back-end</em> because this combination is currently unsupported. <em>NOTE</em>: Separable compilation is only supported when using nvcc, not with clang as native <em>CUDA</em> compiler. It is disabled by default and can be enabled via the CMake flag <code>ALPAKA_CUDA_NVCC_SEPARABLE_COMPILATION</code>.</p>
<p>When an accelerator back-end using <em>OpenMP</em> is enabled, the compiler and the platform have to support the corresponding minimum <em>OpenMP</em> version.</p>
<p>When an accelerator back-end using <em>TBB</em> is enabled, the compiler and the platform have to support the corresponding minimum <em>TBB</em> version.</p>
<h2>Usage </h2>
<p>The library is header only so nothing has to be built. CMake 3.15+ is required to provide the correct defines and include paths. Just call <code>ALPAKA_ADD_EXECUTABLE</code> instead of <code>CUDA_ADD_EXECUTABLE</code> or <code>ADD_EXECUTABLE</code> and the difficulties of the CUDA nvcc compiler in handling <code>.cu</code> and <code>.cpp</code> files are automatically taken care of. Source files do not need any special file ending. Examples of how to utilize alpaka within CMake can be found in the <code>example</code> folder.</p>
<p>The whole alpaka library can be included with: <code>#include &lt;<a class="el" href="alpaka_8hpp.html">alpaka/alpaka.hpp</a>&gt;</code> Code that is not intended to be utilized by the user is hidden in the <code>detail</code> namespace.</p>
<p>Furthermore, for a CUDA-like experience when adopting alpaka we provide the library <a href="https://github.com/alpaka-group/cupla"><em>cupla</em></a>. It enables a simple and straightforward way of porting existing CUDA applications to alpaka and thus to a variety of accelerators.</p>
<h2>Introduction </h2>
<p>For a quick introduction, feel free to playback the recording of our presentation at <a href="http://mygtc.gputechconf.com/quicklink/858sI36">GTC 2016</a>:</p>
<ul>
<li>E. Zenker, R. Widera, G. Juckeland et al., <em>Porting the Plasma Simulation PIConGPU to Heterogeneous Architectures with Alpaka</em>, <a href="http://on-demand.gputechconf.com/gtc/2016/video/S6298.html">video link (39 min)</a></li>
</ul>
<h2>Citing alpaka </h2>
<p>Currently all authors of <b>alpaka</b> are scientists or connected with research. For us to justify the importance and impact of our work, please consider citing us accordingly in your derived work and publications:</p>
<div class="fragment"><div class="line">% Peer-Reviewed Publication %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</div><div class="line">%</div><div class="line">% Peer reviewed and accepted publication in</div><div class="line">%   &quot;2nd International Workshop on Performance Portable</div><div class="line">%    Programming Models for Accelerators (P^3MA)&quot;</div><div class="line">% colocated with the</div><div class="line">%   &quot;2017 ISC High Performance Conference&quot;</div><div class="line">%   in Frankfurt, Germany</div><div class="line">@inproceedings{MathesP3MA2017,</div><div class="line">  author    = {{Matthes}, A. and {Widera}, R. and {Zenker}, E. and {Worpitz}, B. and</div><div class="line">               {Huebl}, A. and {Bussmann}, M.},</div><div class="line">  title     = {Tuning and optimization for a variety of many-core architectures without changing a single line of implementation code</div><div class="line">               using the Alpaka library},</div><div class="line">  archivePrefix = &quot;arXiv&quot;,</div><div class="line">  eprint    = {1706.10086},</div><div class="line">  keywords  = {Computer Science - Distributed, Parallel, and Cluster Computing},</div><div class="line">  day       = {30},</div><div class="line">  month     = {Jun},</div><div class="line">  year      = {2017},</div><div class="line">  url       = {https://arxiv.org/abs/1706.10086},</div><div class="line">}</div><div class="line"></div><div class="line">% Peer-Reviewed Publication %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</div><div class="line">%</div><div class="line">% Peer reviewed and accepted publication in</div><div class="line">%   &quot;The Sixth International Workshop on</div><div class="line">%    Accelerators and Hybrid Exascale Systems (AsHES)&quot;</div><div class="line">% at the</div><div class="line">%   &quot;30th IEEE International Parallel and Distributed</div><div class="line">%    Processing Symposium&quot; in Chicago, IL, USA</div><div class="line">@inproceedings{ZenkerAsHES2016,</div><div class="line">  author    = {Erik Zenker and Benjamin Worpitz and Ren{\&#39;{e}} Widera</div><div class="line">               and Axel Huebl and Guido Juckeland and</div><div class="line">               Andreas Kn{\&quot;{u}}pfer and Wolfgang E. Nagel and Michael Bussmann},</div><div class="line">  title     = {Alpaka - An Abstraction Library for Parallel Kernel Acceleration},</div><div class="line">  archivePrefix = &quot;arXiv&quot;,</div><div class="line">  eprint    = {1602.08477},</div><div class="line">  keywords  = {Computer science;CUDA;Mathematical Software;nVidia;OpenMP;Package;</div><div class="line">               performance portability;Portability;Tesla K20;Tesla K80},</div><div class="line">  day       = {23},</div><div class="line">  month     = {May},</div><div class="line">  year      = {2016},</div><div class="line">  publisher = {IEEE Computer Society},</div><div class="line">  url       = {http://arxiv.org/abs/1602.08477},</div><div class="line">}</div><div class="line"></div><div class="line"></div><div class="line">% Original Work: Benjamin Worpitz&#39; Master Thesis %%%%%%%%%%</div><div class="line">%</div><div class="line">@MasterThesis{Worpitz2015,</div><div class="line">  author = {Benjamin Worpitz},</div><div class="line">  title  = {Investigating performance portability of a highly scalable</div><div class="line">            particle-in-cell simulation code on various multi-core</div><div class="line">            architectures},</div><div class="line">  school = {{Technische Universit{\&quot;{a}}t Dresden}},</div><div class="line">  month  = {Sep},</div><div class="line">  year   = {2015},</div><div class="line">  type   = {Master Thesis},</div><div class="line">  doi    = {10.5281/zenodo.49768},</div><div class="line">  url    = {http://dx.doi.org/10.5281/zenodo.49768}</div><div class="line">}</div></div><!-- fragment --><h2>Authors </h2>
<h3>Maintainers and Core Developers</h3>
<ul>
<li>Benjamin Worpitz (original author)</li>
<li>Rene Widera</li>
</ul>
<h3>Former Members, Contributions and Thanks</h3>
<ul>
<li>Dr. Michael Bussmann</li>
<li>Axel Huebl</li>
<li>Erik Zenker </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Mon May 11 2020 13:20:58 for alpaka by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
