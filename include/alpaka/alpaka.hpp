#pragma once

// ============================================================================
// == ./include/alpaka/alpaka.hpp ==
// ==
/* Copyright 2023 Axel Hübl, Benjamin Worpitz, Erik Zenker, Matthias Werner, René Widera, Bernhard Manfred Gruber,
 *                Jan Stephan, Antonio Di Pilato
 * SPDX-License-Identifier: MPL-2.0
 */

// #pragma once
// Include the whole library.

// version number
	// ============================================================================
	// == ./include/alpaka/version.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Erik Zenker, Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	#include <boost/predef/version_number.h>

	#define ALPAKA_VERSION_MAJOR 1
	#define ALPAKA_VERSION_MINOR 0
	#define ALPAKA_VERSION_PATCH 0

	//! The alpaka library version number
	#define ALPAKA_VERSION BOOST_VERSION_NUMBER(ALPAKA_VERSION_MAJOR, ALPAKA_VERSION_MINOR, ALPAKA_VERSION_PATCH)
	// ==
	// == ./include/alpaka/version.hpp ==
	// ============================================================================

// acc
	// ============================================================================
	// == ./include/alpaka/acc/AccCpuOmp2Blocks.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, René Widera, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Base classes.
		// ============================================================================
		// == ./include/alpaka/atomic/AtomicCpu.hpp ==
		// ==
		/* Copyright 2021 Andrea Bocci, Felice Pantaleo
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		#include <boost/version.hpp>

		#ifndef ALPAKA_DISABLE_ATOMIC_ATOMICREF
		#    define ALPAKA_DISABLE_ATOMIC_ATOMICREF
		#endif

			// ============================================================================
			// == ./include/alpaka/atomic/AtomicAtomicRef.hpp ==
			// ==
			/* Copyright 2022 Felice Pantaleo, Andrea Bocci, Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
				// ============================================================================
				// == ./include/alpaka/atomic/Traits.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, René Widera, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
					// ============================================================================
					// == ./include/alpaka/atomic/Op.hpp ==
					// ==
					/* Copyright 2020 Benjamin Worpitz, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
						// ============================================================================
						// == ./include/alpaka/core/BoostPredef.hpp ==
						// ==
						/* Copyright 2022 Benjamin Worpitz, Matthias Werner, Jan Stephan
						 * SPDX-License-Identifier: MPL-2.0
						 */

						// #pragma once
						#include <boost/predef.h>

						#ifdef __INTEL_COMPILER
						#    warning                                                                                                          \
						        "The Intel Classic compiler (icpc) is no longer supported. Please upgrade to the Intel LLVM compiler (ipcx)."
						#endif

						//---------------------------------------HIP-----------------------------------
						// __HIPCC__ is defined by hipcc (if either __CUDACC__ is defined)
						// https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_porting_guide.md#compiler-defines-summary
						#if !defined(BOOST_LANG_HIP)
						#    if defined(__HIPCC__) && (defined(__CUDACC__) || defined(__HIP__))
						#        include <hip/hip_runtime.h>
						// HIP defines "abort()" as "{asm("trap;");}", which breaks some kernels
						#        undef abort
						#        define BOOST_LANG_HIP BOOST_VERSION_NUMBER(HIP_VERSION_MAJOR, HIP_VERSION_MINOR, 0)
						#        if defined(BOOST_LANG_CUDA) && BOOST_LANG_CUDA
						#            undef BOOST_LANG_CUDA
						#            define BOOST_LANG_CUDA BOOST_VERSION_NUMBER_NOT_AVAILABLE
						#        endif
						#    else
						#        define BOOST_LANG_HIP BOOST_VERSION_NUMBER_NOT_AVAILABLE
						#    endif
						#endif

						// HSA device architecture detection (HSA generated via HIP(clang))
						#if !defined(BOOST_ARCH_HSA)
						#    if defined(__HIP_DEVICE_COMPILE__) && __HIP_DEVICE_COMPILE__ == 1 && defined(__HIP__)
						// __HIP_DEVICE_COMPILE__ does not represent feature capability of target device like CUDA_ARCH.
						// For feature detection there are special macros, see ROCm's HIP porting guide.
						#        define BOOST_ARCH_HSA BOOST_VERSION_NUMBER_AVAILABLE
						#    else
						#        define BOOST_ARCH_HSA BOOST_VERSION_NUMBER_NOT_AVAILABLE
						#    endif
						#endif

						// hip compiler detection
						#if !defined(BOOST_COMP_HIP)
						#    if defined(__HIP__)
						#        define BOOST_COMP_HIP BOOST_VERSION_NUMBER_AVAILABLE
						#    else
						#        define BOOST_COMP_HIP BOOST_VERSION_NUMBER_NOT_AVAILABLE
						#    endif
						#endif

						// clang CUDA compiler detection
						// Currently __CUDA__ is only defined by clang when compiling CUDA code.
						#if defined(__clang__) && defined(__CUDA__)
						#    define BOOST_COMP_CLANG_CUDA BOOST_COMP_CLANG
						#else
						#    define BOOST_COMP_CLANG_CUDA BOOST_VERSION_NUMBER_NOT_AVAILABLE
						#endif

						// PGI and NV HPC SDK compiler detection
						// As of Boost 1.74, Boost.Predef's compiler detection is a bit weird. Recent PGI compilers will be identified as
						// BOOST_COMP_PGI_EMULATED. Boost.Predef has lackluster front-end support and mistakes the EDG front-end
						// for an actual compiler.
						// TODO: Whenever you look at this code please check whether https://github.com/boostorg/predef/issues/28 and
						// https://github.com/boostorg/predef/issues/51 have been resolved.
						// BOOST_COMP_PGI_EMULATED is defined by boost instead of BOOST_COMP_PGI
						#if defined(BOOST_COMP_PGI) && defined(BOOST_COMP_PGI_EMULATED)
						#    undef BOOST_COMP_PGI
						#    define BOOST_COMP_PGI BOOST_COMP_PGI_EMULATED
						#endif
						// ==
						// == ./include/alpaka/core/BoostPredef.hpp ==
						// ============================================================================

						// ============================================================================
						// == ./include/alpaka/core/Common.hpp ==
						// ==
						/* Copyright 2019 Axel Huebl, Benjamin Worpitz, Matthias Werner
						 * SPDX-License-Identifier: MPL-2.0
						 */

						// #pragma once
						// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
							// ============================================================================
							// == ./include/alpaka/core/Debug.hpp ==
							// ==
							/* Copyright 2022 Alexander Matthes, Benjamin Worpitz, Bernhard Manfred Gruber
							 * SPDX-License-Identifier: MPL-2.0
							 */

							// #pragma once
							// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

							#include <iostream>
							#include <string>
							#include <utility>

							//! The no debug level.
							#define ALPAKA_DEBUG_DISABLED 0
							//! The minimal debug level.
							#define ALPAKA_DEBUG_MINIMAL 1
							//! The full debug level.
							#define ALPAKA_DEBUG_FULL 2

							#ifndef ALPAKA_DEBUG
							//! Set the minimum log level if it is not defined.
							#    define ALPAKA_DEBUG ALPAKA_DEBUG_DISABLED
							#endif

							namespace alpaka::core::detail
							{
							    //! Scope logger.
							    class ScopeLogStdOut final
							    {
							    public:
							        explicit ScopeLogStdOut(std::string sScope) : m_sScope(std::move(sScope))
							        {
							            std::cout << "[+] " << m_sScope << std::endl;
							        }
							        ScopeLogStdOut(ScopeLogStdOut const&) = delete;
							        ScopeLogStdOut(ScopeLogStdOut&&) = delete;
							        auto operator=(ScopeLogStdOut const&) -> ScopeLogStdOut& = delete;
							        auto operator=(ScopeLogStdOut&&) -> ScopeLogStdOut& = delete;
							        ~ScopeLogStdOut()
							        {
							            std::cout << "[-] " << m_sScope << std::endl;
							        }

							    private:
							        std::string const m_sScope;
							    };
							} // namespace alpaka::core::detail

							// Define ALPAKA_DEBUG_MINIMAL_LOG_SCOPE.
							#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
							#    define ALPAKA_DEBUG_MINIMAL_LOG_SCOPE ::alpaka::core::detail::ScopeLogStdOut const scopeLogStdOut(__func__)
							#else
							#    define ALPAKA_DEBUG_MINIMAL_LOG_SCOPE
							#endif

							// Define ALPAKA_DEBUG_FULL_LOG_SCOPE.
							#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
							#    define ALPAKA_DEBUG_FULL_LOG_SCOPE ::alpaka::core::detail::ScopeLogStdOut const scopeLogStdOut(__func__)
							#else
							#    define ALPAKA_DEBUG_FULL_LOG_SCOPE
							#endif

							// Define ALPAKA_DEBUG_BREAK.
							#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
							#    if BOOST_COMP_GNUC || BOOST_COMP_CLANG
							#        define ALPAKA_DEBUG_BREAK ::__builtin_trap()
							#    elif BOOST_COMP_MSVC
							#        define ALPAKA_DEBUG_BREAK ::__debugbreak()
							#    else
							#        define ALPAKA_DEBUG_BREAK
							  //#error debug-break for current compiler not implemented!
							#    endif
							#else
							#    define ALPAKA_DEBUG_BREAK
							#endif
							// ==
							// == ./include/alpaka/core/Debug.hpp ==
							// ============================================================================


						// Boost.Uuid errors with VS2017 when intrin.h is not included
						#if defined(_MSC_VER) && _MSC_VER >= 1910
						#    include <intrin.h>
						#endif

						//! All functions that can be used on an accelerator have to be attributed with ALPAKA_FN_ACC or ALPAKA_FN_HOST_ACC.
						//!
						//! Usage:
						//! ALPAKA_FN_ACC
						//! auto add(std::int32_t a, std::int32_t b)
						//! -> std::int32_t;
						#if BOOST_LANG_CUDA || BOOST_LANG_HIP
						#    if defined(ALPAKA_ACC_GPU_CUDA_ONLY_MODE) || defined(ALPAKA_ACC_GPU_HIP_ONLY_MODE)
						#        define ALPAKA_FN_ACC __device__
						#    else
						#        define ALPAKA_FN_ACC __device__ __host__
						#    endif
						#    define ALPAKA_FN_HOST_ACC __device__ __host__
						#    define ALPAKA_FN_HOST __host__
						#else
						#    define ALPAKA_FN_ACC
						#    define ALPAKA_FN_HOST_ACC
						#    define ALPAKA_FN_HOST
						#endif

						//! All functions marked with ALPAKA_FN_ACC or ALPAKA_FN_HOST_ACC that are exported to / imported from different
						//! translation units have to be attributed with ALPAKA_FN_EXTERN. Note that this needs to be applied to both the
						//! declaration and the definition.
						//!
						//! Usage:
						//! ALPAKA_FN_ACC ALPAKA_FN_EXTERN auto add(std::int32_t a, std::int32_t b) -> std::int32_t;
						//!
						//! Warning: If this is used together with the SYCL back-end make sure that your SYCL runtime supports generic
						//! address spaces. Otherwise it is forbidden to use pointers as parameter or return type for functions marked
						//! with ALPAKA_FN_EXTERN.
						#ifdef ALPAKA_ACC_SYCL_ENABLED
						/*
						   This is required by the SYCL standard, section 5.10.1 "SYCL functions and member functions linkage":

						   The default behavior in SYCL applications is that all the definitions and declarations of the functions and member
						   functions are available to the SYCL compiler, in the same translation unit. When this is not the case, all the
						   symbols that need to be exported to a SYCL library or from a C++ library to a SYCL application need to be defined
						   using the macro: SYCL_EXTERNAL.
						*/
						#    define ALPAKA_FN_EXTERN SYCL_EXTERNAL
						#else
						#    define ALPAKA_FN_EXTERN
						#endif

						//! Disable nvcc warning:
						//! 'calling a __host__ function from __host__ __device__ function.'
						//! Usage:
						//! ALPAKA_NO_HOST_ACC_WARNING
						//! ALPAKA_FN_HOST_ACC function_declaration()
						//! WARNING: Only use this method if there is no other way.
						//! Most cases can be solved by #if BOOST_ARCH_PTX or #if BOOST_LANG_CUDA.
						#if(BOOST_LANG_CUDA && !BOOST_COMP_CLANG_CUDA)
						#    if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
						#        define ALPAKA_NO_HOST_ACC_WARNING __pragma(hd_warning_disable)
						#    else
						#        define ALPAKA_NO_HOST_ACC_WARNING _Pragma("hd_warning_disable")
						#    endif
						#else
						#    define ALPAKA_NO_HOST_ACC_WARNING
						#endif

						//! Macro defining the inline function attribute.
						#if BOOST_LANG_CUDA || BOOST_LANG_HIP
						#    define ALPAKA_FN_INLINE __forceinline__
						#else
						#    if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
						// TODO: With C++20 [[msvc::forceinline]] can be used.
						#        define ALPAKA_FN_INLINE __forceinline
						#    else
						#        define ALPAKA_FN_INLINE [[gnu::always_inline]] inline
						#    endif
						#endif

						//! This macro defines a variable lying in global accelerator device memory.
						//!
						//! Example:
						//!   ALPAKA_STATIC_ACC_MEM_GLOBAL int i;
						//!
						//! Those variables behave like ordinary variables when used in file-scope.
						//! They have external linkage (are accessible from other compilation units).
						//! If you want to access it from a different compilation unit, you have to declare it as extern:
						//!   extern ALPAKA_STATIC_ACC_MEM_GLOBAL int i;
						//! Like ordinary variables, only one definition is allowed (ODR)
						//! Failure to do so might lead to linker errors.
						//!
						//! In contrast to ordinary variables, you can not define such variables
						//! as static compilation unit local variables with internal linkage
						//! because this is forbidden by CUDA.
						//!
						//! \attention It is not allowed to initialize the variable together with the declaration.
						//!            To initialize the variable alpaka::createStaticDevMemView and alpaka::memcpy must be used.
						//! \code{.cpp}
						//! ALPAKA_STATIC_ACC_MEM_GLOBAL int foo;
						//!
						//! void initFoo() {
						//!     auto extent = alpaka::Vec<alpaka::DimInt<1u>, size_t>{1};
						//!     auto viewFoo = alpaka::createStaticDevMemView(&foo, device, extent);
						//!     int initialValue = 42;
						//!     alpaka::ViewPlainPtr<DevHost, int, alpaka::DimInt<1u>, size_t> bufHost(&initialValue, devHost, extent);
						//!     alpaka::memcpy(queue, viewGlobalMemUninitialized, bufHost, extent);
						//! }
						//! \endcode
						#if((BOOST_LANG_CUDA && BOOST_COMP_CLANG_CUDA) || (BOOST_LANG_CUDA && BOOST_COMP_NVCC && BOOST_ARCH_PTX)              \
						    || BOOST_LANG_HIP)
						#    define ALPAKA_STATIC_ACC_MEM_GLOBAL __device__
						#else
						#    define ALPAKA_STATIC_ACC_MEM_GLOBAL
						#endif

						//! This macro defines a variable lying in constant accelerator device memory.
						//!
						//! Example:
						//!   ALPAKA_STATIC_ACC_MEM_CONSTANT int i;
						//!
						//! Those variables behave like ordinary variables when used in file-scope.
						//! They have external linkage (are accessible from other compilation units).
						//! If you want to access it from a different compilation unit, you have to declare it as extern:
						//!   extern ALPAKA_STATIC_ACC_MEM_CONSTANT int i;
						//! Like ordinary variables, only one definition is allowed (ODR)
						//! Failure to do so might lead to linker errors.
						//!
						//! In contrast to ordinary variables, you can not define such variables
						//! as static compilation unit local variables with internal linkage
						//! because this is forbidden by CUDA.
						//!
						//! \attention It is not allowed to initialize the variable together with the declaration.
						//!            To initialize the variable alpaka::createStaticDevMemView and alpaka::memcpy must be used.
						//! \code{.cpp}
						//! ALPAKA_STATIC_ACC_MEM_CONSTANT int foo;
						//!
						//! void initFoo() {
						//!     auto extent = alpaka::Vec<alpaka::DimInt<1u>, size_t>{1};
						//!     auto viewFoo = alpaka::createStaticDevMemView(&foo, device, extent);
						//!     int initialValue = 42;
						//!     alpaka::ViewPlainPtr<DevHost, int, alpaka::DimInt<1u>, size_t> bufHost(&initialValue, devHost, extent);
						//!     alpaka::memcpy(queue, viewGlobalMemUninitialized, bufHost, extent);
						//! }
						//! \endcode
						#if((BOOST_LANG_CUDA && BOOST_COMP_CLANG_CUDA) || (BOOST_LANG_CUDA && BOOST_COMP_NVCC && BOOST_ARCH_PTX)              \
						    || BOOST_LANG_HIP)
						#    define ALPAKA_STATIC_ACC_MEM_CONSTANT __constant__
						#else
						#    define ALPAKA_STATIC_ACC_MEM_CONSTANT
						#endif

						//! This macro disables memory optimizations for annotated device memory.
						//!
						//! Example:
						//!   ALPAKA_DEVICE_VOLATILE float* ptr;
						//!
						//! This is useful for pointers, (shared) variables and shared memory which are used in combination with
						//! the alpaka::mem_fence() function. It ensures that memory annotated with this macro will always be written directly
						//! to memory (and not to a register or cache because of compiler optimizations).
						#if(BOOST_LANG_CUDA && BOOST_ARCH_PTX)                                                                                \
						    || (BOOST_LANG_HIP && defined(__HIP_DEVICE_COMPILE__) && __HIP_DEVICE_COMPILE__ == 1)
						#    define ALPAKA_DEVICE_VOLATILE volatile
						#else
						#    define ALPAKA_DEVICE_VOLATILE
						#endif
						// ==
						// == ./include/alpaka/core/Common.hpp ==
						// ============================================================================


					#include <algorithm>
					#include <type_traits>


					namespace alpaka
					{
					    //! The addition function object.
					    struct AtomicAdd
					    {
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					#if BOOST_COMP_GNUC
					#    pragma GCC diagnostic push
					#    pragma GCC diagnostic ignored "-Wconversion"
					#endif
					            ref += value;
					            return old;
					#if BOOST_COMP_GNUC
					#    pragma GCC diagnostic pop
					#endif
					        }
					    };
					    //! The subtraction function object.
					    struct AtomicSub
					    {
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					#if BOOST_COMP_GNUC
					#    pragma GCC diagnostic push
					#    pragma GCC diagnostic ignored "-Wconversion"
					#endif
					            ref -= value;
					#if BOOST_COMP_GNUC
					#    pragma GCC diagnostic pop
					#endif
					            return old;
					        }
					    };
					    //! The minimum function object.
					    struct AtomicMin
					    {
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					            ref = std::min(ref, value);
					            return old;
					        }
					    };
					    //! The maximum function object.
					    struct AtomicMax
					    {
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					            ref = std::max(ref, value);
					            return old;
					        }
					    };
					    //! The exchange function object.
					    struct AtomicExch
					    {
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					            ref = value;
					            return old;
					        }
					    };
					    //! The increment function object.
					    struct AtomicInc
					    {
					        //! Increments up to value, then reset to 0.
					        //!
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					            ref = ((old >= value) ? static_cast<T>(0) : static_cast<T>(old + static_cast<T>(1)));
					            return old;
					        }
					    };
					    //! The decrement function object.
					    struct AtomicDec
					    {
					        //! Decrement down to 0, then reset to value.
					        //!
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					            ref = (((old == static_cast<T>(0)) || (old > value)) ? value : static_cast<T>(old - static_cast<T>(1)));
					            return old;
					        }
					    };
					    //! The and function object.
					    struct AtomicAnd
					    {
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					            ref &= value;
					            return old;
					        }
					    };
					    //! The or function object.
					    struct AtomicOr
					    {
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					            ref |= value;
					            return old;
					        }
					    };
					    //! The exclusive or function object.
					    struct AtomicXor
					    {
					        //! \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto operator()(T* const addr, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;
					            ref ^= value;
					            return old;
					        }
					    };
					    //! The compare and swap function object.
					    struct AtomicCas
					    {
					        //! AtomicCas for non floating point values
					        // \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T, std::enable_if_t<!std::is_floating_point_v<T>, bool> = true>
					        ALPAKA_FN_HOST_ACC auto operator()(T* addr, T const& compare, T const& value) const -> T
					        {
					            auto const old = *addr;
					            auto& ref = *addr;

					// gcc-7.4.0 assumes for an optimization that a signed overflow does not occur here.
					// That's fine, so ignore that warning.
					#if BOOST_COMP_GNUC && (BOOST_COMP_GNUC == BOOST_VERSION_NUMBER(7, 4, 0))
					#    pragma GCC diagnostic push
					#    pragma GCC diagnostic ignored "-Wstrict-overflow"
					#endif
					            // check if values are bit-wise equal
					            ref = ((old == compare) ? value : old);
					#if BOOST_COMP_GNUC && (BOOST_COMP_GNUC == BOOST_VERSION_NUMBER(7, 4, 0))
					#    pragma GCC diagnostic pop
					#endif
					            return old;
					        }
					        //! AtomicCas for floating point values
					        // \return The old value of addr.
					        ALPAKA_NO_HOST_ACC_WARNING
					        template<typename T, std::enable_if_t<std::is_floating_point_v<T>, bool> = true>
					        ALPAKA_FN_HOST_ACC auto operator()(T* addr, T const& compare, T const& value) const -> T
					        {
					            static_assert(sizeof(T) == 4u || sizeof(T) == 8u, "AtomicCas is supporting only 32bit and 64bit values!");
					            // Type to reinterpret too to perform the bit comparison
					            using BitType = std::conditional_t<sizeof(T) == 4u, unsigned int, unsigned long long>;

					            // type used to have a safe way to reinterprete the data into another type
					            // std::variant can not be used because clang8 has issues to compile std::variant
					            struct BitUnion
					            {
					                union
					                {
					                    T value;
					                    BitType r;
					                };
					            };

					            auto const old = *addr;
					            auto& ref = *addr;

					// gcc-7.4.0 assumes for an optimization that a signed overflow does not occur here.
					// That's fine, so ignore that warning.
					#if BOOST_COMP_GNUC && (BOOST_COMP_GNUC == BOOST_VERSION_NUMBER(7, 4, 0))
					#    pragma GCC diagnostic push
					#    pragma GCC diagnostic ignored "-Wstrict-overflow"
					#endif
					            BitUnion o{old};
					            BitUnion c{compare};

					            ref = ((o.r == c.r) ? value : old);
					#if BOOST_COMP_GNUC && (BOOST_COMP_GNUC == BOOST_VERSION_NUMBER(7, 4, 0))
					#    pragma GCC diagnostic pop
					#endif
					            return old;
					        }
					    };
					} // namespace alpaka
					// ==
					// == ./include/alpaka/atomic/Op.hpp ==
					// ============================================================================

				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/core/Concepts.hpp ==
					// ==
					/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include <type_traits>    // amalgamate: file already included

					namespace alpaka::concepts
					{
					    //! Tag used in class inheritance hierarchies that describes that a specific concept (TConcept)
					    //! is implemented by the given base class (TBase).
					    template<typename TConcept, typename TBase>
					    struct Implements
					    {
					    };

					    //! Checks whether the concept is implemented by the given class
					    template<typename TConcept, typename TDerived>
					    struct ImplementsConcept
					    {
					        template<typename TBase>
					        static auto implements(Implements<TConcept, TBase>&) -> std::true_type;
					        static auto implements(...) -> std::false_type;

					        static constexpr auto value = decltype(implements(std::declval<TDerived&>()))::value;
					    };

					    namespace detail
					    {
					        //! Returns the type that implements the given concept in the inheritance hierarchy.
					        template<typename TConcept, typename TDerived, typename Sfinae = void>
					        struct ImplementationBaseType;

					        //! Base case for types that do not inherit from "Implements<TConcept, ...>" is the type itself.
					        template<typename TConcept, typename TDerived>
					        struct ImplementationBaseType<
					            TConcept,
					            TDerived,
					            std::enable_if_t<!ImplementsConcept<TConcept, TDerived>::value>>
					        {
					            using type = TDerived;
					        };

					        //! For types that inherit from "Implements<TConcept, ...>" it finds the base class (TBase) which
					        //! implements the concept.
					        template<typename TConcept, typename TDerived>
					        struct ImplementationBaseType<
					            TConcept,
					            TDerived,
					            std::enable_if_t<ImplementsConcept<TConcept, TDerived>::value>>
					        {
					            template<typename TBase>
					            static auto implementer(Implements<TConcept, TBase>&) -> TBase;

					            using type = decltype(implementer(std::declval<TDerived&>()));

					            static_assert(
					                std::is_base_of_v<type, TDerived>,
					                "The type implementing the concept has to be a publicly accessible base class!");
					        };
					    } // namespace detail

					    //! Returns the type that implements the given concept in the inheritance hierarchy.
					    template<typename TConcept, typename TDerived>
					    using ImplementationBase = typename detail::ImplementationBaseType<TConcept, TDerived>::type;
					} // namespace alpaka::concepts
					// ==
					// == ./include/alpaka/core/Concepts.hpp ==
					// ============================================================================

					// ============================================================================
					// == ./include/alpaka/core/Positioning.hpp ==
					// ==
					/* Copyright 2019 Benjamin Worpitz, René Widera
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					namespace alpaka
					{
					    //! Defines the parallelism hierarchy levels of alpaka
					    namespace hierarchy
					    {
					        struct Grids
					        {
					        };

					        struct Blocks
					        {
					        };

					        struct Threads
					        {
					        };
					    } // namespace hierarchy
					    //! Defines the origins available for getting extent and indices of kernel executions.
					    namespace origin
					    {
					        //! This type is used to get the extents/indices relative to the grid.
					        struct Grid;
					        //! This type is used to get the extent/indices relative to a/the current block.
					        struct Block;
					        //! This type is used to get the extents relative to the thread.
					        struct Thread;
					    } // namespace origin
					    //! Defines the units available for getting extent and indices of kernel executions.
					    namespace unit
					    {
					        //! This type is used to get the extent/indices in units of blocks.
					        struct Blocks;
					        //! This type is used to get the extent/indices in units of threads.
					        struct Threads;
					        //! This type is used to get the extents/indices in units of elements.
					        struct Elems;
					    } // namespace unit

					    using namespace origin;
					    using namespace unit;
					} // namespace alpaka
					// ==
					// == ./include/alpaka/core/Positioning.hpp ==
					// ============================================================================


				// #include <type_traits>    // amalgamate: file already included

				namespace alpaka
				{
				    struct ConceptAtomicGrids
				    {
				    };
				    struct ConceptAtomicBlocks
				    {
				    };
				    struct ConceptAtomicThreads
				    {
				    };

				    namespace detail
				    {
				        template<typename THierarchy>
				        struct AtomicHierarchyConceptType;

				        template<>
				        struct AtomicHierarchyConceptType<hierarchy::Threads>
				        {
				            using type = ConceptAtomicThreads;
				        };

				        template<>
				        struct AtomicHierarchyConceptType<hierarchy::Blocks>
				        {
				            using type = ConceptAtomicBlocks;
				        };

				        template<>
				        struct AtomicHierarchyConceptType<hierarchy::Grids>
				        {
				            using type = ConceptAtomicGrids;
				        };
				    } // namespace detail

				    template<typename THierarchy>
				    using AtomicHierarchyConcept = typename detail::AtomicHierarchyConceptType<THierarchy>::type;

				    //! The atomic operation trait.
				    namespace trait
				    {
				        //! The atomic operation trait.
				        template<typename TOp, typename TAtomic, typename T, typename THierarchy, typename TSfinae = void>
				        struct AtomicOp;
				    } // namespace trait

				    //! Executes the given operation atomically.
				    //!
				    //! \tparam TOp The operation type.
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TOp, typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicOp(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& = THierarchy()) -> T
				    {
				        using ImplementationBase = typename concepts::ImplementationBase<AtomicHierarchyConcept<THierarchy>, TAtomic>;
				        return trait::AtomicOp<TOp, ImplementationBase, T, THierarchy>::atomicOp(atomic, addr, value);
				    }

				    //! Executes the given operation atomically.
				    //!
				    //! \tparam TOp The operation type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \tparam T The value type.
				    //! \param atomic The atomic implementation.
				    //! \param addr The value to change atomically.
				    //! \param compare The comparison value used in the atomic operation.
				    //! \param value The value used in the atomic operation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TOp, typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicOp(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& compare,
				        T const& value,
				        THierarchy const& = THierarchy()) -> T
				    {
				        using ImplementationBase = typename concepts::ImplementationBase<AtomicHierarchyConcept<THierarchy>, TAtomic>;
				        return trait::AtomicOp<TOp, ImplementationBase, T, THierarchy>::atomicOp(atomic, addr, compare, value);
				    }

				    //! Executes an atomic add operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicAdd(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicAdd>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic sub operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicSub(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicSub>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic min operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicMin(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicMin>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic max operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicMax(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicMax>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic exchange operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicExch(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicExch>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic increment operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicInc(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicInc>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic decrement operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicDec(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicDec>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic and operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicAnd(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicAnd>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic or operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicOr(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicOr>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic xor operation.
				    //!
				    //! \tparam T The value type.
				    //! \tparam TAtomic The atomic implementation type.
				    //! \param addr The value to change atomically.
				    //! \param value The value used in the atomic operation.
				    //! \param atomic The atomic implementation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicXor(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicXor>(atomic, addr, value, hier);
				    }

				    //! Executes an atomic compare-and-swap operation.
				    //!
				    //! \tparam TAtomic The atomic implementation type.
				    //! \tparam T The value type.
				    //! \param atomic The atomic implementation.
				    //! \param addr The value to change atomically.
				    //! \param compare The comparison value used in the atomic operation.
				    //! \param value The value used in the atomic operation.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TAtomic, typename T, typename THierarchy = hierarchy::Grids>
				    ALPAKA_FN_HOST_ACC auto atomicCas(
				        TAtomic const& atomic,
				        T* const addr,
				        T const& compare,
				        T const& value,
				        THierarchy const& hier = THierarchy()) -> T
				    {
				        return atomicOp<AtomicCas>(atomic, addr, compare, value, hier);
				    }
				} // namespace alpaka
				// ==
				// == ./include/alpaka/atomic/Traits.hpp ==
				// ============================================================================

			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

			#include <array>
			#include <atomic>
			// #include <type_traits>    // amalgamate: file already included

			#ifndef ALPAKA_DISABLE_ATOMIC_ATOMICREF
			#    ifndef ALPAKA_HAS_STD_ATOMIC_REF
			#        include <boost/atomic.hpp>
			#    endif

			namespace alpaka
			{
			    namespace detail
			    {
			#    if defined(ALPAKA_HAS_STD_ATOMIC_REF)
			        template<typename T>
			        using atomic_ref = std::atomic_ref<T>;
			#    else
			        template<typename T>
			        using atomic_ref = boost::atomic_ref<T>;
			#    endif
			    } // namespace detail
			    //! The atomic ops based on atomic_ref for CPU accelerators.
			    //
			    //  Atomics can be used in the grids, blocks and threads hierarchy levels.
			    //

			    class AtomicAtomicRef
			    {
			    };

			    template<typename T>
			    void isSupportedByAtomicAtomicRef()
			    {
			        static_assert(
			            std::is_trivially_copyable_v<T> && detail::atomic_ref<T>::required_alignment <= alignof(T),
			            "Type not supported by AtomicAtomicRef, please recompile defining "
			            "ALPAKA_DISABLE_ATOMIC_ATOMICREF.");
			    }

			    namespace trait
			    {
			        //! The CPU accelerators AtomicAdd.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicAdd, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                return ref.fetch_add(value);
			            }
			        };

			        //! The CPU accelerators AtomicSub.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicSub, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                return ref.fetch_sub(value);
			            }
			        };

			        //! The CPU accelerators AtomicMin.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicMin, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                T old = ref;
			                T result = old;
			                result = std::min(result, value);
			                while(!ref.compare_exchange_weak(old, result))
			                {
			                    result = old;
			                    result = std::min(result, value);
			                }
			                return old;
			            }
			        };

			        //! The CPU accelerators AtomicMax.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicMax, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                T old = ref;
			                T result = old;
			                result = std::max(result, value);
			                while(!ref.compare_exchange_weak(old, result))
			                {
			                    result = old;
			                    result = std::max(result, value);
			                }
			                return old;
			            }
			        };

			        //! The CPU accelerators AtomicExch.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicExch, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                T old = ref;
			                T result = value;
			                while(!ref.compare_exchange_weak(old, result))
			                {
			                    result = value;
			                }
			                return old;
			            }
			        };

			        //! The CPU accelerators AtomicInc.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicInc, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                T old = ref;
			                T result = ((old >= value) ? 0 : static_cast<T>(old + 1));
			                while(!ref.compare_exchange_weak(old, result))
			                {
			                    result = ((old >= value) ? 0 : static_cast<T>(old + 1));
			                }
			                return old;
			            }
			        };

			        //! The CPU accelerators AtomicDec.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicDec, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                T old = ref;
			                T result = ((old >= value) ? 0 : static_cast<T>(old - 1));
			                while(!ref.compare_exchange_weak(old, result))
			                {
			                    result = ((old >= value) ? 0 : static_cast<T>(old - 1));
			                }
			                return old;
			            }
			        };

			        //! The CPU accelerators AtomicAnd.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicAnd, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                return ref.fetch_and(value);
			            }
			        };

			        //! The CPU accelerators AtomicOr.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicOr, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                return ref.fetch_or(value);
			            }
			        };

			        //! The CPU accelerators AtomicXor.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicXor, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(AtomicAtomicRef const&, T* const addr, T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                return ref.fetch_xor(value);
			            }
			        };

			        //! The CPU accelerators AtomicCas.
			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicCas, AtomicAtomicRef, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(
			                AtomicAtomicRef const&,
			                T* const addr,
			                T const& compare,
			                T const& value) -> T
			            {
			                isSupportedByAtomicAtomicRef<T>();
			                detail::atomic_ref<T> ref(*addr);
			                T old = ref;
			                T result;
			                do
			                {
			                    result = ((old == compare) ? value : old);
			                } while(!ref.compare_exchange_weak(old, result));
			                return old;
			            }
			        };
			    } // namespace trait
			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/atomic/AtomicAtomicRef.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/atomic/AtomicStdLibLock.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Matthias Werner, René Widera, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/atomic/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

			// #include <array>    // amalgamate: file already included
			#include <mutex>

			#ifdef ALPAKA_DISABLE_ATOMIC_ATOMICREF

			namespace alpaka
			{
			    //! The CPU threads accelerator atomic ops.
			    //
			    //  Atomics can be used in the grids, blocks and threads hierarchy levels.
			    //  Atomics are not guaranteed to be save between devices.
			    //
			    // \tparam THashTableSize size of the hash table to allow concurrency between
			    //                        atomics to different addresses
			    template<size_t THashTableSize>
			    class AtomicStdLibLock
			    {
			    public:
			        template<typename TAtomic, typename TOp, typename T, typename THierarchy, typename TSfinae>
			        friend struct trait::AtomicOp;

			        static constexpr auto nextPowerOf2(size_t const value, size_t const bit = 0u) -> size_t
			        {
			            return value <= (static_cast<size_t>(1u) << bit) ? (static_cast<size_t>(1u) << bit)
			                                                             : nextPowerOf2(value, bit + 1u);
			        }

			        //! get a hash value of the pointer
			        //
			        // This is no perfect hash, there will be collisions if the size of pointer type
			        // is not a power of two.
			        template<typename TPtr>
			        static auto hash(TPtr const* const ptr) -> size_t
			        {
			            auto const ptrAddr = reinterpret_cast<size_t>(ptr);
			            // using power of two for the next division will increase the performance
			            constexpr size_t typeSizePowerOf2 = nextPowerOf2(sizeof(TPtr));
			            // division removes the stride between indices
			            return (ptrAddr / typeSizePowerOf2);
			        }

			        template<typename TPtr>
			        auto getMutex(TPtr const* const ptr) const -> std::mutex&
			        {
			            //! get the size of the hash table
			            //
			            // The size is at least 1 or THashTableSize rounded up to the next power of 2
			            constexpr size_t hashTableSize = THashTableSize == 0u ? 1u : nextPowerOf2(THashTableSize);

			            size_t const hashedAddr = hash(ptr) & (hashTableSize - 1u);
			#    if BOOST_COMP_CLANG
			#        pragma clang diagnostic push
			#        pragma clang diagnostic ignored "-Wexit-time-destructors"
			#    endif
			            static std::array<
			                std::mutex,
			                hashTableSize>
			                m_mtxAtomic; //!< The mutex protecting access for an atomic operation.
			#    if BOOST_COMP_CLANG
			#        pragma clang diagnostic pop
			#    endif
			            return m_mtxAtomic[hashedAddr];
			        }
			    };

			    namespace trait
			    {
			        //! The CPU threads accelerator atomic operation.
			        template<typename TOp, typename T, typename THierarchy, size_t THashTableSize>
			        struct AtomicOp<TOp, AtomicStdLibLock<THashTableSize>, T, THierarchy>
			        {
			            ALPAKA_FN_HOST static auto atomicOp(
			                AtomicStdLibLock<THashTableSize> const& atomic,
			                T* const addr,
			                T const& value) -> T
			            {
			                std::lock_guard<std::mutex> lock(atomic.getMutex(addr));
			                return TOp()(addr, value);
			            }
			            ALPAKA_FN_HOST static auto atomicOp(
			                AtomicStdLibLock<THashTableSize> const& atomic,
			                T* const addr,
			                T const& compare,
			                T const& value) -> T
			            {
			                std::lock_guard<std::mutex> lock(atomic.getMutex(addr));
			                return TOp()(addr, compare, value);
			            }
			        };
			    } // namespace trait
			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/atomic/AtomicStdLibLock.hpp ==
			// ============================================================================


		namespace alpaka
		{
		#ifndef ALPAKA_DISABLE_ATOMIC_ATOMICREF
		    using AtomicCpu = AtomicAtomicRef;
		#else
		    using AtomicCpu = AtomicStdLibLock<16>;
		#endif // ALPAKA_DISABLE_ATOMIC_ATOMICREF

		} // namespace alpaka
		// ==
		// == ./include/alpaka/atomic/AtomicCpu.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/atomic/AtomicHierarchy.hpp ==
		// ==
		/* Copyright 2020 Benjamin Worpitz, René Widera, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/atomic/Traits.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/meta/InheritFromList.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			namespace alpaka::meta
			{
			    template<typename TBaseList>
			    class InheritFromList;

			    template<template<typename...> class TList, typename... TBases>
			    class InheritFromList<TList<TBases...>> : public TBases...
			    {
			    };
			} // namespace alpaka::meta
			// ==
			// == ./include/alpaka/meta/InheritFromList.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/meta/Unique.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include <type_traits>    // amalgamate: file already included

			namespace alpaka::meta
			{
			    namespace detail
			    {
			        template<typename T, typename... Ts>
			        struct UniqueHelper
			        {
			            using type = T;
			        };

			        template<template<typename...> class TList, typename... Ts, typename U, typename... Us>
			        struct UniqueHelper<TList<Ts...>, U, Us...>
			            : std::conditional_t<
			                  (std::is_same_v<U, Ts> || ...),
			                  UniqueHelper<TList<Ts...>, Us...>,
			                  UniqueHelper<TList<Ts..., U>, Us...>>
			        {
			        };

			        template<typename T>
			        struct UniqueImpl;

			        template<template<typename...> class TList, typename... Ts>
			        struct UniqueImpl<TList<Ts...>>
			        {
			            using type = typename UniqueHelper<TList<>, Ts...>::type;
			        };
			    } // namespace detail

			    //! Trait that returns a list with only unique (no equal) types (a set). Duplicates will be filtered out.
			    template<typename TList>
			    using Unique = typename detail::UniqueImpl<TList>::type;
			} // namespace alpaka::meta
			// ==
			// == ./include/alpaka/meta/Unique.hpp ==
			// ============================================================================


		#include <tuple>

		namespace alpaka
		{
		    //! build a single class to inherit from different atomic implementations
		    //
		    //  This implementation inherit from all three hierarchies.
		    //  The multiple usage of the same type for different levels is allowed.
		    //  The class provide the feature that each atomic operation can be focused
		    //  to a hierarchy level in alpaka. A operation to a hierarchy is independent
		    //  to the memory hierarchy.
		    //
		    //  \tparam TGridAtomic atomic implementation for atomic operations between grids within a device
		    //  \tparam TBlockAtomic atomic implementation for atomic operations between blocks within a grid
		    //  \tparam TThreadAtomic atomic implementation for atomic operations between threads within a block
		    template<typename TGridAtomic, typename TBlockAtomic, typename TThreadAtomic>
		    using AtomicHierarchy = alpaka::meta::InheritFromList<alpaka::meta::Unique<std::tuple<
		        TGridAtomic,
		        TBlockAtomic,
		        TThreadAtomic,
		        concepts::Implements<ConceptAtomicGrids, TGridAtomic>,
		        concepts::Implements<ConceptAtomicBlocks, TBlockAtomic>,
		        concepts::Implements<ConceptAtomicThreads, TThreadAtomic>>>>;
		} // namespace alpaka
		// ==
		// == ./include/alpaka/atomic/AtomicHierarchy.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/atomic/AtomicNoOp.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, René Widera, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/atomic/Traits.hpp"    // amalgamate: file already expanded

		namespace alpaka
		{
		    //! The NoOp atomic ops.
		    class AtomicNoOp
		    {
		    };

		    namespace trait
		    {
		        //! The NoOp atomic operation.
		        template<typename TOp, typename T, typename THierarchy>
		        struct AtomicOp<TOp, AtomicNoOp, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicNoOp const& /* atomic */, T* const addr, T const& value) -> T
		            {
		                return TOp()(addr, value);
		            }

		            ALPAKA_FN_HOST static auto atomicOp(
		                AtomicNoOp const& /* atomic */,
		                T* const addr,
		                T const& compare,
		                T const& value) -> T
		            {
		                return TOp()(addr, compare, value);
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/atomic/AtomicNoOp.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/atomic/AtomicOmpBuiltIn.hpp ==
		// ==
		/* Copyright 2022 René Widera, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/atomic/Op.hpp"    // amalgamate: file already expanded
		// #include "alpaka/atomic/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

		#ifdef _OPENMP

		namespace alpaka
		{
		    //! The OpenMP accelerators atomic ops.
		    //
		    //  Atomics can be used in the blocks and threads hierarchy levels.
		    //  Atomics are not guaranteed to be safe between devices or grids.
		    class AtomicOmpBuiltIn
		    {
		    };

		    namespace trait
		    {
		// check for OpenMP 3.1+
		// "omp atomic capture" is not supported before OpenMP 3.1
		#    if _OPENMP >= 201107

		        //! The OpenMP accelerators atomic operation: ADD
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicAdd, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic push
		#            pragma GCC diagnostic ignored "-Wconversion"
		#        endif
		#        pragma omp atomic capture
		                {
		                    old = ref;
		                    ref += value;
		                }
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic pop
		#        endif
		                return old;
		            }
		        };

		        //! The OpenMP accelerators atomic operation: SUB
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicSub, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic push
		#            pragma GCC diagnostic ignored "-Wconversion"
		#        endif
		#        pragma omp atomic capture
		                {
		                    old = ref;
		                    ref -= value;
		                }
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic pop
		#        endif
		                return old;
		            }
		        };

		        //! The OpenMP accelerators atomic operation: EXCH
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicExch, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        pragma omp atomic capture
		                {
		                    old = ref;
		                    ref = value;
		                }
		                return old;
		            }
		        };

		        //! The OpenMP accelerators atomic operation: AND
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicAnd, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic push
		#            pragma GCC diagnostic ignored "-Wconversion"
		#        endif
		#        pragma omp atomic capture
		                {
		                    old = ref;
		                    ref &= value;
		                }
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic pop
		#        endif
		                return old;
		            }
		        };

		        //! The OpenMP accelerators atomic operation: OR
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicOr, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic push
		#            pragma GCC diagnostic ignored "-Wconversion"
		#        endif
		#        pragma omp atomic capture
		                {
		                    old = ref;
		                    ref |= value;
		                }
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic pop
		#        endif
		                return old;
		            }
		        };

		        //! The OpenMP accelerators atomic operation: XOR
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicXor, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic push
		#            pragma GCC diagnostic ignored "-Wconversion"
		#        endif
		#        pragma omp atomic capture
		                {
		                    old = ref;
		                    ref ^= value;
		                }
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic pop
		#        endif
		                return old;
		            }
		        };

		#    endif // _OPENMP >= 201107

		// check for OpenMP 5.1+
		// "omp atomic compare" was introduced with OpenMP 5.1
		#    if _OPENMP >= 202011

		        //! The OpenMP accelerators atomic operation: Min
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicMin, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        pragma omp atomic capture compare
		                {
		                    old = ref;
		                    ref = (ref <= value) ? ref : value;
		                }
		                return old;
		            }
		        };

		        //! The OpenMP accelerators atomic operation: Max
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicMax, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        pragma omp atomic capture compare
		                {
		                    old = ref;
		                    ref = (ref >= value) ? ref : value;
		                }
		                return old;
		            }
		        };

		        //! The OpenMP accelerators atomic operation: Inc
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicInc, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic push
		#            pragma GCC diagnostic ignored "-Wconversion"
		#        endif
		#        pragma omp atomic capture compare
		                {
		                    old = ref;
		                    ref = ((ref >= value) ? 0 : (ref + 1));
		                }
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic pop
		#        endif
		                return old;
		            }
		        };

		        //! The OpenMP accelerators atomic operation: Dec
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicDec, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic push
		#            pragma GCC diagnostic ignored "-Wconversion"
		#        endif
		#        pragma omp atomic capture compare
		                {
		                    old = ref;
		                    ref = ((ref == 0) || (ref > value)) ? value : (ref - 1);
		                }
		#        if BOOST_COMP_GNUC
		#            pragma GCC diagnostic pop
		#        endif
		                return old;
		            }
		        };

		        //! The OpenMP accelerators atomic operation: Cas
		        template<typename T, typename THierarchy>
		        struct AtomicOp<AtomicCas, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(
		                AtomicOmpBuiltIn const&,
		                T* const addr,
		                T const& compare,
		                T const& value) -> T
		            {
		                T old;
		                auto& ref(*addr);
		// atomically update ref, but capture the original value in old
		#        pragma omp atomic capture compare
		                {
		                    old = ref;
		                    ref = (ref == compare ? value : ref);
		                }
		                return old;
		            }
		        };

		#    else
		        //! The OpenMP accelerators atomic operation
		        //
		        // generic implementations for operations where native atomics are not available
		        template<typename TOp, typename T, typename THierarchy>
		        struct AtomicOp<TOp, AtomicOmpBuiltIn, T, THierarchy>
		        {
		            ALPAKA_FN_HOST static auto atomicOp(AtomicOmpBuiltIn const&, T* const addr, T const& value) -> T
		            {
		                T old;
		// \TODO: Currently not only the access to the same memory location is protected by a mutex but all atomic ops on all
		// threads.
		#        pragma omp critical(AlpakaOmpAtomicOp)
		                {
		                    old = TOp()(addr, value);
		                }
		                return old;
		            }
		            ALPAKA_FN_HOST static auto atomicOp(
		                AtomicOmpBuiltIn const&,
		                T* const addr,
		                T const& compare,
		                T const& value) -> T
		            {
		                T old;
		// \TODO: Currently not only the access to the same memory location is protected by a mutex but all atomic ops on all
		// threads.
		#        pragma omp critical(AlpakaOmpAtomicOp2)
		                {
		                    old = TOp()(addr, compare, value);
		                }
		                return old;
		            }
		        };

		#    endif // _OPENMP >= 202011

		    } // namespace trait
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/atomic/AtomicOmpBuiltIn.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/block/shared/dyn/BlockSharedMemDynMember.hpp ==
		// ==
		/* Copyright 2023 Jeffrey Kelling, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
			// ============================================================================
			// == ./include/alpaka/block/shared/dyn/BlockSharedDynMemberAllocKiB.hpp ==
			// ==
			/* Copyright 2022 Jeffrey Kelling
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			#include <cstdint>

			namespace alpaka
			{
			#ifndef ALPAKA_BLOCK_SHARED_DYN_MEMBER_ALLOC_KIB
			#    define ALPAKA_BLOCK_SHARED_DYN_MEMBER_ALLOC_KIB 47u
			#endif
			    constexpr std::uint32_t BlockSharedDynMemberAllocKiB = ALPAKA_BLOCK_SHARED_DYN_MEMBER_ALLOC_KIB;
			} // namespace alpaka
			// ==
			// == ./include/alpaka/block/shared/dyn/BlockSharedDynMemberAllocKiB.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/block/shared/dyn/Traits.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			namespace alpaka
			{
			    struct ConceptBlockSharedDyn
			    {
			    };

			    //! The block shared dynamic memory operation traits.
			    namespace trait
			    {
			        //! The block shared dynamic memory get trait.
			        template<typename T, typename TBlockSharedMemDyn, typename TSfinae = void>
			        struct GetDynSharedMem;
			    } // namespace trait

			    //! Get block shared dynamic memory.
			    //!
			    //! The available size of the memory can be defined by specializing the trait
			    //! BlockSharedMemDynSizeBytes for a kernel.
			    //! The Memory can be accessed by all threads within a block.
			    //! Access to the memory is not thread safe.
			    //!
			    //! \tparam T The element type.
			    //! \tparam TBlockSharedMemDyn The block shared dynamic memory implementation type.
			    //! \param blockSharedMemDyn The block shared dynamic memory implementation.
			    //! \return Pointer to pre-allocated contiguous memory.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TBlockSharedMemDyn>
			    ALPAKA_FN_ACC auto getDynSharedMem(TBlockSharedMemDyn const& blockSharedMemDyn) -> T*
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptBlockSharedDyn, TBlockSharedMemDyn>;
			        return trait::GetDynSharedMem<T, ImplementationBase>::getMem(blockSharedMemDyn);
			    }
			} // namespace alpaka
			// ==
			// == ./include/alpaka/block/shared/dyn/Traits.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/core/Assert.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

			#include <cassert>
			// #include <type_traits>    // amalgamate: file already included

			#define ALPAKA_ASSERT(...) assert(__VA_ARGS__)

			#if defined(ALPAKA_DEBUG_OFFLOAD_ASSUME_HOST) || defined(SYCL_EXT_ONEAPI_ASSERT)
			#    define ALPAKA_ASSERT_OFFLOAD(EXPRESSION) ALPAKA_ASSERT(EXPRESSION)
			#elif defined __AMDGCN__ && (!defined NDEBUG)
			#    define ALPAKA_ASSERT_OFFLOAD(EXPRESSION)                                                                         \
			        do                                                                                                            \
			        {                                                                                                             \
			            if(!(EXPRESSION))                                                                                         \
			                __builtin_trap();                                                                                     \
			        } while(false)
			#else
			#    define ALPAKA_ASSERT_OFFLOAD(EXPRESSION)                                                                         \
			        do                                                                                                            \
			        {                                                                                                             \
			        } while(false)
			#endif

			namespace alpaka::core
			{
			    namespace detail
			    {
			        template<typename TArg>
			        struct AssertValueUnsigned
			        {
			            ALPAKA_NO_HOST_ACC_WARNING ALPAKA_FN_HOST_ACC static constexpr auto assertValueUnsigned(
			                [[maybe_unused]] TArg const& arg)
			            {
			                if constexpr(std::is_signed_v<TArg>)
			                    ALPAKA_ASSERT_OFFLOAD(arg >= 0);

			                // Nothing to do for unsigned types.
			            }
			        };
			    } // namespace detail

			    //! This method checks integral values if they are greater or equal zero.
			    //! The implementation prevents warnings for checking this for unsigned types.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TArg>
			    ALPAKA_FN_HOST_ACC constexpr auto assertValueUnsigned(TArg const& arg) -> void
			    {
			        detail::AssertValueUnsigned<TArg>::assertValueUnsigned(arg);
			    }

			    namespace detail
			    {
			        template<typename TLhs, typename TRhs>
			        struct AssertGreaterThan
			        {
			            ALPAKA_NO_HOST_ACC_WARNING ALPAKA_FN_HOST_ACC static constexpr auto assertGreaterThan(
			                [[maybe_unused]] TRhs const& rhs)
			            {
			                if constexpr(std::is_signed_v<TRhs> || (TLhs::value != 0u))
			                    ALPAKA_ASSERT_OFFLOAD(TLhs::value > rhs);

			                // Nothing to do for unsigned types comparing to zero.
			            }
			        };
			    } // namespace detail

			    //! This function asserts that the integral value TLhs is greater than TRhs.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TLhs, typename TRhs>
			    ALPAKA_FN_HOST_ACC constexpr auto assertGreaterThan(TRhs const& rhs) -> void
			    {
			        detail::AssertGreaterThan<TLhs, TRhs>::assertGreaterThan(rhs);
			    }
			} // namespace alpaka::core
			// ==
			// == ./include/alpaka/core/Assert.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/core/Vectorize.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

			#include <cstddef>
			// #include <cstdint>    // amalgamate: file already included

			//! Suggests vectorization of the directly following loop to the compiler.
			//!
			//! Usage:
			//!  `ALPAKA_VECTORIZE_HINT
			//!  for(...){...}`
			// \TODO: Implement for other compilers.
			// See: http://stackoverflow.com/questions/2706286/pragmas-swp-ivdep-prefetch-support-in-various-compilers
			/*#if BOOST_COMP_HPACC
			    #define ALPAKA_VECTORIZE_HINT(...)  _Pragma("ivdep")
			#elif BOOST_COMP_PGI
			    #define ALPAKA_VECTORIZE_HINT(...)  _Pragma("vector")
			#elif BOOST_COMP_MSVC
			    #define ALPAKA_VECTORIZE_HINT(...)  __pragma(loop(ivdep))
			#elif BOOST_COMP_GNUC
			    #define ALPAKA_VECTORIZE_HINT(...)  _Pragma("GCC ivdep")
			#else
			    #define ALPAKA_VECTORIZE_HINT(...)
			#endif*/

			namespace alpaka::core::vectorization
			{
			    // The alignment required to enable optimal performance dependant on the target architecture.
			    constexpr std::size_t defaultAlignment =
			#if defined(__AVX512BW__) || defined(__AVX512F__) || defined(__MIC__)
			        64u
			#elif defined(__AVX__) || defined(__AVX2__)
			        32u
			#else
			        16u
			#endif
			        ;

			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    // By default there is no vectorization.
			    template<typename TElem>
			    struct GetVectorizationSizeElems
			    {
			        static constexpr std::size_t value = 1u;
			    };

			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<double>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512F__) || defined(__MIC__)
			            // addition (AVX512F,KNC): vaddpd / _mm512_add_pd
			            // subtraction (AVX512F,KNC): vsubpd / _mm512_sub_pd
			            // multiplication (AVX512F,KNC): vmulpd / _mm512_mul_pd
			            8u;
			#elif defined(__AVX__)
			            // addition (AVX): vaddpd / _mm256_add_pd
			            // subtraction (AVX): vsubpd / _mm256_sub_pd
			            // multiplication (AVX): vmulpd / _mm256_mul_pd
			            4u;
			#elif defined(__SSE2__)
			            // addition (SSE2): addpd / _mm_add_pd
			            // subtraction (SSE2): subpd / _mm_sub_pd
			            // multiplication (SSE2): mulpd / _mm_mul_pd
			            2u;
			#elif defined(__ARM_NEON__)
			            // No support for double precision vectorization!
			            1u;
			#elif defined(__ALTIVEC__)
			            2u;
			#else
			            1u;
			#endif
			    };
			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<float>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512F__) || defined(__MIC__)
			            // addition (AVX512F,KNC): vaddps / _mm512_add_ps
			            // subtraction (AVX512F,KNC): vsubps / _mm512_sub_ps
			            // multiplication (AVX512F,KNC): vmulps / _mm512_mul_ps
			            16u;
			#elif defined(__AVX__)
			            // addition (AVX): vaddps / _mm256_add_ps
			            // subtraction (AVX): vsubps / _mm256_sub_ps
			            // multiplication (AVX): vmulps / _mm256_mul_ps
			            8u;
			#elif defined(__SSE__)
			            // addition (SSE): addps / _mm_add_ps
			            // subtraction (SSE): subps / _mm_sub_ps
			            // multiplication (SSE): mulps / _mm_mul_ps
			            4u;
			#elif defined(__ARM_NEON__)
			            4u;
			#elif defined(__ALTIVEC__)
			            4u;
			#else
			            1u;
			#endif
			    };
			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<std::int8_t>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512BW__)
			            // addition (AVX512BW): vpaddb / _mm512_mask_add_epi8
			            // subtraction (AVX512BW): vpsubb / _mm512_sub_epi8
			            // multiplication: -
			            64u;
			#elif defined(__AVX2__)
			            // addition (AVX2): vpaddb / _mm256_add_epi8
			            // subtraction (AVX2): vpsubb / _mm256_sub_epi8
			            // multiplication: -
			            32u;
			#elif defined(__SSE2__)
			            // addition (SSE2): paddb / _mm_add_epi8
			            // subtraction (SSE2): psubb / _mm_sub_epi8
			            // multiplication: -
			            16u;
			#elif defined(__ARM_NEON__)
			            16u;
			#elif defined(__ALTIVEC__)
			            16u;
			#elif defined(__CUDA_ARCH__)
			            // addition: __vadd4
			            // subtraction: __vsub4
			            // multiplication: -
			            4u;
			#else
			            1u;
			#endif
			    };
			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<std::uint8_t>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512BW__)
			            // addition (AVX512BW): vpaddb / _mm512_mask_add_epi8
			            // subtraction (AVX512BW): vpsubb / _mm512_sub_epi8
			            // multiplication: -
			            64u;
			#elif defined(__AVX2__)
			            // addition (AVX2): vpaddb / _mm256_add_epi8
			            // subtraction (AVX2): vpsubb / _mm256_sub_epi8
			            // multiplication: -
			            32u;
			#elif defined(__SSE2__)
			            // addition (SSE2): paddb / _mm_add_epi8
			            // subtraction (SSE2): psubb / _mm_sub_epi8
			            // multiplication: -
			            16u;
			#elif defined(__ARM_NEON__)
			            16u;
			#elif defined(__ALTIVEC__)
			            16u;
			#elif defined(__CUDA_ARCH__)
			            // addition: __vadd4
			            // subtraction: __vsub4
			            // multiplication: -
			            4u;
			#else
			            1u;
			#endif
			    };
			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<std::int16_t>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512BW__)
			            // addition (AVX512BW): vpaddw / _mm512_mask_add_epi16
			            // subtraction (AVX512BW): vpsubw / _mm512_mask_sub_epi16
			            // multiplication (AVX512BW): vpmullw / _mm512_mask_mullo_epi16
			            32u;
			#elif defined(__AVX2__)
			            // addition (AVX2): vpaddw / _mm256_add_epi16
			            // subtraction (AVX2): vpsubw / _mm256_sub_epi16
			            // multiplication (AVX2): vpmullw / _mm256_mullo_epi16
			            16u;
			#elif defined(__SSE2__)
			            // addition (SSE2): paddw / _mm_add_epi16
			            // subtraction (SSE2): psubw / _mm_sub_epi16
			            // multiplication (SSE2): pmullw / _mm_mullo_epi16
			            8u;
			#elif defined(__ARM_NEON__)
			            8u;
			#elif defined(__ALTIVEC__)
			            8u;
			#elif defined(__CUDA_ARCH__)
			            // addition: __vadd2
			            // subtraction: __vsub2
			            // multiplication: -
			            2u;
			#else
			            1u;
			#endif
			    };
			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<std::uint16_t>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512BW__)
			            // addition (AVX512BW): vpaddusw / _mm512_mask_adds_epu16
			            // subtraction (AVX512BW): vpsubw / _mm512_subs_epu16
			            // multiplication: ?
			            32u;
			#elif defined(__AVX2__)
			            // addition (AVX2): vpaddusw / _mm256_adds_epu16
			            // subtraction (AVX2): vpsubusw / _mm256_subs_epu16
			            // multiplication: ?
			            16u;
			#elif defined(__SSE2__)
			            // addition (SSE2): paddusw / _mm_adds_epu16
			            // subtraction (SSE2): psubusw / _mm_subs_epu16
			            // multiplication: ?
			            8u;
			#elif defined(__ARM_NEON__)
			            8u;
			#elif defined(__ALTIVEC__)
			            8u;
			#elif defined(__CUDA_ARCH__)
			            // addition: __vadd2
			            // subtraction: __vsub2
			            // multiplication: -
			            2u;
			#else
			            1u;
			#endif
			    };
			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<std::int32_t>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512F__) || defined(__MIC__)
			            // addition (AVX512F,KNC): vpaddd / _mm512_mask_add_epi32
			            // subtraction (AVX512F,KNC): vpsubd / _mm512_mask_sub_epi32
			            // multiplication (AVX512F,KNC): vpmulld / _mm512_mask_mullo_epi32
			            16u;
			#elif defined(__AVX2__)
			            // addition (AVX2): vpaddd / _mm256_add_epi32
			            // subtraction (AVX2): vpsubd / _mm256_sub_epi32
			            // multiplication (AVX2): vpmulld / _mm256_mullo_epi32
			            8u;
			#elif defined(__SSE2__)
			            // addition (SSE2): paddd / _mm_add_epi32
			            // subtraction (SSE2): psubd / _mm_sub_epi32
			            // multiplication (SSE4.1): pmulld / _mm_mullo_epi32
			            4u;
			#elif defined(__ARM_NEON__)
			            4u;
			#elif defined(__ALTIVEC__)
			            4u;
			#else
			            1u;
			#endif
			    };
			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<std::uint32_t>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512F__) || defined(__MIC__)
			            // addition (AVX512F,KNC): vpaddd / _mm512_mask_add_epi32
			            // subtraction (AVX512F,KNC): vpsubd / _mm512_mask_sub_epi32
			            // multiplication: ?
			            16u;
			#elif defined(__AVX2__)
			            // addition (AVX2): vpaddd / _mm256_add_epi32
			            // subtraction (AVX2): vpsubd / _mm256_sub_epi32
			            // multiplication: ?
			            8u;
			#elif defined(__SSE2__)
			            // addition (SSE2): paddd / _mm_add_epi32
			            // subtraction (SSE2): psubd / _mm_sub_epi32
			            // multiplication: ?
			            4u;
			#elif defined(__ARM_NEON__)
			            4u;
			#elif defined(__ALTIVEC__)
			            4u;
			#else
			            1u;
			#endif
			    };
			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<std::int64_t>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512F__)
			            // addition (AVX512F): vpaddq / _mm512_mask_add_epi64
			            // subtraction (AVX512F): vpsubq / _mm512_mask_sub_epi64
			            // multiplication (AVX512DQ): vpmullq / _mm512_mask_mullo_epi64
			            8u;
			#elif defined(__AVX2__)
			            // addition (AVX2): vpaddq / _mm256_add_epi64
			            // subtraction (AVX2): vpsubq / _mm256_sub_epi64
			            // multiplication: -
			            4u;
			#elif defined(__SSE2__)
			            // addition (SSE2): paddq / _mm_add_epi64
			            // subtraction (SSE2): psubq / _mm_sub_epi64
			            // multiplication: -
			            2u;
			#elif defined(__ARM_NEON__)
			            2u;
			#else
			            1u;
			#endif
			    };
			    // Number of elements of the given type that can be processed in parallel in a vector register.
			    template<>
			    struct GetVectorizationSizeElems<std::uint64_t>
			    {
			        static constexpr std::size_t value =
			#if defined(__AVX512F__)
			            // addition (AVX512F): vpaddq / _mm512_mask_add_epi64
			            // subtraction (AVX512F): vpsubq / _mm512_mask_sub_epi64
			            // multiplication: ?
			            8u;
			#elif defined(__AVX2__)
			            // addition (AVX2): vpaddq / _mm256_add_epi64
			            // subtraction (AVX2): vpsubq / _mm256_sub_epi64
			            // multiplication: ?
			            4u;
			#elif defined(__SSE2__)
			            // addition (SSE2): paddq / _mm_add_epi64
			            // subtraction (SSE2): psubq / _mm_sub_epi64
			            // multiplication: ?
			            2u;
			#elif defined(__ARM_NEON__)
			            2u;
			#else
			            1u;
			#endif
			    };
			} // namespace alpaka::core::vectorization
			// ==
			// == ./include/alpaka/core/Vectorize.hpp ==
			// ============================================================================


		// #include <array>    // amalgamate: file already included
		// #include <cstdint>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included

		namespace alpaka
		{
		    namespace detail
		    {
		        //! "namespace" for static constexpr members that should be in BlockSharedMemDynMember
		        //! but cannot be because having a static const member breaks GCC 10
		        //! OpenMP target: type not mappable.
		        template<std::size_t TStaticAllocKiB>
		        struct BlockSharedMemDynMemberStatic
		        {
		            //! Storage size in bytes
		            static constexpr std::uint32_t staticAllocBytes = static_cast<std::uint32_t>(TStaticAllocKiB << 10u);
		        };
		    } // namespace detail

		#if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
		#    pragma warning(push)
		#    pragma warning(disable : 4324) // warning C4324: structure was padded due to alignment specifier
		#endif
		    //! Dynamic block shared memory provider using fixed-size
		    //! member array to allocate memory on the stack or in shared
		    //! memory.
		    template<std::size_t TStaticAllocKiB = BlockSharedDynMemberAllocKiB>
		    class alignas(core::vectorization::defaultAlignment) BlockSharedMemDynMember
		        : public concepts::Implements<ConceptBlockSharedDyn, BlockSharedMemDynMember<TStaticAllocKiB>>
		    {
		    public:
		        BlockSharedMemDynMember(std::size_t sizeBytes) : m_dynPitch(getPitch(sizeBytes))
		        {
		            ALPAKA_ASSERT_OFFLOAD(static_cast<std::uint32_t>(sizeBytes) <= staticAllocBytes());
		        }

		        auto dynMemBegin() const -> uint8_t*
		        {
		            return std::data(m_mem);
		        }

		        /*! \return the pointer to the begin of data after the portion allocated as dynamical shared memory.
		         */
		        auto staticMemBegin() const -> uint8_t*
		        {
		            return std::data(m_mem) + m_dynPitch;
		        }

		        /*! \return the remaining capacity for static block shared memory,
		                    returns a 32-bit type for register efficiency on GPUs
		            */
		        auto staticMemCapacity() const -> std::uint32_t
		        {
		            return staticAllocBytes() - m_dynPitch;
		        }

		        //! \return size of statically allocated memory available for both
		        //!         dynamic and static shared memory. Value is of a 32-bit type
		        //!         for register efficiency on GPUs
		        static constexpr auto staticAllocBytes() -> std::uint32_t
		        {
		            return detail::BlockSharedMemDynMemberStatic<TStaticAllocKiB>::staticAllocBytes;
		        }

		    private:
		        static auto getPitch(std::size_t sizeBytes) -> std::uint32_t
		        {
		            constexpr auto alignment = core::vectorization::defaultAlignment;
		            return static_cast<std::uint32_t>((sizeBytes / alignment + (sizeBytes % alignment > 0u)) * alignment);
		        }

		        mutable std::array<uint8_t, detail::BlockSharedMemDynMemberStatic<TStaticAllocKiB>::staticAllocBytes> m_mem;
		        std::uint32_t m_dynPitch;
		    };
		#if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
		#    pragma warning(pop)
		#endif

		    namespace trait
		    {
		        template<typename T, std::size_t TStaticAllocKiB>
		        struct GetDynSharedMem<T, BlockSharedMemDynMember<TStaticAllocKiB>>
		        {
		#if BOOST_COMP_GNUC
		#    pragma GCC diagnostic push
		#    pragma GCC diagnostic ignored                                                                                    \
		        "-Wcast-align" // "cast from 'unsigned char*' to 'unsigned int*' increases required alignment of target type"
		#endif
		            static auto getMem(BlockSharedMemDynMember<TStaticAllocKiB> const& mem) -> T*
		            {
		                static_assert(
		                    core::vectorization::defaultAlignment >= alignof(T),
		                    "Unable to get block shared dynamic memory for types with alignment higher than "
		                    "defaultAlignment!");
		                return reinterpret_cast<T*>(mem.dynMemBegin());
		            }
		#if BOOST_COMP_GNUC
		#    pragma GCC diagnostic pop
		#endif
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/block/shared/dyn/BlockSharedMemDynMember.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/block/shared/st/BlockSharedMemStMember.hpp ==
		// ==
		/* Copyright 2022 Jeffrey Kelling, Rene Widera, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
			// ============================================================================
			// == ./include/alpaka/block/shared/st/Traits.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Erik Zenker, Matthias Werner, René Widera, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			namespace alpaka
			{
			    struct ConceptBlockSharedSt
			    {
			    };

			    //! The block shared static memory operation trait.
			    namespace trait
			    {
			        //! The block shared static memory variable allocation operation trait.
			        template<typename T, std::size_t TuniqueId, typename TBlockSharedMemSt, typename TSfinae = void>
			        struct DeclareSharedVar;
			        //! The block shared static memory free operation trait.
			        template<typename TBlockSharedMemSt, typename TSfinae = void>
			        struct FreeSharedVars;
			    } // namespace trait

			    //! Declare a block shared variable.
			    //!
			    //! The variable is uninitialized and not default constructed!
			    //! The variable can be accessed by all threads within a block.
			    //! Access to the variable is not thread safe.
			    //!
			    //! \tparam T The element type.
			    //! \tparam TuniqueId id those is unique inside a kernel
			    //! \tparam TBlockSharedMemSt The block shared allocator implementation type.
			    //! \param blockSharedMemSt The block shared allocator implementation.
			    //! \return Uninitialized variable stored in shared memory.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, std::size_t TuniqueId, typename TBlockSharedMemSt>
			    ALPAKA_FN_ACC auto declareSharedVar(TBlockSharedMemSt const& blockSharedMemSt) -> T&
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptBlockSharedSt, TBlockSharedMemSt>;
			        return trait::DeclareSharedVar<T, TuniqueId, ImplementationBase>::declareVar(blockSharedMemSt);
			    }

			    //! Frees all memory used by block shared variables.
			    //!
			    //! \tparam TBlockSharedMemSt The block shared allocator implementation type.
			    //! \param blockSharedMemSt The block shared allocator implementation.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TBlockSharedMemSt>
			    ALPAKA_FN_ACC auto freeSharedVars(TBlockSharedMemSt& blockSharedMemSt) -> void
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptBlockSharedSt, TBlockSharedMemSt>;
			        trait::FreeSharedVars<ImplementationBase>::freeVars(blockSharedMemSt);
			    }
			} // namespace alpaka
			// ==
			// == ./include/alpaka/block/shared/st/Traits.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/block/shared/st/detail/BlockSharedMemStMemberImpl.hpp ==
			// ==
			/* Copyright 2022 Jeffrey Kelling, Rene Widera, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/block/shared/st/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Vectorize.hpp"    // amalgamate: file already expanded

			// #include <algorithm>    // amalgamate: file already included
			// #include <cstdint>    // amalgamate: file already included
			#include <functional>
			#include <limits>
			// #include <type_traits>    // amalgamate: file already included

			namespace alpaka::detail
			{
			    //! Implementation of static block shared memory provider.
			    //!
			    //! externally allocated fixed-size memory, likely provided by BlockSharedMemDynMember.
			    template<std::size_t TMinDataAlignBytes = core::vectorization::defaultAlignment>
			    class BlockSharedMemStMemberImpl
			    {
			        struct MetaData
			        {
			            //! Unique id if the next data chunk.
			            std::uint32_t id = std::numeric_limits<std::uint32_t>::max();
			            //! Offset to the next meta data header, relative to m_mem.
			            //! To access the meta data header the offset must by aligned first.
			            std::uint32_t offset = 0;
			        };

			        static constexpr std::uint32_t metaDataSize = sizeof(MetaData);

			    public:
			#ifndef NDEBUG
			        BlockSharedMemStMemberImpl(std::uint8_t* mem, std::size_t capacity)
			            : m_mem(mem)
			            , m_capacity(static_cast<std::uint32_t>(capacity))
			        {
			            ALPAKA_ASSERT_OFFLOAD((m_mem == nullptr) == (m_capacity == 0u));
			        }
			#else
			        BlockSharedMemStMemberImpl(std::uint8_t* mem, std::size_t) : m_mem(mem)
			        {
			        }
			#endif

			        template<typename T>
			        void alloc(std::uint32_t id) const
			        {
			            // Add meta data chunk in front of the user data
			            m_allocdBytes = varChunkEnd<MetaData>(m_allocdBytes);
			            ALPAKA_ASSERT_OFFLOAD(m_allocdBytes <= m_capacity);
			            auto* meta = getLatestVarPtr<MetaData>();

			            // Allocate variable
			            m_allocdBytes = varChunkEnd<T>(m_allocdBytes);
			            ALPAKA_ASSERT_OFFLOAD(m_allocdBytes <= m_capacity);

			            // Update meta data with id and offset for the allocated variable.
			            meta->id = id;
			            meta->offset = m_allocdBytes;
			        }

			#if BOOST_COMP_GNUC
			#    pragma GCC diagnostic push
			#    pragma GCC diagnostic ignored                                                                                    \
			        "-Wcast-align" // "cast from 'unsigned char*' to 'unsigned int*' increases required alignment of target type"
			#endif

			        //! Give the pointer to an exiting variable
			        //!
			        //! @tparam T type of the variable
			        //! @param id unique id of the variable
			        //! @return nullptr if variable with id not exists
			        template<typename T>
			        auto getVarPtr(std::uint32_t id) const -> T*
			        {
			            // Offset in bytes to the next unaligned meta data header behind the variable.
			            std::uint32_t off = 0;

			            // Iterate over allocated data only
			            while(off < m_allocdBytes)
			            {
			                // Adjust offset to be aligned
			                std::uint32_t const alignedMetaDataOffset
			                    = varChunkEnd<MetaData>(off) - static_cast<std::uint32_t>(sizeof(MetaData));
			                ALPAKA_ASSERT_OFFLOAD(
			                    (alignedMetaDataOffset + static_cast<std::uint32_t>(sizeof(MetaData))) <= m_allocdBytes);
			                auto* metaDataPtr = reinterpret_cast<MetaData*>(m_mem + alignedMetaDataOffset);
			                off = metaDataPtr->offset;

			                if(metaDataPtr->id == id)
			                    return reinterpret_cast<T*>(&m_mem[off - sizeof(T)]);
			            }

			            // Variable not found.
			            return nullptr;
			        }

			        //! Get last allocated variable.
			        template<typename T>
			        auto getLatestVarPtr() const -> T*
			        {
			            return reinterpret_cast<T*>(&m_mem[m_allocdBytes - sizeof(T)]);
			        }

			    private:
			#if BOOST_COMP_GNUC
			#    pragma GCC diagnostic pop
			#endif

			        //! Byte offset to the end of the memory chunk
			        //!
			        //! Calculate bytes required to store a type with a aligned starting address in m_mem.
			        //! Start offset to the origin of the user data chunk can be calculated with `result - sizeof(T)`.
			        //! The padding is always before the origin of the user data chunk and can be zero byte.
			        //!
			        //! \tparam T type should fit into the chunk
			        //! \param byteOffset Current byte offset.
			        //! \result Byte offset to the end of the data chunk, relative to m_mem..
			        template<typename T>
			        auto varChunkEnd(std::uint32_t byteOffset) const -> std::uint32_t
			        {
			            auto const ptr = reinterpret_cast<std::size_t>(m_mem + byteOffset);
			            constexpr size_t align = std::max(TMinDataAlignBytes, alignof(T));
			            std::size_t const newPtrAdress = ((ptr + align - 1u) / align) * align + sizeof(T);
			            return static_cast<uint32_t>(newPtrAdress - reinterpret_cast<std::size_t>(m_mem));
			        }

			        //! Offset in bytes relative to m_mem to next free data area.
			        //! The last aligned before the free area is always a meta data header.
			        mutable std::uint32_t m_allocdBytes = 0u;

			        //! Memory layout
			        //! |Header|Padding|Variable|Padding|Header|....uninitialized Data ....
			        //! Size of padding can be zero if data after padding is already aligned.
			        std::uint8_t* const m_mem;
			#ifndef NDEBUG
			        const std::uint32_t m_capacity;
			#endif
			    };
			} // namespace alpaka::detail
			// ==
			// == ./include/alpaka/block/shared/st/detail/BlockSharedMemStMemberImpl.hpp ==
			// ============================================================================

		// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Vectorize.hpp"    // amalgamate: file already expanded

		// #include <algorithm>    // amalgamate: file already included
		// #include <cstdint>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included

		namespace alpaka
		{
		    //! Static block shared memory provider using a pointer to
		    //! externally allocated fixed-size memory, likely provided by
		    //! BlockSharedMemDynMember.
		    //! \warning This class is not thread safe!
		    template<std::size_t TDataAlignBytes = core::vectorization::defaultAlignment>
		    class BlockSharedMemStMember
		        : public detail::BlockSharedMemStMemberImpl<TDataAlignBytes>
		        , public concepts::Implements<ConceptBlockSharedSt, BlockSharedMemStMember<TDataAlignBytes>>
		    {
		    public:
		        using detail::BlockSharedMemStMemberImpl<TDataAlignBytes>::BlockSharedMemStMemberImpl;
		    };

		    namespace trait
		    {
		        template<typename T, std::size_t TDataAlignBytes, std::size_t TuniqueId>
		        struct DeclareSharedVar<T, TuniqueId, BlockSharedMemStMember<TDataAlignBytes>>
		        {
		            static auto declareVar(BlockSharedMemStMember<TDataAlignBytes> const& smem) -> T&
		            {
		                auto* data = smem.template getVarPtr<T>(TuniqueId);

		                if(!data)
		                {
		                    smem.template alloc<T>(TuniqueId);
		                    data = smem.template getLatestVarPtr<T>();
		                }
		                ALPAKA_ASSERT(data != nullptr);
		                return *data;
		            }
		        };
		        template<std::size_t TDataAlignBytes>
		        struct FreeSharedVars<BlockSharedMemStMember<TDataAlignBytes>>
		        {
		            static auto freeVars(BlockSharedMemStMember<TDataAlignBytes> const&) -> void
		            {
		                // shared memory block data will be reused
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/block/shared/st/BlockSharedMemStMember.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/block/sync/BlockSyncNoOp.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, René Widera, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
			// ============================================================================
			// == ./include/alpaka/block/sync/Traits.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			namespace alpaka
			{
			    struct ConceptBlockSync
			    {
			    };

			    //! The block synchronization traits.
			    namespace trait
			    {
			        //! The block synchronization operation trait.
			        template<typename TBlockSync, typename TSfinae = void>
			        struct SyncBlockThreads;

			        //! The block synchronization and predicate operation trait.
			        template<typename TOp, typename TBlockSync, typename TSfinae = void>
			        struct SyncBlockThreadsPredicate;
			    } // namespace trait

			    //! Synchronizes all threads within the current block (independently for all blocks).
			    //!
			    //! \tparam TBlockSync The block synchronization implementation type.
			    //! \param blockSync The block synchronization implementation.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TBlockSync>
			    ALPAKA_FN_ACC auto syncBlockThreads(TBlockSync const& blockSync) -> void
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptBlockSync, TBlockSync>;
			        trait::SyncBlockThreads<ImplementationBase>::syncBlockThreads(blockSync);
			    }

			    //! The counting function object.
			    struct BlockCount
			    {
			        enum
			        {
			            InitialValue = 0u
			        };

			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename T>
			        ALPAKA_FN_HOST_ACC auto operator()(T const& currentResult, T const& value) const -> T
			        {
			            return currentResult + static_cast<T>(value != static_cast<T>(0));
			        }
			    };
			    //! The logical and function object.
			    struct BlockAnd
			    {
			        enum
			        {
			            InitialValue = 1u
			        };

			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename T>
			        ALPAKA_FN_HOST_ACC auto operator()(T const& currentResult, T const& value) const -> T
			        {
			            return static_cast<T>(currentResult && (value != static_cast<T>(0)));
			        }
			    };
			    //! The logical or function object.
			    struct BlockOr
			    {
			        enum
			        {
			            InitialValue = 0u
			        };

			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename T>
			        ALPAKA_FN_HOST_ACC auto operator()(T const& currentResult, T const& value) const -> T
			        {
			            return static_cast<T>(currentResult || (value != static_cast<T>(0)));
			        }
			    };

			    //! Synchronizes all threads within the current block (independently for all blocks),
			    //! evaluates the predicate for all threads and returns the combination of all the results
			    //! computed via TOp.
			    //!
			    //! \tparam TOp The operation used to combine the predicate values of all threads.
			    //! \tparam TBlockSync The block synchronization implementation type.
			    //! \param blockSync The block synchronization implementation.
			    //! \param predicate The predicate value of the current thread.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TOp, typename TBlockSync>
			    ALPAKA_FN_ACC auto syncBlockThreadsPredicate(TBlockSync const& blockSync, int predicate) -> int
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptBlockSync, TBlockSync>;
			        return trait::SyncBlockThreadsPredicate<TOp, ImplementationBase>::syncBlockThreadsPredicate(
			            blockSync,
			            predicate);
			    }
			} // namespace alpaka
			// ==
			// == ./include/alpaka/block/sync/Traits.hpp ==
			// ============================================================================

		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

		namespace alpaka
		{
		    //! The no op block synchronization.
		    class BlockSyncNoOp : public concepts::Implements<ConceptBlockSync, BlockSyncNoOp>
		    {
		    };

		    namespace trait
		    {
		        template<>
		        struct SyncBlockThreads<BlockSyncNoOp>
		        {
		            ALPAKA_NO_HOST_ACC_WARNING
		            ALPAKA_FN_ACC static auto syncBlockThreads(BlockSyncNoOp const& /* blockSync */) -> void
		            {
		                // Nothing to do.
		            }
		        };

		        template<typename TOp>
		        struct SyncBlockThreadsPredicate<TOp, BlockSyncNoOp>
		        {
		            ALPAKA_NO_HOST_ACC_WARNING
		            ALPAKA_FN_ACC static auto syncBlockThreadsPredicate(BlockSyncNoOp const& /* blockSync */, int predicate)
		                -> int
		            {
		                return predicate;
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/block/sync/BlockSyncNoOp.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/core/DemangleTypeNames.hpp ==
		// ==
		/* Copyright 2022 Andrea Bocci, Antonio Di Pilato
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

		#include <boost/core/demangle.hpp>

		namespace alpaka::core
		{
		#if BOOST_COMP_CLANG
		#    pragma clang diagnostic push
		#    pragma clang diagnostic ignored "-Wexit-time-destructors"
		#    pragma clang diagnostic ignored "-Wmissing-variable-declarations"
		#endif
		    template<typename T>
		    inline const std::string demangled = boost::core::demangle(typeid(T).name());
		#if BOOST_COMP_CLANG
		#    pragma clang diagnostic pop
		#endif
		} // namespace alpaka::core
		// ==
		// == ./include/alpaka/core/DemangleTypeNames.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/idx/bt/IdxBtZero.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/idx/Traits.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include <type_traits>    // amalgamate: file already included
			// #include <utility>    // amalgamate: file already included

			namespace alpaka
			{
			    struct ConceptIdxBt
			    {
			    };
			    struct ConceptIdxGb
			    {
			    };

			    //! The idx trait.
			    namespace trait
			    {
			        //! The idx type trait.
			        template<typename T, typename TSfinae = void>
			        struct IdxType;
			    } // namespace trait

			    template<typename T>
			    using Idx = typename trait::IdxType<T>::type;

			    namespace trait
			    {
			        //! The arithmetic idx type trait specialization.
			        template<typename T>
			        struct IdxType<T, std::enable_if_t<std::is_arithmetic_v<T>>>
			        {
			            using type = std::decay_t<T>;
			        };

			        //! The index get trait.
			        template<typename TIdx, typename TOrigin, typename TUnit, typename TSfinae = void>
			        struct GetIdx;
			    } // namespace trait
			} // namespace alpaka
			// ==
			// == ./include/alpaka/idx/Traits.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/vec/Vec.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Erik Zenker, Matthias Werner, René Widera, Andrea Bocci, Jan Stephan,
			 * Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
				// ============================================================================
				// == ./include/alpaka/core/Align.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, René Widera, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

				// #include <cstddef>    // amalgamate: file already included
				// #include <type_traits>    // amalgamate: file already included

				namespace alpaka::core
				{
				    //! Rounds to the next higher power of two (if not already power of two).
				    // Adapted from llvm/ADT/SmallPtrSet.h
				    template<std::size_t N>
				    struct RoundUpToPowerOfTwo;

				    //! Defines implementation details that should not be used directly by the user.
				    namespace detail
				    {
				        //! Base case for N being a power of two.
				        template<std::size_t N, bool TisPowerTwo>
				        struct RoundUpToPowerOfTwoHelper : std::integral_constant<std::size_t, N>
				        {
				        };
				        //! Case for N not being a power of two.
				        // We could just use NextVal = N+1, but this converges faster.  N|(N-1) sets
				        // the right-most zero bits to one all at once, e.g. 0b0011000 -> 0b0011111.
				        template<std::size_t N>
				        struct RoundUpToPowerOfTwoHelper<N, false>
				            : std::integral_constant<std::size_t, RoundUpToPowerOfTwo<(N | (N - 1)) + 1>::value>
				        {
				        };
				    } // namespace detail
				    template<std::size_t N>
				    struct RoundUpToPowerOfTwo
				        : std::integral_constant<std::size_t, detail::RoundUpToPowerOfTwoHelper<N, (N & (N - 1)) == 0>::value>
				    {
				    };

				    //! The alignment specifics.
				    namespace align
				    {
				        //! Calculates the optimal alignment for data of the given size.
				        template<std::size_t TsizeBytes>
				        struct OptimalAlignment
				            : std::integral_constant<
				                  std::size_t,
				#if BOOST_COMP_GNUC
				                  // GCC does not support alignments larger then 128: "warning: requested alignment 256 is larger
				                  // than 128[-Wattributes]".
				                  (TsizeBytes > 64) ? 128 :
				#endif
				                                    (RoundUpToPowerOfTwo<TsizeBytes>::value)>
				        {
				        };
				    } // namespace align
				} // namespace alpaka::core

				// The optimal alignment for a type is the next higher or equal power of two.
				#define ALPAKA_OPTIMAL_ALIGNMENT(...)                                                                                 \
				    ::alpaka::core::align::OptimalAlignment<sizeof(std::remove_cv_t<__VA_ARGS__>)>::value
				// ==
				// == ./include/alpaka/core/Align.hpp ==
				// ============================================================================

			// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/core/Unreachable.hpp ==
				// ==
				/* Copyright 2022 Jan Stephan, Jeffrey Kelling
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

				//! Before CUDA 11.5 nvcc is unable to correctly identify return statements in 'if constexpr' branches. It will issue
				//! a false warning about a missing return statement unless it is told that the following code section is unreachable.
				//!
				//! \param x A dummy value for the expected return type of the calling function.
				#if(BOOST_COMP_NVCC && BOOST_ARCH_PTX)
				#    if BOOST_LANG_CUDA >= BOOST_VERSION_NUMBER(11, 3, 0)
				#        define ALPAKA_UNREACHABLE(...) __builtin_unreachable()
				#    else
				#        define ALPAKA_UNREACHABLE(...) return __VA_ARGS__
				#    endif
				#elif BOOST_COMP_MSVC
				#    define ALPAKA_UNREACHABLE(...) __assume(false)
				#elif BOOST_COMP_GNUC || BOOST_COMP_CLANG
				#    define ALPAKA_UNREACHABLE(...) __builtin_unreachable()
				#else
				#    define ALPAKA_UNREACHABLE(...)
				#endif
				// ==
				// == ./include/alpaka/core/Unreachable.hpp ==
				// ============================================================================

				// ============================================================================
				// == ./include/alpaka/dim/DimIntegralConst.hpp ==
				// ==
				/* Copyright 2020 Benjamin Worpitz, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
					// ============================================================================
					// == ./include/alpaka/dim/Traits.hpp ==
					// ==
					/* Copyright 2020 Benjamin Worpitz, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					namespace alpaka
					{
					    //! The dimension trait.
					    namespace trait
					    {
					        //! The dimension getter type trait.
					        template<typename T, typename TSfinae = void>
					        struct DimType;
					    } // namespace trait

					    //! The dimension type trait alias template to remove the ::type.
					    template<typename T>
					    using Dim = typename trait::DimType<T>::type;
					} // namespace alpaka
					// ==
					// == ./include/alpaka/dim/Traits.hpp ==
					// ============================================================================


				// #include <type_traits>    // amalgamate: file already included

				namespace alpaka
				{
				    // N(th) dimension(s).
				    template<std::size_t N>
				    using DimInt = std::integral_constant<std::size_t, N>;
				} // namespace alpaka
				// ==
				// == ./include/alpaka/dim/DimIntegralConst.hpp ==
				// ============================================================================

			// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/extent/Traits.hpp ==
				// ==
				/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Andrea Bocci, Jan Stephan, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
				// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
				// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/meta/Fold.hpp ==
					// ==
					/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

					namespace alpaka::meta
					{
					    ALPAKA_NO_HOST_ACC_WARNING
					    template<typename TFnObj, typename T>
					    ALPAKA_FN_HOST_ACC constexpr auto foldr(TFnObj const& /* f */, T const& t) -> T
					    {
					        return t;
					    }
					    ALPAKA_NO_HOST_ACC_WARNING
					    template<typename TFnObj, typename T0, typename T1, typename... Ts>
					    ALPAKA_FN_HOST_ACC constexpr auto foldr(TFnObj const& f, T0 const& t0, T1 const& t1, Ts const&... ts)
					    {
					        return f(t0, foldr(f, t1, ts...));
					    }
					} // namespace alpaka::meta
					// ==
					// == ./include/alpaka/meta/Fold.hpp ==
					// ============================================================================


				// #include <functional>    // amalgamate: file already included
				// #include <type_traits>    // amalgamate: file already included
				// #include <utility>    // amalgamate: file already included

				namespace alpaka
				{
				    //! The extent traits.
				    namespace trait
				    {
				        //! The extent get trait.
				        //!
				        //! If not specialized explicitly it returns 1.
				        template<typename TIdxIntegralConst, typename TExtent, typename TSfinae = void>
				        struct GetExtent
				        {
				            ALPAKA_NO_HOST_ACC_WARNING
				            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const&) -> Idx<TExtent>
				            {
				                return static_cast<Idx<TExtent>>(1);
				            }
				        };

				        //! The extent set trait.
				        template<typename TIdxIntegralConst, typename TExtent, typename TExtentVal, typename TSfinae = void>
				        struct SetExtent;
				    } // namespace trait

				    //! \return The extent in the given dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<std::size_t Tidx, typename TExtent>
				    ALPAKA_FN_HOST_ACC auto getExtent(TExtent const& extent = TExtent()) -> Idx<TExtent>
				    {
				        return trait::GetExtent<DimInt<Tidx>, TExtent>::getExtent(extent);
				    }
				    //! \return The width.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TExtent>
				    ALPAKA_FN_HOST_ACC auto getWidth(TExtent const& extent = TExtent()) -> Idx<TExtent>
				    {
				        return getExtent<Dim<TExtent>::value - 1u>(extent);
				    }
				    //! \return The height.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TExtent>
				    ALPAKA_FN_HOST_ACC auto getHeight(TExtent const& extent = TExtent()) -> Idx<TExtent>
				    {
				        return getExtent<Dim<TExtent>::value - 2u>(extent);
				    }
				    //! \return The depth.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TExtent>
				    ALPAKA_FN_HOST_ACC auto getDepth(TExtent const& extent = TExtent()) -> Idx<TExtent>
				    {
				        return getExtent<Dim<TExtent>::value - 3u>(extent);
				    }

				    namespace detail
				    {
				        ALPAKA_NO_HOST_ACC_WARNING
				        template<typename TExtent, size_t... TIndices>
				        ALPAKA_FN_HOST_ACC auto getExtentProductInternal(
				            TExtent const& extent,
				            std::index_sequence<TIndices...> const& /* indices */) -> Idx<TExtent>
				        {
				            return (getExtent<TIndices>(extent) * ... * Idx<TExtent>(1u));
				        }
				    } // namespace detail

				    //! \return The product of the extent.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TExtent>
				    ALPAKA_FN_HOST_ACC auto getExtentProduct(TExtent const& extent = TExtent()) -> Idx<TExtent>
				    {
				        using IdxSequence = std::make_index_sequence<Dim<TExtent>::value>;
				        return detail::getExtentProductInternal(extent, IdxSequence());
				    }

				    //! Sets the extent in the given dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<std::size_t Tidx, typename TExtent, typename TExtentVal>
				    ALPAKA_FN_HOST_ACC auto setExtent(TExtent& extent, TExtentVal const& extentVal) -> void
				    {
				        trait::SetExtent<DimInt<Tidx>, TExtent, TExtentVal>::setExtent(extent, extentVal);
				    }
				    //! Sets the width.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TExtent, typename TWidth>
				    ALPAKA_FN_HOST_ACC auto setWidth(TExtent& extent, TWidth const& width) -> void
				    {
				        setExtent<Dim<TExtent>::value - 1u>(extent, width);
				    }
				    //! Sets the height.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TExtent, typename THeight>
				    ALPAKA_FN_HOST_ACC auto setHeight(TExtent& extent, THeight const& height) -> void
				    {
				        setExtent<Dim<TExtent>::value - 2u>(extent, height);
				    }
				    //! Sets the depth.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TExtent, typename TDepth>
				    ALPAKA_FN_HOST_ACC auto setDepth(TExtent& extent, TDepth const& depth) -> void
				    {
				        setExtent<Dim<TExtent>::value - 3u>(extent, depth);
				    }

				    // Trait specializations for unsigned integral types.
				    namespace trait
				    {
				        //! The unsigned integral width get trait specialization.
				        template<typename TExtent>
				        struct GetExtent<DimInt<0u>, TExtent, std::enable_if_t<std::is_integral_v<TExtent>>>
				        {
				            ALPAKA_NO_HOST_ACC_WARNING
				            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const& extent) -> Idx<TExtent>
				            {
				                return extent;
				            }
				        };
				        //! The unsigned integral width set trait specialization.
				        template<typename TExtent, typename TExtentVal>
				        struct SetExtent<DimInt<0u>, TExtent, TExtentVal, std::enable_if_t<std::is_integral_v<TExtent>>>
				        {
				            ALPAKA_NO_HOST_ACC_WARNING
				            ALPAKA_FN_HOST_ACC static auto setExtent(TExtent const& extent, TExtentVal const& extentVal) -> void
				            {
				                extent = extentVal;
				            }
				        };
				    } // namespace trait
				} // namespace alpaka
				// ==
				// == ./include/alpaka/extent/Traits.hpp ==
				// ============================================================================

			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/meta/Fold.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/meta/Functional.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

				namespace alpaka::meta
				{
				    template<typename T>
				    struct min
				    {
				        ALPAKA_NO_HOST_ACC_WARNING
				        ALPAKA_FN_HOST_ACC
				        constexpr auto operator()(const T& lhs, const T& rhs) const
				        {
				            return (lhs < rhs) ? lhs : rhs;
				        }
				    };

				    template<typename T>
				    struct max
				    {
				        ALPAKA_NO_HOST_ACC_WARNING
				        ALPAKA_FN_HOST_ACC
				        constexpr auto operator()(const T& lhs, const T& rhs) const
				        {
				            return (lhs > rhs) ? lhs : rhs;
				        }
				    };
				} // namespace alpaka::meta
				// ==
				// == ./include/alpaka/meta/Functional.hpp ==
				// ============================================================================

				// ============================================================================
				// == ./include/alpaka/meta/IntegerSequence.hpp ==
				// ==
				/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
					// ============================================================================
					// == ./include/alpaka/meta/Set.hpp ==
					// ==
					/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include <utility>    // amalgamate: file already included

					namespace alpaka::meta
					{
					    namespace detail
					    {
					        //! Empty dependent type.
					        template<typename T>
					        struct Empty
					        {
					        };

					        template<typename... Ts>
					        struct IsParameterPackSetImpl;
					        template<>
					        struct IsParameterPackSetImpl<>
					        {
					            static constexpr bool value = true;
					        };
					        // Based on code by Roland Bock: https://gist.github.com/rbock/ad8eedde80c060132a18
					        // Linearly inherits from empty<T> and checks if it has already inherited from this type.
					        template<typename T, typename... Ts>
					        struct IsParameterPackSetImpl<T, Ts...>
					            : public IsParameterPackSetImpl<Ts...>
					            , public virtual Empty<T>
					        {
					            using Base = IsParameterPackSetImpl<Ts...>;

					            static constexpr bool value = Base::value && !std::is_base_of_v<Empty<T>, Base>;
					        };
					    } // namespace detail
					    //! Trait that tells if the parameter pack contains only unique (no equal) types.
					    template<typename... Ts>
					    using IsParameterPackSet = detail::IsParameterPackSetImpl<Ts...>;

					    namespace detail
					    {
					        template<typename TList>
					        struct IsSetImpl;
					        template<template<typename...> class TList, typename... Ts>
					        struct IsSetImpl<TList<Ts...>>
					        {
					            static constexpr bool value = IsParameterPackSet<Ts...>::value;
					        };
					    } // namespace detail
					    //! Trait that tells if the template contains only unique (no equal) types.
					    template<typename TList>
					    using IsSet = detail::IsSetImpl<TList>;
					} // namespace alpaka::meta
					// ==
					// == ./include/alpaka/meta/Set.hpp ==
					// ============================================================================


				// #include <cstddef>    // amalgamate: file already included
				// #include <type_traits>    // amalgamate: file already included
				// #include <utility>    // amalgamate: file already included

				namespace alpaka::meta
				{
				    namespace detail
				    {
				        template<typename TDstType, typename TIntegerSequence>
				        struct ConvertIntegerSequence;
				        template<typename TDstType, typename T, T... Tvals>
				        struct ConvertIntegerSequence<TDstType, std::integer_sequence<T, Tvals...>>
				        {
				            using type = std::integer_sequence<TDstType, static_cast<TDstType>(Tvals)...>;
				        };
				    } // namespace detail
				    template<typename TDstType, typename TIntegerSequence>
				    using ConvertIntegerSequence = typename detail::ConvertIntegerSequence<TDstType, TIntegerSequence>::type;

				    namespace detail
				    {
				        template<bool TisSizeNegative, bool TbIsBegin, typename T, T Tbegin, typename TIntCon, typename TIntSeq>
				        struct MakeIntegerSequenceHelper
				        {
				            static_assert(!TisSizeNegative, "MakeIntegerSequence<T, N> requires N to be non-negative.");
				        };
				        template<typename T, T Tbegin, T... Tvals>
				        struct MakeIntegerSequenceHelper<
				            false,
				            true,
				            T,
				            Tbegin,
				            std::integral_constant<T, Tbegin>,
				            std::integer_sequence<T, Tvals...>>
				        {
				            using type = std::integer_sequence<T, Tvals...>;
				        };
				        template<typename T, T Tbegin, T TIdx, T... Tvals>
				        struct MakeIntegerSequenceHelper<
				            false,
				            false,
				            T,
				            Tbegin,
				            std::integral_constant<T, TIdx>,
				            std::integer_sequence<T, Tvals...>>
				        {
				            using type = typename MakeIntegerSequenceHelper<
				                false,
				                TIdx == (Tbegin + 1),
				                T,
				                Tbegin,
				                std::integral_constant<T, TIdx - 1>,
				                std::integer_sequence<T, TIdx - 1, Tvals...>>::type;
				        };
				    } // namespace detail

				    template<typename T, T Tbegin, T Tsize>
				    using MakeIntegerSequenceOffset = typename detail::MakeIntegerSequenceHelper<
				        (Tsize < 0),
				        (Tsize == 0),
				        T,
				        Tbegin,
				        std::integral_constant<T, Tbegin + Tsize>,
				        std::integer_sequence<T>>::type;

				    //! Checks if the integral values are unique.
				    template<typename T, T... Tvals>
				    struct IntegralValuesUnique
				    {
				        static constexpr bool value = meta::IsParameterPackSet<std::integral_constant<T, Tvals>...>::value;
				    };

				    //! Checks if the values in the index sequence are unique.
				    template<typename TIntegerSequence>
				    struct IntegerSequenceValuesUnique;
				    //! Checks if the values in the index sequence are unique.
				    template<typename T, T... Tvals>
				    struct IntegerSequenceValuesUnique<std::integer_sequence<T, Tvals...>>
				    {
				        static constexpr bool value = IntegralValuesUnique<T, Tvals...>::value;
				    };

				    //! Checks if the integral values are within the given range.
				    template<typename T, T Tmin, T Tmax, T... Tvals>
				    struct IntegralValuesInRange;
				    //! Checks if the integral values are within the given range.
				    template<typename T, T Tmin, T Tmax>
				    struct IntegralValuesInRange<T, Tmin, Tmax>
				    {
				        static constexpr bool value = true;
				    };
				    //! Checks if the integral values are within the given range.
				    template<typename T, T Tmin, T Tmax, T I, T... Tvals>
				    struct IntegralValuesInRange<T, Tmin, Tmax, I, Tvals...>
				    {
				        static constexpr bool value
				            = (I >= Tmin) && (I <= Tmax) && IntegralValuesInRange<T, Tmin, Tmax, Tvals...>::value;
				    };

				    //! Checks if the values in the index sequence are within the given range.
				    template<typename TIntegerSequence, typename T, T Tmin, T Tmax>
				    struct IntegerSequenceValuesInRange;
				    //! Checks if the values in the index sequence are within the given range.
				    template<typename T, T... Tvals, T Tmin, T Tmax>
				    struct IntegerSequenceValuesInRange<std::integer_sequence<T, Tvals...>, T, Tmin, Tmax>
				    {
				        static constexpr bool value = IntegralValuesInRange<T, Tmin, Tmax, Tvals...>::value;
				    };
				} // namespace alpaka::meta
				// ==
				// == ./include/alpaka/meta/IntegerSequence.hpp ==
				// ============================================================================

				// ============================================================================
				// == ./include/alpaka/offset/Traits.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
				// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
				// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded

				// #include <type_traits>    // amalgamate: file already included

				namespace alpaka
				{
				    //! The offset traits.
				    namespace trait
				    {
				        //! The x offset get trait.
				        //!
				        //! If not specialized explicitly it returns 0.
				        template<typename TIdx, typename TOffsets, typename TSfinae = void>
				        struct GetOffset
				        {
				            ALPAKA_NO_HOST_ACC_WARNING
				            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const&) -> Idx<TOffsets>
				            {
				                return static_cast<Idx<TOffsets>>(0);
				            }
				        };

				        //! The x offset set trait.
				        template<typename TIdx, typename TOffsets, typename TOffset, typename TSfinae = void>
				        struct SetOffset;
				    } // namespace trait

				    //! \return The offset in the given dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<std::size_t Tidx, typename TOffsets>
				    ALPAKA_FN_HOST_ACC auto getOffset(TOffsets const& offsets) -> Idx<TOffsets>
				    {
				        return trait::GetOffset<DimInt<Tidx>, TOffsets>::getOffset(offsets);
				    }
				    //! \return The offset in x dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TOffsets>
				    ALPAKA_FN_HOST_ACC auto getOffsetX(TOffsets const& offsets = TOffsets()) -> Idx<TOffsets>
				    {
				        return getOffset<Dim<TOffsets>::value - 1u>(offsets);
				    }
				    //! \return The offset in y dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TOffsets>
				    ALPAKA_FN_HOST_ACC auto getOffsetY(TOffsets const& offsets = TOffsets()) -> Idx<TOffsets>
				    {
				        return getOffset<Dim<TOffsets>::value - 2u>(offsets);
				    }
				    //! \return The offset in z dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TOffsets>
				    ALPAKA_FN_HOST_ACC auto getOffsetZ(TOffsets const& offsets = TOffsets()) -> Idx<TOffsets>
				    {
				        return getOffset<Dim<TOffsets>::value - 3u>(offsets);
				    }

				    //! Sets the offset in the given dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<std::size_t Tidx, typename TOffsets, typename TOffset>
				    ALPAKA_FN_HOST_ACC auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
				    {
				        trait::SetOffset<DimInt<Tidx>, TOffsets, TOffset>::setOffset(offsets, offset);
				    }
				    //! Sets the offset in x dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TOffsets, typename TOffset>
				    ALPAKA_FN_HOST_ACC auto setOffsetX(TOffsets const& offsets, TOffset const& offset) -> void
				    {
				        setOffset<Dim<TOffsets>::value - 1u>(offsets, offset);
				    }
				    //! Sets the offset in y dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TOffsets, typename TOffset>
				    ALPAKA_FN_HOST_ACC auto setOffsetY(TOffsets const& offsets, TOffset const& offset) -> void
				    {
				        setOffset<Dim<TOffsets>::value - 2u>(offsets, offset);
				    }
				    //! Sets the offset in z dimension.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TOffsets, typename TOffset>
				    ALPAKA_FN_HOST_ACC auto setOffsetZ(TOffsets const& offsets, TOffset const& offset) -> void
				    {
				        setOffset<Dim<TOffsets>::value - 3u>(offsets, offset);
				    }

				    // Trait specializations for unsigned integral types.
				    namespace trait
				    {
				        //! The unsigned integral x offset get trait specialization.
				        template<typename TOffsets>
				        struct GetOffset<DimInt<0u>, TOffsets, std::enable_if_t<std::is_integral_v<TOffsets>>>
				        {
				            ALPAKA_NO_HOST_ACC_WARNING
				            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const& offset) -> Idx<TOffsets>
				            {
				                return offset;
				            }
				        };
				        //! The unsigned integral x offset set trait specialization.
				        template<typename TOffsets, typename TOffset>
				        struct SetOffset<DimInt<0u>, TOffsets, TOffset, std::enable_if_t<std::is_integral_v<TOffsets>>>
				        {
				            ALPAKA_NO_HOST_ACC_WARNING
				            ALPAKA_FN_HOST_ACC static auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
				            {
				                offsets = offset;
				            }
				        };
				    } // namespace trait
				} // namespace alpaka
				// ==
				// == ./include/alpaka/offset/Traits.hpp ==
				// ============================================================================

				// ============================================================================
				// == ./include/alpaka/vec/Traits.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */


				// #pragma once
				// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/meta/IntegerSequence.hpp"    // amalgamate: file already expanded
				// #include "alpaka/offset/Traits.hpp"    // amalgamate: file already expanded

				// #include <utility>    // amalgamate: file already included

				namespace alpaka
				{
				    //! The vec traits.
				    namespace trait
				    {
				        //! Trait for selecting a sub-vector.
				        template<typename TVec, typename TIndexSequence, typename TSfinae = void>
				        struct SubVecFromIndices;

				        //! Trait for casting a vector.
				        template<typename TVal, typename TVec, typename TSfinae = void>
				        struct CastVec;

				        //! Trait for reversing a vector.
				        template<typename TVec, typename TSfinae = void>
				        struct ReverseVec;

				        //! Trait for concatenating two vectors.
				        template<typename TVecL, typename TVecR, typename TSfinae = void>
				        struct ConcatVec;
				    } // namespace trait

				    //! Builds a new vector by selecting the elements of the source vector in the given order.
				    //! Repeating and swizzling elements is allowed.
				    //! \return The sub-vector consisting of the elements specified by the indices.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TIndexSequence, typename TVec>
				    ALPAKA_FN_HOST_ACC constexpr auto subVecFromIndices(TVec const& vec)
				    {
				        return trait::SubVecFromIndices<TVec, TIndexSequence>::subVecFromIndices(vec);
				    }
				    //! \tparam TVec has to specialize SubVecFromIndices.
				    //! \return The sub-vector consisting of the first N elements of the source vector.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TSubDim, typename TVec>
				    ALPAKA_FN_HOST_ACC constexpr auto subVecBegin(TVec const& vec)
				    {
				        static_assert(
				            TSubDim::value <= Dim<TVec>::value,
				            "The sub-Vec has to be smaller (or same size) then the original Vec.");

				        //! A sequence of integers from 0 to dim-1.
				        using IdxSubSequence = std::make_integer_sequence<std::size_t, TSubDim::value>;
				        return subVecFromIndices<IdxSubSequence>(vec);
				    }
				    //! \tparam TVec has to specialize SubVecFromIndices.
				    //! \return The sub-vector consisting of the last N elements of the source vector.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TSubDim, typename TVec>
				    ALPAKA_FN_HOST_ACC constexpr auto subVecEnd(TVec const& vec)
				    {
				        static_assert(
				            TSubDim::value <= Dim<TVec>::value,
				            "The sub-Vec has to be smaller (or same size) then the original Vec.");

				        constexpr std::size_t idxOffset = Dim<TVec>::value - TSubDim::value;

				        //! A sequence of integers from 0 to dim-1.
				        using IdxSubSequence = meta::MakeIntegerSequenceOffset<std::size_t, idxOffset, TSubDim::value>;
				        return subVecFromIndices<IdxSubSequence>(vec);
				    }

				    //! \return The casted vector.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TVal, typename TVec>
				    ALPAKA_FN_HOST_ACC constexpr auto castVec(TVec const& vec)
				    {
				        return trait::CastVec<TVal, TVec>::castVec(vec);
				    }

				    //! \return The reverseVec vector.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TVec>
				    ALPAKA_FN_HOST_ACC constexpr auto reverseVec(TVec const& vec)
				    {
				        return trait::ReverseVec<TVec>::reverseVec(vec);
				    }

				    //! \return The concatenated vector.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TVecL, typename TVecR>
				    ALPAKA_FN_HOST_ACC constexpr auto concatVec(TVecL const& vecL, TVecR const& vecR)
				    {
				        return trait::ConcatVec<TVecL, TVecR>::concatVec(vecL, vecR);
				    }
				} // namespace alpaka
				// ==
				// == ./include/alpaka/vec/Traits.hpp ==
				// ============================================================================


			// #include <algorithm>    // amalgamate: file already included
			// #include <cstdint>    // amalgamate: file already included
			// #include <limits>    // amalgamate: file already included
			#include <ostream>
			// #include <tuple>    // amalgamate: file already included
			// #include <type_traits>    // amalgamate: file already included
			// #include <utility>    // amalgamate: file already included

			namespace alpaka
			{
			    template<typename TDim, typename TVal>
			    class Vec;

			    //! Single value constructor helper.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<
			        typename TDim,
			        template<std::size_t>
			        class TTFnObj,
			        typename... TArgs,
			        typename TIdxSize,
			        TIdxSize... TIndices>
			    ALPAKA_FN_HOST_ACC auto createVecFromIndexedFnArbitrary(
			        std::integer_sequence<TIdxSize, TIndices...> const& /* indices */,
			        TArgs&&... args)
			    {
			        return Vec<TDim, decltype(TTFnObj<0>::create(std::forward<TArgs>(args)...))>(
			            (TTFnObj<TIndices>::create(std::forward<TArgs>(args)...))...);
			    }
			    //! Creator using func<idx>(args...) to initialize all values of the vector.
			    //! The idx is in the range [0, TDim].
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TDim, template<std::size_t> class TTFnObj, typename... TArgs>
			    ALPAKA_FN_HOST_ACC auto createVecFromIndexedFn(TArgs&&... args)
			    {
			        using IdxSequence = std::make_integer_sequence<typename TDim::value_type, TDim::value>;
			        return createVecFromIndexedFnArbitrary<TDim, TTFnObj>(IdxSequence(), std::forward<TArgs>(args)...);
			    }

			    //! Creator using func<idx>(args...) to initialize all values of the vector.
			    //! The idx is in the range [TIdxOffset, TIdxOffset + TDim].
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TDim, template<std::size_t> class TTFnObj, typename TIdxOffset, typename... TArgs>
			    ALPAKA_FN_HOST_ACC auto createVecFromIndexedFnOffset(TArgs&&... args)
			    {
			        using IdxSubSequenceSigned = meta::MakeIntegerSequenceOffset<std::intmax_t, TIdxOffset::value, TDim::value>;
			        using IdxSubSequence = meta::ConvertIntegerSequence<typename TIdxOffset::value_type, IdxSubSequenceSigned>;
			        return createVecFromIndexedFnArbitrary<TDim, TTFnObj>(IdxSubSequence(), std::forward<TArgs>(args)...);
			    }

			    //! A n-dimensional vector.
			    template<typename TDim, typename TVal>
			    class Vec final
			    {
			    public:
			        static_assert(TDim::value >= 0u, "Invalid dimensionality");

			        using Dim = TDim;
			        using Val = TVal;

			    private:
			        //! A sequence of integers from 0 to dim-1.
			        //! This can be used to write compile time indexing algorithms.
			        using IdxSequence = std::make_integer_sequence<std::size_t, TDim::value>;

			    public:
			        ALPAKA_FN_HOST_ACC constexpr Vec() : m_data{}
			        {
			        }

			        //! Value constructor.
			        //! This constructor is only available if the number of parameters matches the vector idx.
			        ALPAKA_NO_HOST_ACC_WARNING
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC >= BOOST_VERSION_NUMBER(11, 3, 0)                                              \
			    && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 4, 0)
			        // This constructor tries to avoid SFINAE, which crashes nvcc 11.3. We also need to have a first
			        // argument, so an unconstrained ctor with forwarding references does not hijack the compiler provided
			        // copy-ctor.
			        template<typename... TArgs>
			        ALPAKA_FN_HOST_ACC constexpr Vec(TVal arg0, TArgs&&... args)
			            : m_data{std::move(arg0), static_cast<TVal>(std::forward<TArgs>(args))...}
			        {
			            static_assert(
			                1 + sizeof...(TArgs) == TDim::value && (std::is_convertible_v<std::decay_t<TArgs>, TVal> && ...),
			                "Wrong number of arguments to Vec constructor or types are not convertible to TVal.");
			        }
			#else
			        template<
			            typename... TArgs,
			            typename = std::enable_if_t<
			                sizeof...(TArgs) == TDim::value && (std::is_convertible_v<std::decay_t<TArgs>, TVal> && ...)>>
			        ALPAKA_FN_HOST_ACC constexpr Vec(TArgs&&... args) : m_data{static_cast<TVal>(std::forward<TArgs>(args))...}
			        {
			        }
			#endif

			        //! \brief Single value constructor.
			        //!
			        //! Creates a vector with all values set to val.
			        //! \param val The initial value.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC static constexpr auto all(TVal const& val) -> Vec<TDim, TVal>
			        {
			            Vec<TDim, TVal> v;
			            for(auto& e : v)
			                e = val;
			            return v;
			        }

			        //! Zero value constructor.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC static constexpr auto zeros() -> Vec<TDim, TVal>
			        {
			            return all(static_cast<TVal>(0));
			        }

			        //! One value constructor.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC static constexpr auto ones() -> Vec<TDim, TVal>
			        {
			            return all(static_cast<TVal>(1));
			        }

			        ALPAKA_FN_HOST_ACC constexpr auto begin() -> TVal*
			        {
			            return m_data;
			        }

			        ALPAKA_FN_HOST_ACC constexpr auto begin() const -> TVal const*
			        {
			            return m_data;
			        }

			        ALPAKA_FN_HOST_ACC constexpr auto end() -> TVal*
			        {
			            return m_data + TDim::value;
			        }

			        ALPAKA_FN_HOST_ACC constexpr auto end() const -> TVal const*
			        {
			            return m_data + TDim::value;
			        }

			        //! Value reference accessor at the given non-unsigned integer index.
			        //! \return A reference to the value at the given index.
			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename TIdx, typename = std::enable_if_t<std::is_integral_v<TIdx>>>
			        ALPAKA_FN_HOST_ACC constexpr auto operator[](TIdx const iIdx) -> TVal&
			        {
			            core::assertValueUnsigned(iIdx);
			            auto const idx = static_cast<typename TDim::value_type>(iIdx);
			            core::assertGreaterThan<TDim>(idx);
			            return m_data[idx];
			        }

			        //! Value accessor at the given non-unsigned integer index.
			        //! \return The value at the given index.
			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename TIdx, typename = std::enable_if_t<std::is_integral_v<TIdx>>>
			        ALPAKA_FN_HOST_ACC constexpr auto operator[](TIdx const iIdx) const -> TVal
			        {
			            core::assertValueUnsigned(iIdx);
			            auto const idx = static_cast<typename TDim::value_type>(iIdx);
			            core::assertGreaterThan<TDim>(idx);
			            return m_data[idx];
			        }

			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename TFnObj, std::size_t... TIndices>
			        [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto foldrByIndices(
			            TFnObj const& f,
			            std::integer_sequence<std::size_t, TIndices...>) const
			        {
			            return meta::foldr(f, (*this)[TIndices]...);
			        }

			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename TFnObj, std::size_t... TIndices>
			        [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto foldrByIndices(
			            TFnObj const& f,
			            std::integer_sequence<std::size_t, TIndices...>,
			            TVal initial) const
			        {
			            return meta::foldr(f, (*this)[TIndices]..., initial);
			        }

			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename TFnObj>
			        [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto foldrAll(TFnObj const& f) const
			        {
			            return foldrByIndices(f, IdxSequence());
			        }

			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename TFnObj>
			        [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto foldrAll(TFnObj const& f, TVal initial) const
			        {
			            return foldrByIndices(f, IdxSequence(), initial);
			        }

			// suppress strange warning produced by nvcc+MSVC in release mode
			#if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
			#    pragma warning(push)
			#    pragma warning(disable : 4702) // unreachable code
			#endif
			        //! \return The product of all values.
			        ALPAKA_NO_HOST_ACC_WARNING
			        [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto prod() const -> TVal
			        {
			            return foldrAll(std::multiplies<TVal>(), TVal(1));
			        }
			#if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
			#    pragma warning(pop)
			#endif
			        //! \return The sum of all values.
			        ALPAKA_NO_HOST_ACC_WARNING
			        [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto sum() const -> TVal
			        {
			            return foldrAll(std::plus<TVal>(), TVal(0));
			        }

			        //! \return The min of all values.
			        ALPAKA_NO_HOST_ACC_WARNING
			        [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto min() const -> TVal
			        {
			            return foldrAll(meta::min<TVal>(), std::numeric_limits<TVal>::max());
			        }

			        //! \return The max of all values.
			        ALPAKA_NO_HOST_ACC_WARNING
			        [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto max() const -> TVal
			        {
			            return foldrAll(meta::max<TVal>(), std::numeric_limits<TVal>::min());
			        }

			        //! \return The index of the minimal element.
			        [[nodiscard]] ALPAKA_FN_HOST constexpr auto minElem() const -> typename TDim::value_type
			        {
			            return static_cast<typename TDim::value_type>(
			                std::distance(std::begin(m_data), std::min_element(std::begin(m_data), std::end(m_data))));
			        }

			        //! \return The index of the maximal element.
			        [[nodiscard]] ALPAKA_FN_HOST constexpr auto maxElem() const -> typename TDim::value_type
			        {
			            return static_cast<typename TDim::value_type>(
			                std::distance(std::begin(m_data), std::max_element(std::begin(m_data), std::end(m_data))));
			        }

			        template<size_t I>
			        ALPAKA_FN_HOST_ACC constexpr auto get() -> TVal&
			        {
			            return (*this)[I];
			        }

			        template<size_t I>
			        [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto get() const -> TVal
			        {
			            return (*this)[I];
			        }

			        //! \return The element-wise sum of two vectors.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC friend constexpr auto operator+(Vec const& p, Vec const& q) -> Vec
			        {
			            Vec r;
			            if constexpr(TDim::value > 0)
			            {
			                for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                    r[i] = p[i] + q[i];
			            }
			            return r;
			        }

			        //! \return The element-wise difference of two vectors.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC friend constexpr auto operator-(Vec const& p, Vec const& q) -> Vec
			        {
			            Vec r;
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 3, 0)
			            if(TDim::value > 0)
			#else
			            if constexpr(TDim::value > 0)
			#endif
			            {
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 3, 0)
			#    pragma diag_suppress = unsigned_compare_with_zero
			#endif
			                for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 3, 0)
			#    pragma diag_default = unsigned_compare_with_zero
			#endif
			                    r[i] = p[i] - q[i];
			            }
			            return r;
			        }

			        //! \return The element-wise product of two vectors.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC friend constexpr auto operator*(Vec const& p, Vec const& q) -> Vec
			        {
			            Vec r;
			            if constexpr(TDim::value > 0)
			            {
			                for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                    r[i] = p[i] * q[i];
			            }
			            return r;
			        }

			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC friend constexpr auto operator==(Vec const& a, Vec const& b) -> bool
			        {
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 3, 0)
			            if(TDim::value > 0)
			#else
			            if constexpr(TDim::value > 0)
			#endif
			            {
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 3, 0)
			#    pragma diag_suppress = unsigned_compare_with_zero
			#endif
			                for(typename TDim::value_type i(0); i < TDim::value; ++i)
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 3, 0)
			#    pragma diag_default = unsigned_compare_with_zero
			#endif
			                {
			                    if(a[i] != b[i])
			                        return false;
			                }
			            }
			            return true;
			        }

			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC friend constexpr auto operator!=(Vec const& a, Vec const& b) -> bool
			        {
			            return !(a == b);
			        }

			        //! \return The element-wise less than relation of two vectors.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC friend constexpr auto operator<(Vec const& p, Vec const& q) -> Vec<TDim, bool>
			        {
			            Vec<TDim, bool> r;
			            if constexpr(TDim::value > 0)
			            {
			                for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                    r[i] = p[i] < q[i];
			            }
			            return r;
			        }

			        //! \return The element-wise less than relation of two vectors.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC friend constexpr auto operator<=(Vec const& p, Vec const& q) -> Vec<TDim, bool>
			        {
			            Vec<TDim, bool> r;
			            if constexpr(TDim::value > 0)
			            {
			                for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                    r[i] = p[i] <= q[i];
			            }
			            return r;
			        }

			        //! \return The element-wise greater than relation of two vectors.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC friend constexpr auto operator>(Vec const& p, Vec const& q) -> Vec<TDim, bool>
			        {
			            Vec<TDim, bool> r;
			            if constexpr(TDim::value > 0)
			            {
			                for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                    r[i] = p[i] > q[i];
			            }
			            return r;
			        }

			        //! \return The element-wise greater equal than relation of two vectors.
			        ALPAKA_NO_HOST_ACC_WARNING
			        ALPAKA_FN_HOST_ACC friend constexpr auto operator>=(Vec const& p, Vec const& q) -> Vec<TDim, bool>
			        {
			            Vec<TDim, bool> r;
			            if constexpr(TDim::value > 0)
			            {
			                for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                    r[i] = p[i] >= q[i];
			            }
			            return r;
			        }

			        ALPAKA_FN_HOST friend constexpr auto operator<<(std::ostream& os, Vec const& v) -> std::ostream&
			        {
			            os << "(";
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 3, 0)
			            if(TDim::value > 0)
			#else
			            if constexpr(TDim::value > 0)
			#endif
			            {
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 3, 0)
			#    pragma diag_suppress = unsigned_compare_with_zero
			#endif
			                for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			#if BOOST_COMP_NVCC && BOOST_COMP_NVCC < BOOST_VERSION_NUMBER(11, 3, 0)
			#    pragma diag_default = unsigned_compare_with_zero
			#endif
			                {
			                    os << v[i];
			                    if(i != TDim::value - 1)
			                        os << ", ";
			                }
			            }
			            else
			                os << ".";
			            os << ")";

			            return os;
			        }

			    private:
			        // Zero sized arrays are not allowed, therefore zero-dimensional vectors have one member.
			        TVal m_data[TDim::value == 0u ? 1u : TDim::value];
			    };

			    template<typename TFirstIndex, typename... TRestIndices>
			    Vec(TFirstIndex&&, TRestIndices&&...) -> Vec<DimInt<1 + sizeof...(TRestIndices)>, std::decay_t<TFirstIndex>>;

			    //! Converts a Vec to a std::array
			    template<typename TDim, typename TVal>
			    ALPAKA_FN_HOST_ACC constexpr auto toArray(Vec<TDim, TVal> const& v) -> std::array<TVal, TDim::value>
			    {
			        std::array<TVal, TDim::value> a{};
			        if constexpr(TDim::value > 0)
			        {
			            for(unsigned i = 0; i < TDim::value; i++)
			                a[i] = v[i];
			        }
			        return a;
			    }

			    //! \return The element-wise minimum of one or more vectors.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<
			        typename TDim,
			        typename TVal,
			        typename... Vecs,
			        typename = std::enable_if_t<(std::is_same_v<Vec<TDim, TVal>, Vecs> && ...)>>
			    ALPAKA_FN_HOST_ACC constexpr auto elementwise_min(Vec<TDim, TVal> const& p, Vecs const&... qs) -> Vec<TDim, TVal>
			    {
			        Vec<TDim, TVal> r;
			        if constexpr(TDim::value > 0)
			        {
			            for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                r[i] = std::min({p[i], qs[i]...});
			        }
			        return r;
			    }

			    //! \return The element-wise maximum of one or more vectors.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<
			        typename TDim,
			        typename TVal,
			        typename... Vecs,
			        typename = std::enable_if_t<(std::is_same_v<Vec<TDim, TVal>, Vecs> && ...)>>
			    ALPAKA_FN_HOST_ACC constexpr auto elementwise_max(Vec<TDim, TVal> const& p, Vecs const&... qs) -> Vec<TDim, TVal>
			    {
			        Vec<TDim, TVal> r;
			        if constexpr(TDim::value > 0)
			        {
			            for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                r[i] = std::max({p[i], qs[i]...});
			        }
			        return r;
			    }

			    namespace trait
			    {
			        //! The Vec dimension get trait specialization.
			        template<typename TDim, typename TVal>
			        struct DimType<Vec<TDim, TVal>>
			        {
			            using type = TDim;
			        };

			        //! The Vec idx type trait specialization.
			        template<typename TDim, typename TVal>
			        struct IdxType<Vec<TDim, TVal>>
			        {
			            using type = TVal;
			        };

			        //! Specialization for selecting a sub-vector.
			        template<typename TDim, typename TVal, std::size_t... TIndices>
			        struct SubVecFromIndices<Vec<TDim, TVal>, std::index_sequence<TIndices...>>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING ALPAKA_FN_HOST_ACC static constexpr auto subVecFromIndices(
			                Vec<TDim, TVal> const& vec) -> Vec<DimInt<sizeof...(TIndices)>, TVal>
			            {
			                if constexpr(std::is_same_v<std::index_sequence<TIndices...>, std::make_index_sequence<TDim::value>>)
			                {
			                    return vec; // Return whole vector.
			                }
			                else
			                {
			                    static_assert(
			                        sizeof...(TIndices) <= TDim::value,
			                        "The sub-vector's dimensionality must be smaller than or equal to the original "
			                        "dimensionality.");
			                    return {vec[TIndices]...}; // Return sub-vector.
			                }
			                ALPAKA_UNREACHABLE({});
			            }
			        };

			        template<typename TValNew, typename TDim, typename TVal>
			        struct CastVec<TValNew, Vec<TDim, TVal>>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static constexpr auto castVec(Vec<TDim, TVal> const& vec) -> Vec<TDim, TValNew>
			            {
			                if constexpr(std::is_same_v<TValNew, TVal>)
			                {
			                    return vec;
			                }
			                else
			                {
			                    Vec<TDim, TValNew> r;
			                    if constexpr(TDim::value > 0)
			                    {
			                        for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                            r[i] = static_cast<TValNew>(vec[i]);
			                    }
			                    return r;
			                }
			                ALPAKA_UNREACHABLE({});
			            }
			        };
			    } // namespace trait

			    namespace trait
			    {
			        //! ReverseVec specialization for Vec.
			        template<typename TDim, typename TVal>
			        struct ReverseVec<Vec<TDim, TVal>>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static constexpr auto reverseVec(Vec<TDim, TVal> const& vec) -> Vec<TDim, TVal>
			            {
			                if constexpr(TDim::value <= 1)
			                {
			                    return vec;
			                }
			                else
			                {
			                    Vec<TDim, TVal> r;
			                    for(typename TDim::value_type i = 0; i < TDim::value; ++i)
			                        r[i] = vec[TDim::value - 1u - i];
			                    return r;
			                }
			                ALPAKA_UNREACHABLE({});
			            }
			        };

			        //! Concatenation specialization for Vec.
			        template<typename TDimL, typename TDimR, typename TVal>
			        struct ConcatVec<Vec<TDimL, TVal>, Vec<TDimR, TVal>>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static constexpr auto concatVec(
			                Vec<TDimL, TVal> const& vecL,
			                Vec<TDimR, TVal> const& vecR) -> Vec<DimInt<TDimL::value + TDimR::value>, TVal>
			            {
			                Vec<DimInt<TDimL::value + TDimR::value>, TVal> r;
			                if constexpr(TDimL::value > 0)
			                {
			                    for(typename TDimL::value_type i = 0; i < TDimL::value; ++i)
			                        r[i] = vecL[i];
			                }
			                if constexpr(TDimR::value > 0)
			                {
			                    for(typename TDimR::value_type i = 0; i < TDimR::value; ++i)
			                        r[TDimL::value + i] = vecR[i];
			                }
			                return r;
			            }
			        };
			    } // namespace trait

			    namespace detail
			    {
			        //! A function object that returns the extent for each index.
			        template<std::size_t Tidx>
			        struct CreateExtent
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TExtent>
			            ALPAKA_FN_HOST_ACC static constexpr auto create(TExtent const& extent) -> Idx<TExtent>
			            {
			                return getExtent<Tidx>(extent);
			            }
			        };
			    } // namespace detail

			    //! \tparam TExtent has to specialize GetExtent.
			    //! \return The extent vector.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TExtent>
			    ALPAKA_FN_HOST_ACC auto constexpr getExtentVec(TExtent const& extent = {}) -> Vec<Dim<TExtent>, Idx<TExtent>>
			    {
			        return createVecFromIndexedFn<Dim<TExtent>, detail::CreateExtent>(extent);
			    }

			    //! \tparam TExtent has to specialize GetExtent.
			    //! \return The extent but only the last TDim elements.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TDim, typename TExtent>
			    ALPAKA_FN_HOST_ACC auto constexpr getExtentVecEnd(TExtent const& extent = {}) -> Vec<TDim, Idx<TExtent>>
			    {
			        static_assert(TDim::value <= Dim<TExtent>::value, "Cannot get more items than the extent holds");

			        using IdxOffset = std::integral_constant<
			            std::intmax_t,
			            static_cast<std::intmax_t>(Dim<TExtent>::value) - static_cast<std::intmax_t>(TDim::value)>;
			        return createVecFromIndexedFnOffset<TDim, detail::CreateExtent, IdxOffset>(extent);
			    }

			    namespace detail
			    {
			        //! A function object that returns the offsets for each index.
			        template<std::size_t Tidx>
			        struct CreateOffset
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TOffsets>
			            ALPAKA_FN_HOST_ACC static constexpr auto create(TOffsets const& offsets) -> Idx<TOffsets>
			            {
			                return getOffset<Tidx>(offsets);
			            }
			        };
			    } // namespace detail

			    //! \tparam TOffsets has to specialize GetOffset.
			    //! \return The offset vector.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TOffsets>
			    ALPAKA_FN_HOST_ACC constexpr auto getOffsetVec(TOffsets const& offsets = {}) -> Vec<Dim<TOffsets>, Idx<TOffsets>>
			    {
			        return createVecFromIndexedFn<Dim<TOffsets>, detail::CreateOffset>(offsets);
			    }

			    //! \tparam TOffsets has to specialize GetOffset.
			    //! \return The offset vector but only the last TDim elements.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TDim, typename TOffsets>
			    ALPAKA_FN_HOST_ACC constexpr auto getOffsetVecEnd(TOffsets const& offsets = {}) -> Vec<TDim, Idx<TOffsets>>
			    {
			        static_assert(TDim::value <= Dim<TOffsets>::value, "Cannot get more items than the offsets hold");

			        using IdxOffset = std::integral_constant<
			            std::size_t,
			            static_cast<std::size_t>(
			                static_cast<std::intmax_t>(Dim<TOffsets>::value) - static_cast<std::intmax_t>(TDim::value))>;
			        return createVecFromIndexedFnOffset<TDim, detail::CreateOffset, IdxOffset>(offsets);
			    }

			    namespace trait
			    {
			        //! The Vec extent get trait specialization.
			        template<typename TIdxIntegralConst, typename TDim, typename TVal>
			        struct GetExtent<
			            TIdxIntegralConst,
			            Vec<TDim, TVal>,
			            std::enable_if_t<(TDim::value > TIdxIntegralConst::value)>>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static constexpr auto getExtent(Vec<TDim, TVal> const& extent) -> TVal
			            {
			                return extent[TIdxIntegralConst::value];
			            }
			        };
			        //! The Vec extent set trait specialization.
			        template<typename TIdxIntegralConst, typename TDim, typename TVal, typename TExtentVal>
			        struct SetExtent<
			            TIdxIntegralConst,
			            Vec<TDim, TVal>,
			            TExtentVal,
			            std::enable_if_t<(TDim::value > TIdxIntegralConst::value)>>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static constexpr auto setExtent(Vec<TDim, TVal>& extent, TExtentVal const& extentVal)
			                -> void
			            {
			                extent[TIdxIntegralConst::value] = extentVal;
			            }
			        };

			        //! The Vec offset get trait specialization.
			        template<typename TIdxIntegralConst, typename TDim, typename TVal>
			        struct GetOffset<
			            TIdxIntegralConst,
			            Vec<TDim, TVal>,
			            std::enable_if_t<(TDim::value > TIdxIntegralConst::value)>>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static constexpr auto getOffset(Vec<TDim, TVal> const& offsets) -> TVal
			            {
			                return offsets[TIdxIntegralConst::value];
			            }
			        };
			        //! The Vec offset set trait specialization.
			        template<typename TIdxIntegralConst, typename TDim, typename TVal, typename TOffset>
			        struct SetOffset<
			            TIdxIntegralConst,
			            Vec<TDim, TVal>,
			            TOffset,
			            std::enable_if_t<(TDim::value > TIdxIntegralConst::value)>>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static constexpr auto setOffset(Vec<TDim, TVal>& offsets, TOffset const& offset) -> void
			            {
			                offsets[TIdxIntegralConst::value] = offset;
			            }
			        };
			    } // namespace trait
			} // namespace alpaka

			#if defined(__clang__)
			#    pragma GCC diagnostic push
			#    pragma GCC diagnostic ignored "-Wmismatched-tags"
			#endif
			namespace std
			{
			    template<typename TDim, typename TVal>
			    struct tuple_size<alpaka::Vec<TDim, TVal>> : integral_constant<size_t, TDim::value>
			    {
			    };

			    template<size_t I, typename TDim, typename TVal>
			    struct tuple_element<I, alpaka::Vec<TDim, TVal>>
			    {
			        using type = TVal;
			    };
			} // namespace std
			#if defined(__clang__)
			#    pragma GCC diagnostic pop
			#endif
			// ==
			// == ./include/alpaka/vec/Vec.hpp ==
			// ============================================================================


		namespace alpaka
		{
		    namespace bt
		    {
		        //! A zero block thread index provider.
		        template<typename TDim, typename TIdx>
		        class IdxBtZero : public concepts::Implements<ConceptIdxBt, IdxBtZero<TDim, TIdx>>
		        {
		        };
		    } // namespace bt

		    namespace trait
		    {
		        //! The zero block thread index provider dimension get trait specialization.
		        template<typename TDim, typename TIdx>
		        struct DimType<bt::IdxBtZero<TDim, TIdx>>
		        {
		            using type = TDim;
		        };

		        //! The zero block thread index provider block thread index get trait specialization.
		        template<typename TDim, typename TIdx>
		        struct GetIdx<bt::IdxBtZero<TDim, TIdx>, origin::Block, unit::Threads>
		        {
		            //! \return The index of the current thread in the block.
		            template<typename TWorkDiv>
		            ALPAKA_FN_HOST static auto getIdx(
		                bt::IdxBtZero<TDim, TIdx> const& /* idx */,
		                TWorkDiv const& /* workDiv */) -> Vec<TDim, TIdx>
		            {
		                return Vec<TDim, TIdx>::zeros();
		            }
		        };

		        //! The zero block thread index idx type trait specialization.
		        template<typename TDim, typename TIdx>
		        struct IdxType<bt::IdxBtZero<TDim, TIdx>>
		        {
		            using type = TIdx;
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/idx/bt/IdxBtZero.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/idx/gb/IdxGbRef.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

		namespace alpaka
		{
		    namespace gb
		    {
		        //! A IdxGbRef grid block index.
		        template<typename TDim, typename TIdx>
		        class IdxGbRef : public concepts::Implements<ConceptIdxGb, IdxGbRef<TDim, TIdx>>
		        {
		        public:
		            IdxGbRef(Vec<TDim, TIdx> const& gridBlockIdx) : m_gridBlockIdx(gridBlockIdx)
		            {
		            }

		            Vec<TDim, TIdx> const& m_gridBlockIdx;
		        };
		    } // namespace gb

		    namespace trait
		    {
		        //! The IdxGbRef grid block index dimension get trait specialization.
		        template<typename TDim, typename TIdx>
		        struct DimType<gb::IdxGbRef<TDim, TIdx>>
		        {
		            using type = TDim;
		        };

		        //! The IdxGbRef grid block index grid block index get trait specialization.
		        template<typename TDim, typename TIdx>
		        struct GetIdx<gb::IdxGbRef<TDim, TIdx>, origin::Grid, unit::Blocks>
		        {
		            //! \return The index of the current block in the grid.
		            template<typename TWorkDiv>
		            ALPAKA_FN_HOST static auto getIdx(gb::IdxGbRef<TDim, TIdx> const& idx, TWorkDiv const& /* workDiv */)
		                -> Vec<TDim, TIdx>
		            {
		                return idx.m_gridBlockIdx;
		            }
		        };

		        //! The IdxGbRef grid block index idx type trait specialization.
		        template<typename TDim, typename TIdx>
		        struct IdxType<gb::IdxGbRef<TDim, TIdx>>
		        {
		            using type = TIdx;
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/idx/gb/IdxGbRef.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/intrinsic/IntrinsicCpu.hpp ==
		// ==
		/* Copyright 2022 Sergei Bastrakov, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/intrinsic/IntrinsicFallback.hpp ==
			// ==
			/* Copyright 2022 Sergei Bastrakov, Jeffrey Kelling, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
				// ============================================================================
				// == ./include/alpaka/intrinsic/Traits.hpp ==
				// ==
				/* Copyright 2022 Sergei Bastrakov, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

				// #include <cstdint>    // amalgamate: file already included
				// #include <type_traits>    // amalgamate: file already included

				namespace alpaka
				{
				    struct ConceptIntrinsic
				    {
				    };

				    //! The intrinsics traits.
				    namespace trait
				    {
				        //! The popcount trait.
				        template<typename TWarp, typename TSfinae = void>
				        struct Popcount;

				        //! The ffs trait.
				        template<typename TWarp, typename TSfinae = void>
				        struct Ffs;
				    } // namespace trait

				    //! Returns the number of 1 bits in the given 32-bit value.
				    //!
				    //! \tparam TIntrinsic The intrinsic implementation type.
				    //! \param intrinsic The intrinsic implementation.
				    //! \param value The input value.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TIntrinsic>
				    ALPAKA_FN_ACC auto popcount(TIntrinsic const& intrinsic, std::uint32_t value) -> std::int32_t
				    {
				        using ImplementationBase = concepts::ImplementationBase<ConceptIntrinsic, TIntrinsic>;
				        return trait::Popcount<ImplementationBase>::popcount(intrinsic, value);
				    }

				    //! Returns the number of 1 bits in the given 64-bit value.
				    //!
				    //! \tparam TIntrinsic The intrinsic implementation type.
				    //! \param intrinsic The intrinsic implementation.
				    //! \param value The input value.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TIntrinsic>
				    ALPAKA_FN_ACC auto popcount(TIntrinsic const& intrinsic, std::uint64_t value) -> std::int32_t
				    {
				        using ImplementationBase = concepts::ImplementationBase<ConceptIntrinsic, TIntrinsic>;
				        return trait::Popcount<ImplementationBase>::popcount(intrinsic, value);
				    }

				    //! Returns the 1-based position of the least significant bit set to 1
				    //! in the given 32-bit value. Returns 0 for input value 0.
				    //!
				    //! \tparam TIntrinsic The intrinsic implementation type.
				    //! \param intrinsic The intrinsic implementation.
				    //! \param value The input value.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TIntrinsic>
				    ALPAKA_FN_ACC auto ffs(TIntrinsic const& intrinsic, std::int32_t value) -> std::int32_t
				    {
				        using ImplementationBase = concepts::ImplementationBase<ConceptIntrinsic, TIntrinsic>;
				        return trait::Ffs<ImplementationBase>::ffs(intrinsic, value);
				    }

				    //! Returns the 1-based position of the least significant bit set to 1
				    //! in the given 64-bit value. Returns 0 for input value 0.
				    //!
				    //! \tparam TIntrinsic The intrinsic implementation type.
				    //! \param intrinsic The intrinsic implementation.
				    //! \param value The input value.
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename TIntrinsic>
				    ALPAKA_FN_ACC auto ffs(TIntrinsic const& intrinsic, std::int64_t value) -> std::int32_t
				    {
				        using ImplementationBase = concepts::ImplementationBase<ConceptIntrinsic, TIntrinsic>;
				        return trait::Ffs<ImplementationBase>::ffs(intrinsic, value);
				    }
				} // namespace alpaka
				// ==
				// == ./include/alpaka/intrinsic/Traits.hpp ==
				// ============================================================================


			namespace alpaka
			{
			    namespace detail
			    {
			        //! Fallback implementation of popcount.
			        template<typename TValue>
			        static auto popcountFallback(TValue value) -> std::int32_t
			        {
			            TValue count = 0;
			            while(value != 0)
			            {
			                count += value & 1u;
			                value >>= 1u;
			            }
			            return static_cast<std::int32_t>(count);
			        }

			        //! Fallback implementation of ffs.
			        template<typename TValue>
			        static auto ffsFallback(TValue value) -> std::int32_t
			        {
			            if(value == 0)
			                return 0;
			            std::int32_t result = 1;
			            while((value & 1) == 0)
			            {
			                value >>= 1;
			                result++;
			            }
			            return result;
			        }
			    } // namespace detail

			    //! The Fallback intrinsic.
			    class IntrinsicFallback : public concepts::Implements<ConceptIntrinsic, IntrinsicFallback>
			    {
			    };

			    namespace trait
			    {
			        template<>
			        struct Popcount<IntrinsicFallback>
			        {
			            static auto popcount(IntrinsicFallback const& /*intrinsic*/, std::uint32_t value) -> std::int32_t
			            {
			                return alpaka::detail::popcountFallback(value);
			            }

			            static auto popcount(IntrinsicFallback const& /*intrinsic*/, std::uint64_t value) -> std::int32_t
			            {
			                return alpaka::detail::popcountFallback(value);
			            }
			        };

			        template<>
			        struct Ffs<IntrinsicFallback>
			        {
			            static auto ffs(IntrinsicFallback const& /*intrinsic*/, std::int32_t value) -> std::int32_t
			            {
			                return alpaka::detail::ffsFallback(value);
			            }

			            static auto ffs(IntrinsicFallback const& /*intrinsic*/, std::int64_t value) -> std::int32_t
			            {
			                return alpaka::detail::ffsFallback(value);
			            }
			        };
			    } // namespace trait
			} // namespace alpaka
			// ==
			// == ./include/alpaka/intrinsic/IntrinsicFallback.hpp ==
			// ============================================================================

		// #include "alpaka/intrinsic/Traits.hpp"    // amalgamate: file already expanded

		#include <bitset>
		#include <climits>
		#if __has_include(<bit>)
		#    include <bit>
		#endif

		#if BOOST_COMP_MSVC
		// #    include <intrin.h>    // amalgamate: file already included
		#endif

		namespace alpaka
		{
		    //! The CPU intrinsic.
		    class IntrinsicCpu : public concepts::Implements<ConceptIntrinsic, IntrinsicCpu>
		    {
		    };

		    namespace trait
		    {
		        template<>
		        struct Popcount<IntrinsicCpu>
		        {
		            template<typename UnsignedIntegral>
		            static auto popcount(IntrinsicCpu const& /*intrinsic*/, UnsignedIntegral value) -> std::int32_t
		            {
		#ifdef __cpp_lib_bitops
		                return std::popcount(value);
		#elif BOOST_COMP_GNUC || BOOST_COMP_CLANG
		                if constexpr(sizeof(UnsignedIntegral) == 8)
		                    return __builtin_popcountll(value);
		                else
		                    return __builtin_popcount(value);
		#elif BOOST_COMP_MSVC
		                if constexpr(sizeof(UnsignedIntegral) == 8)
		                    return static_cast<std::int32_t>(__popcnt64(value));
		                else
		                    return __popcnt(value);
		#else
		                // Fallback to standard library
		                return static_cast<std::int32_t>(std::bitset<sizeof(UnsignedIntegral) * CHAR_BIT>(value).count());
		#endif
		            }
		        };

		        template<>
		        struct Ffs<IntrinsicCpu>
		        {
		            template<typename Integral>
		            static auto ffs(IntrinsicCpu const& /*intrinsic*/, Integral value) -> std::int32_t
		            {
		#ifdef __cpp_lib_bitops
		                return value == 0 ? 0 : std::countr_zero(static_cast<std::make_unsigned_t<Integral>>(value)) + 1;
		#elif BOOST_COMP_GNUC || BOOST_COMP_CLANG
		                if constexpr(sizeof(Integral) == 8)
		                    return __builtin_ffsll(value);
		                else
		                    return __builtin_ffs(value);
		#elif BOOST_COMP_MSVC
		                // Implementation based on
		                // https://gitlab.freedesktop.org/cairo/cairo/commit/f5167dc2e1a13d8c4e5d66d7178a24b9b5e7ac7a
		                unsigned long index = 0u;
		                if constexpr(sizeof(Integral) == 8)
		                    return _BitScanForward64(&index, value) == 0 ? 0 : static_cast<std::int32_t>(index + 1u);
		                else
		                    return _BitScanForward(&index, value) == 0 ? 0 : static_cast<std::int32_t>(index + 1u);
		#else
		                return alpaka::detail::ffsFallback(value);
		#endif
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/intrinsic/IntrinsicCpu.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/math/MathStdLib.hpp ==
		// ==
		/* Copyright 2022 Alexander Matthes, Axel Huebl, Benjamin Worpitz, Matthias Werner, Bernhard Manfred Gruber,
		 * Jeffrey Kelling, Sergei Bastrakov
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
			// ============================================================================
			// == ./include/alpaka/core/Decay.hpp ==
			// ==
			/* Copyright 2022 Sergei Bastrakov, Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			//! Wrapper around std::decay_t for parameter pack expansion expressions
			//
			// Works around PGI compiler internal error when used in empty template pack
			// extension as discussed in #995. It seems not possible to make a workaround
			// with pure C++ tools, like an alias template, so macro it is. Note that
			// there is no known issue outside of empty parameter pack expansions,
			// so the normal std::decay_t can and should be used there.
			//
			// The choice of macro over writing typename std::decay<Type>::type explicitly
			// in parameter pack expansion expressions is to avoid warnings from diagnostic
			// tools, and also for brevity.
			#if BOOST_COMP_PGI
			#    define ALPAKA_DECAY_T(Type) typename std::decay<Type>::type
			#else
			#    define ALPAKA_DECAY_T(Type) std::decay_t<Type>
			#endif

			namespace alpaka
			{
			    //! Provides a decaying wrapper around std::is_same. Example: is_decayed_v<volatile float, float> returns true.
			    template<typename T, typename U>
			    inline constexpr auto is_decayed_v = std::is_same_v<ALPAKA_DECAY_T(T), ALPAKA_DECAY_T(U)>;
			} // namespace alpaka
			// ==
			// == ./include/alpaka/core/Decay.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/math/Traits.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber, Sergei Bastrakov
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			#include <cmath>
			#include <complex>
			#if __has_include(<numbers>)
			#    include <numbers>
			#endif

			namespace alpaka::math
			{
			    namespace constants
			    {
			#ifdef __cpp_lib_math_constants
			        inline constexpr double e = std::numbers::e;
			        inline constexpr double log2e = std::numbers::log2e;
			        inline constexpr double log10e = std::numbers::log10e;
			        inline constexpr double pi = std::numbers::pi;
			        inline constexpr double inv_pi = std::numbers::inv_pi;
			        inline constexpr double ln2 = std::numbers::ln2;
			        inline constexpr double ln10 = std::numbers::ln10;
			        inline constexpr double sqrt2 = std::numbers::sqrt2;

			        template<typename T>
			        inline constexpr T e_v = std::numbers::e_v<T>;

			        template<typename T>
			        inline constexpr T log2e_v = std::numbers::log2e_v<T>;

			        template<typename T>
			        inline constexpr T log10e_v = std::numbers::log10e_v<T>;

			        template<typename T>
			        inline constexpr T pi_v = std::numbers::pi_v<T>;

			        template<typename T>
			        inline constexpr T inv_pi_v = std::numbers::inv_pi_v<T>;

			        template<typename T>
			        inline constexpr T ln2_v = std::numbers::ln2_v<T>;

			        template<typename T>
			        inline constexpr T ln10_v = std::numbers::ln10_v<T>;

			        template<typename T>
			        inline constexpr T sqrt2_v = std::numbers::sqrt2_v<T>;
			#else
			        inline constexpr double e = M_E;
			        inline constexpr double log2e = M_LOG2E;
			        inline constexpr double log10e = M_LOG10E;
			        inline constexpr double pi = M_PI;
			        inline constexpr double inv_pi = M_1_PI;
			        inline constexpr double ln2 = M_LN2;
			        inline constexpr double ln10 = M_LN10;
			        inline constexpr double sqrt2 = M_SQRT2;

			        template<typename T>
			        inline constexpr T e_v = static_cast<T>(e);

			        template<typename T>
			        inline constexpr T log2e_v = static_cast<T>(log2e);

			        template<typename T>
			        inline constexpr T log10e_v = static_cast<T>(log10e);

			        template<typename T>
			        inline constexpr T pi_v = static_cast<T>(pi);

			        template<typename T>
			        inline constexpr T inv_pi_v = static_cast<T>(inv_pi);

			        template<typename T>
			        inline constexpr T ln2_v = static_cast<T>(ln2);

			        template<typename T>
			        inline constexpr T ln10_v = static_cast<T>(ln10);

			        template<typename T>
			        inline constexpr T sqrt2_v = static_cast<T>(sqrt2);

			        // Use predefined float constants when available
			#    if defined(M_Ef)
			        template<>
			        inline constexpr float e_v<float> = M_Ef;
			#    endif

			#    if defined(M_LOG2Ef)
			        template<>
			        inline constexpr float log2e_v<float> = M_LOG2Ef;
			#    endif

			#    if defined(M_LOG10Ef)
			        template<>
			        inline constexpr float log10e_v<float> = M_LOG10Ef;
			#    endif

			#    if defined(M_PIf)
			        template<>
			        inline constexpr float pi_v<float> = M_PIf;
			#    endif

			#    if defined(M_1_PIf)
			        template<>
			        inline constexpr float inv_pi_v<float> = M_1_PIf;
			#    endif

			#    if defined(M_LN2f)
			        template<>
			        inline constexpr float ln2_v<float> = M_LN2f;
			#    endif

			#    if defined(M_LN10f)
			        template<>
			        inline constexpr float ln10_v<float> = M_LN10f;
			#    endif

			#    if defined(M_SQRT2f)
			        template<>
			        inline constexpr float sqrt2_v<float> = M_SQRT2f;
			#    endif

			#endif
			    } // namespace constants

			    struct ConceptMathAbs
			    {
			    };

			    struct ConceptMathAcos
			    {
			    };

			    struct ConceptMathAcosh
			    {
			    };

			    struct ConceptMathArg
			    {
			    };

			    struct ConceptMathAsin
			    {
			    };

			    struct ConceptMathAsinh
			    {
			    };

			    struct ConceptMathAtan
			    {
			    };

			    struct ConceptMathAtanh
			    {
			    };

			    struct ConceptMathAtan2
			    {
			    };

			    struct ConceptMathCbrt
			    {
			    };

			    struct ConceptMathCeil
			    {
			    };

			    struct ConceptMathConj
			    {
			    };

			    struct ConceptMathCos
			    {
			    };

			    struct ConceptMathCosh
			    {
			    };

			    struct ConceptMathErf
			    {
			    };

			    struct ConceptMathExp
			    {
			    };

			    struct ConceptMathFloor
			    {
			    };

			    struct ConceptMathFmod
			    {
			    };

			    struct ConceptMathIsfinite
			    {
			    };

			    struct ConceptMathIsinf
			    {
			    };

			    struct ConceptMathIsnan
			    {
			    };

			    struct ConceptMathLog
			    {
			    };

			    struct ConceptMathMax
			    {
			    };

			    struct ConceptMathMin
			    {
			    };

			    struct ConceptMathPow
			    {
			    };

			    struct ConceptMathRemainder
			    {
			    };

			    struct ConceptMathRound
			    {
			    };

			    struct ConceptMathRsqrt
			    {
			    };

			    struct ConceptMathSin
			    {
			    };

			    struct ConceptMathSinh
			    {
			    };

			    struct ConceptMathSinCos
			    {
			    };

			    struct ConceptMathSqrt
			    {
			    };

			    struct ConceptMathTan
			    {
			    };

			    struct ConceptMathTanh
			    {
			    };

			    struct ConceptMathTrunc
			    {
			    };

			    //! The math traits.
			    namespace trait
			    {
			        //! The abs trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Abs
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find abs(TArg) in the namespace of your type.
			                using std::abs;
			                return abs(arg);
			            }
			        };

			        //! The acos trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Acos
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find acos(TArg) in the namespace of your type.
			                using std::acos;
			                return acos(arg);
			            }
			        };

			        //! The acosh trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Acosh
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find acosh(TArg) in the namespace of your type.
			                using std::acosh;
			                return acosh(arg);
			            }
			        };

			        //! The arg trait.
			        template<typename T, typename TArgument, typename TSfinae = void>
			        struct Arg
			        {
			            // It is unclear why this is needed here and not in other math trait structs. But removing it causes
			            // warnings with calling a __host__ function from a __host__ __device__ function when building for CUDA.
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArgument const& argument)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find arg(TArgument) in the namespace of your type.
			                using std::arg;
			                return arg(argument);
			            }
			        };

			        //! The asin trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Asin
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find asin(TArg) in the namespace of your type.
			                using std::asin;
			                return asin(arg);
			            }
			        };

			        //! The asin trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Asinh
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find asin(TArg) in the namespace of your type.
			                using std::asinh;
			                return asinh(arg);
			            }
			        };

			        //! The atan trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Atan
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find atan(TArg) in the namespace of your type.
			                using std::atan;
			                return atan(arg);
			            }
			        };

			        //! The atanh trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Atanh
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find atanh(TArg) in the namespace of your type.
			                using std::atanh;
			                return atanh(arg);
			            }
			        };

			        //! The atan2 trait.
			        template<typename T, typename Ty, typename Tx, typename TSfinae = void>
			        struct Atan2
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, Ty const& y, Tx const& x)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find atan2(Tx, Ty) in the namespace of your type.
			                using std::atan2;
			                return atan2(y, x);
			            }
			        };

			        //! The cbrt trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Cbrt
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find cbrt(TArg) in the namespace of your type.
			                using std::cbrt;
			                return cbrt(arg);
			            } //! The erf trait.
			        };

			        //! The ceil trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Ceil
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find ceil(TArg) in the namespace of your type.
			                using std::ceil;
			                return ceil(arg);
			            }
			        };

			        //! The conj trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Conj
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find conj(TArg) in the namespace of your type.
			                using std::conj;
			                return conj(arg);
			            }
			        };

			        //! The cos trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Cos
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find cos(TArg) in the namespace of your type.
			                using std::cos;
			                return cos(arg);
			            }
			        };

			        //! The cosh trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Cosh
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find cos(TArg) in the namespace of your type.
			                using std::cosh;
			                return cosh(arg);
			            }
			        };

			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Erf
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find erf(TArg) in the namespace of your type.
			                using std::erf;
			                return erf(arg);
			            }
			        };

			        //! The exp trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Exp
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find exp(TArg) in the namespace of your type.
			                using std::exp;
			                return exp(arg);
			            }
			        };

			        //! The floor trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Floor
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find floor(TArg) in the namespace of your type.
			                using std::floor;
			                return floor(arg);
			            }
			        };

			        //! The fmod trait.
			        template<typename T, typename Tx, typename Ty, typename TSfinae = void>
			        struct Fmod
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, Tx const& x, Ty const& y)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find fmod(Tx, Ty) in the namespace of your type.
			                using std::fmod;
			                return fmod(x, y);
			            }
			        };

			        //! The isfinite trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Isfinite
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find isfinite(TArg) in the namespace of your type.
			                using std::isfinite;
			                return isfinite(arg);
			            }
			        };

			        //! The isinf trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Isinf
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find isinf(TArg) in the namespace of your type.
			                using std::isinf;
			                return isinf(arg);
			            }
			        };

			        //! The isnan trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Isnan
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find isnan(TArg) in the namespace of your type.
			                using std::isnan;
			                return isnan(arg);
			            }
			        };

			        //! The log trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Log
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find log(TArg) in the namespace of your type.
			                using std::log;
			                return log(arg);
			            }
			        };

			        //! The max trait.
			        template<typename T, typename Tx, typename Ty, typename TSfinae = void>
			        struct Max
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, Tx const& x, Ty const& y)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find max(Tx, Ty) in the namespace of your type.
			                using std::max;
			                return max(x, y);
			            }
			        };

			        //! The min trait.
			        template<typename T, typename Tx, typename Ty, typename TSfinae = void>
			        struct Min
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, Tx const& x, Ty const& y)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find min(Tx, Ty) in the namespace of your type.
			                using std::min;
			                return min(x, y);
			            }
			        };

			        //! The pow trait.
			        template<typename T, typename TBase, typename TExp, typename TSfinae = void>
			        struct Pow
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TBase const& base, TExp const& exp)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find pow(base, exp) in the namespace of your type.
			                using std::pow;
			                return pow(base, exp);
			            }
			        };

			        //! The remainder trait.
			        template<typename T, typename Tx, typename Ty, typename TSfinae = void>
			        struct Remainder
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, Tx const& x, Ty const& y)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find remainder(Tx, Ty) in the namespace of your type.
			                using std::remainder;
			                return remainder(x, y);
			            }
			        };

			        //! The round trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Round
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find round(TArg) in the namespace of your type.
			                using std::round;
			                return round(arg);
			            }
			        };

			        //! The round trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Lround
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find lround(TArg) in the namespace of your type.
			                using std::lround;
			                return lround(arg);
			            }
			        };

			        //! The round trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Llround
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find llround(TArg) in the namespace of your type.
			                using std::llround;
			                return llround(arg);
			            }
			        };

			        namespace detail
			        {
			            //! Fallback implementation when no better ADL match was found
			            template<typename TArg>
			            ALPAKA_FN_HOST_ACC auto rsqrt(TArg const& arg)
			            {
			                // Still use ADL to try find sqrt(arg)
			                using std::sqrt;
			                return static_cast<TArg>(1) / sqrt(arg);
			            }
			        } // namespace detail

			        //! The rsqrt trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Rsqrt
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find rsqrt(TArg) in the namespace of your type.
			                using detail::rsqrt;
			                return rsqrt(arg);
			            }
			        };

			        //! The sin trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Sin
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find sin(TArg) in the namespace of your type.
			                using std::sin;
			                return sin(arg);
			            }
			        };

			        //! The sin trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Sinh
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find sin(TArg) in the namespace of your type.
			                using std::sinh;
			                return sinh(arg);
			            }
			        };

			        namespace detail
			        {
			            //! Fallback implementation when no better ADL match was found
			            template<typename TArg>
			            ALPAKA_FN_HOST_ACC auto sincos(TArg const& arg, TArg& result_sin, TArg& result_cos)
			            {
			                // Still use ADL to try find sin(arg) and cos(arg)
			                using std::sin;
			                result_sin = sin(arg);
			                using std::cos;
			                result_cos = cos(arg);
			            }
			        } // namespace detail

			        //! The sincos trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct SinCos
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg, TArg& result_sin, TArg& result_cos)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find sincos(TArg, TArg&, TArg&) in the namespace of your type.
			                using detail::sincos;
			                return sincos(arg, result_sin, result_cos);
			            }
			        };

			        //! The sqrt trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Sqrt
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find sqrt(TArg) in the namespace of your type.
			                using std::sqrt;
			                return sqrt(arg);
			            }
			        };

			        //! The tan trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Tan
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find tan(TArg) in the namespace of your type.
			                using std::tan;
			                return tan(arg);
			            }
			        };

			        //! The tanh trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Tanh
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find tanh(TArg) in the namespace of your type.
			                using std::tanh;
			                return tanh(arg);
			            }
			        };

			        //! The trunc trait.
			        template<typename T, typename TArg, typename TSfinae = void>
			        struct Trunc
			        {
			            ALPAKA_FN_HOST_ACC auto operator()(T const& /* ctx */, TArg const& arg)
			            {
			                // This is an ADL call. If you get a compile error here then your type is not supported by the
			                // backend and we could not find trunc(TArg) in the namespace of your type.
			                using std::trunc;
			                return trunc(arg);
			            }
			        };
			    } // namespace trait

			    //! Computes the absolute value.
			    //!
			    //! \tparam T The type of the object specializing Abs.
			    //! \tparam TArg The arg type.
			    //! \param abs_ctx The object specializing Abs.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto abs(T const& abs_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathAbs, T>;
			        return trait::Abs<ImplementationBase, TArg>{}(abs_ctx, arg);
			    }

			    //! Computes the principal value of the arc cosine.
			    //!
			    //! The valid real argument range is [-1.0, 1.0]. For other values
			    //! the result may depend on the backend and compilation options, will
			    //! likely be NaN.
			    //!
			    //! \tparam TArg The arg type.
			    //! \param acos_ctx The object specializing Acos.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto acos(T const& acos_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathAcos, T>;
			        return trait::Acos<ImplementationBase, TArg>{}(acos_ctx, arg);
			    }

			    //! Computes the principal value of the hyperbolic arc cosine.
			    //!
			    //! The valid real argument range is [1.0, Inf]. For other values
			    //! the result may depend on the backend and compilation options, will
			    //! likely be NaN.
			    //!
			    //! \tparam TArg The arg type.
			    //! \param acosh_ctx The object specializing Acos.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto acosh(T const& acosh_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathAcosh, T>;
			        return trait::Acosh<ImplementationBase, TArg>{}(acosh_ctx, arg);
			    }

			    //! Computes the complex argument of the value.
			    //!
			    //! \tparam T The type of the object specializing Arg.
			    //! \tparam TArgument The argument type.
			    //! \param arg_ctx The object specializing Arg.
			    //! \param argument The argument.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArgument>
			    ALPAKA_FN_HOST_ACC auto arg(T const& arg_ctx, TArgument const& argument)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathArg, T>;
			        return trait::Arg<ImplementationBase, TArgument>{}(arg_ctx, argument);
			    }

			    //! Computes the principal value of the arc sine.
			    //!
			    //! The valid real argument range is [-1.0, 1.0]. For other values
			    //! the result may depend on the backend and compilation options, will
			    //! likely be NaN.
			    //!
			    //! \tparam TArg The arg type.
			    //! \param asin_ctx The object specializing Asin.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto asin(T const& asin_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathAsin, T>;
			        return trait::Asin<ImplementationBase, TArg>{}(asin_ctx, arg);
			    }

			    //! Computes the principal value of the hyperbolic arc sine.
			    //!
			    //! \tparam TArg The arg type.
			    //! \param asinh_ctx The object specializing Asin.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto asinh(T const& asinh_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathAsinh, T>;
			        return trait::Asinh<ImplementationBase, TArg>{}(asinh_ctx, arg);
			    }

			    //! Computes the principal value of the arc tangent.
			    //!
			    //! \tparam TArg The arg type.
			    //! \param atan_ctx The object specializing Atan.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto atan(T const& atan_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathAtan, T>;
			        return trait::Atan<ImplementationBase, TArg>{}(atan_ctx, arg);
			    }

			    //! Computes the principal value of the hyperbolic arc tangent.
			    //!
			    //! The valid real argument range is [-1.0, 1.0]. For other values
			    //! the result may depend on the backend and compilation options, will
			    //! likely be NaN.

			    //! \tparam TArg The arg type.
			    //! \param atanh_ctx The object specializing Atanh.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto atanh(T const& atanh_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathAtanh, T>;
			        return trait::Atanh<ImplementationBase, TArg>{}(atanh_ctx, arg);
			    }

			    //! Computes the arc tangent of y/x using the signs of arguments to determine the correct quadrant.
			    //!
			    //! \tparam T The type of the object specializing Atan2.
			    //! \tparam Ty The y arg type.
			    //! \tparam Tx The x arg type.
			    //! \param atan2_ctx The object specializing Atan2.
			    //! \param y The y arg.
			    //! \param x The x arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename Ty, typename Tx>
			    ALPAKA_FN_HOST_ACC auto atan2(T const& atan2_ctx, Ty const& y, Tx const& x)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathAtan2, T>;
			        return trait::Atan2<ImplementationBase, Ty, Tx>{}(atan2_ctx, y, x);
			    }

			    //! Computes the cbrt.
			    //!
			    //! \tparam T The type of the object specializing Cbrt.
			    //! \tparam TArg The arg type.
			    //! \param cbrt_ctx The object specializing Cbrt.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto cbrt(T const& cbrt_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathCbrt, T>;
			        return trait::Cbrt<ImplementationBase, TArg>{}(cbrt_ctx, arg);
			    }

			    //! Computes the smallest integer value not less than arg.
			    //!
			    //! \tparam T The type of the object specializing Ceil.
			    //! \tparam TArg The arg type.
			    //! \param ceil_ctx The object specializing Ceil.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto ceil(T const& ceil_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathCeil, T>;
			        return trait::Ceil<ImplementationBase, TArg>{}(ceil_ctx, arg);
			    }

			    //! Computes the complex conjugate of arg.
			    //!
			    //! \tparam T The type of the object specializing Conj.
			    //! \tparam TArg The arg type.
			    //! \param conj_ctx The object specializing Conj.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto conj(T const& conj_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathConj, T>;
			        return trait::Conj<ImplementationBase, TArg>{}(conj_ctx, arg);
			    }

			    //! Computes the cosine (measured in radians).
			    //!
			    //! \tparam T The type of the object specializing Cos.
			    //! \tparam TArg The arg type.
			    //! \param cos_ctx The object specializing Cos.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto cos(T const& cos_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathCos, T>;
			        return trait::Cos<ImplementationBase, TArg>{}(cos_ctx, arg);
			    }

			    //! Computes the hyperbolic cosine (measured in radians).
			    //!
			    //! \tparam T The type of the object specializing Cos.
			    //! \tparam TArg The arg type.
			    //! \param cosh_ctx The object specializing Cos.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto cosh(T const& cosh_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathCosh, T>;
			        return trait::Cosh<ImplementationBase, TArg>{}(cosh_ctx, arg);
			    }

			    //! Computes the error function of arg.
			    //!
			    //! \tparam T The type of the object specializing Erf.
			    //! \tparam TArg The arg type.
			    //! \param erf_ctx The object specializing Erf.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto erf(T const& erf_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathErf, T>;
			        return trait::Erf<ImplementationBase, TArg>{}(erf_ctx, arg);
			    }

			    //! Computes the e (Euler's number, 2.7182818) raised to the given power arg.
			    //!
			    //! \tparam T The type of the object specializing Exp.
			    //! \tparam TArg The arg type.
			    //! \param exp_ctx The object specializing Exp.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto exp(T const& exp_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathExp, T>;
			        return trait::Exp<ImplementationBase, TArg>{}(exp_ctx, arg);
			    }

			    //! Computes the largest integer value not greater than arg.
			    //!
			    //! \tparam T The type of the object specializing Floor.
			    //! \tparam TArg The arg type.
			    //! \param floor_ctx The object specializing Floor.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto floor(T const& floor_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathFloor, T>;
			        return trait::Floor<ImplementationBase, TArg>{}(floor_ctx, arg);
			    }

			    //! Computes the floating-point remainder of the division operation x/y.
			    //!
			    //! \tparam T The type of the object specializing Fmod.
			    //! \tparam Tx The type of the first argument.
			    //! \tparam Ty The type of the second argument.
			    //! \param fmod_ctx The object specializing Fmod.
			    //! \param x The first argument.
			    //! \param y The second argument.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename Tx, typename Ty>
			    ALPAKA_FN_HOST_ACC auto fmod(T const& fmod_ctx, Tx const& x, Ty const& y)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathFmod, T>;
			        return trait::Fmod<ImplementationBase, Tx, Ty>{}(fmod_ctx, x, y);
			    }

			    //! Checks if given value is finite.
			    //!
			    //! \tparam T The type of the object specializing Isfinite.
			    //! \tparam TArg The arg type.
			    //! \param ctx The object specializing Isfinite.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto isfinite(T const& ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathIsfinite, T>;
			        return trait::Isfinite<ImplementationBase, TArg>{}(ctx, arg);
			    }

			    //! Checks if given value is inf.
			    //!
			    //! \tparam T The type of the object specializing Isinf.
			    //! \tparam TArg The arg type.
			    //! \param ctx The object specializing Isinf.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto isinf(T const& ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathIsinf, T>;
			        return trait::Isinf<ImplementationBase, TArg>{}(ctx, arg);
			    }

			    //! Checks if given value is NaN.
			    //!
			    //! \tparam T The type of the object specializing Isnan.
			    //! \tparam TArg The arg type.
			    //! \param ctx The object specializing Isnan.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto isnan(T const& ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathIsnan, T>;
			        return trait::Isnan<ImplementationBase, TArg>{}(ctx, arg);
			    }

			    //! Computes the the natural (base e) logarithm of arg.
			    //!
			    //! Valid real arguments are non-negative. For other values the result
			    //! may depend on the backend and compilation options, will likely
			    //! be NaN.
			    //!
			    //! \tparam T The type of the object specializing Log.
			    //! \tparam TArg The arg type.
			    //! \param log_ctx The object specializing Log.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto log(T const& log_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathLog, T>;
			        return trait::Log<ImplementationBase, TArg>{}(log_ctx, arg);
			    }

			    //! Returns the larger of two arguments.
			    //! NaNs are treated as missing data (between a NaN and a numeric value, the numeric value is chosen).
			    //!
			    //! \tparam T The type of the object specializing Max.
			    //! \tparam Tx The type of the first argument.
			    //! \tparam Ty The type of the second argument.
			    //! \param max_ctx The object specializing Max.
			    //! \param x The first argument.
			    //! \param y The second argument.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename Tx, typename Ty>
			    ALPAKA_FN_HOST_ACC auto max(T const& max_ctx, Tx const& x, Ty const& y)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathMax, T>;
			        return trait::Max<ImplementationBase, Tx, Ty>{}(max_ctx, x, y);
			    }

			    //! Returns the smaller of two arguments.
			    //! NaNs are treated as missing data (between a NaN and a numeric value, the numeric value is chosen).
			    //!
			    //! \tparam T The type of the object specializing Min.
			    //! \tparam Tx The type of the first argument.
			    //! \tparam Ty The type of the second argument.
			    //! \param min_ctx The object specializing Min.
			    //! \param x The first argument.
			    //! \param y The second argument.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename Tx, typename Ty>
			    ALPAKA_FN_HOST_ACC auto min(T const& min_ctx, Tx const& x, Ty const& y)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathMin, T>;
			        return trait::Min<ImplementationBase, Tx, Ty>{}(min_ctx, x, y);
			    }

			    //! Computes the value of base raised to the power exp.
			    //!
			    //! Valid real arguments for base are non-negative. For other values
			    //! the result may depend on the backend and compilation options, will
			    //! likely be NaN.
			    //!
			    //! \tparam T The type of the object specializing Pow.
			    //! \tparam TBase The base type.
			    //! \tparam TExp The exponent type.
			    //! \param pow_ctx The object specializing Pow.
			    //! \param base The base.
			    //! \param exp The exponent.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TBase, typename TExp>
			    ALPAKA_FN_HOST_ACC auto pow(T const& pow_ctx, TBase const& base, TExp const& exp)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathPow, T>;
			        return trait::Pow<ImplementationBase, TBase, TExp>{}(pow_ctx, base, exp);
			    }

			    //! Computes the IEEE remainder of the floating point division operation x/y.
			    //!
			    //! \tparam T The type of the object specializing Remainder.
			    //! \tparam Tx The type of the first argument.
			    //! \tparam Ty The type of the second argument.
			    //! \param remainder_ctx The object specializing Max.
			    //! \param x The first argument.
			    //! \param y The second argument.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename Tx, typename Ty>
			    ALPAKA_FN_HOST_ACC auto remainder(T const& remainder_ctx, Tx const& x, Ty const& y)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathRemainder, T>;
			        return trait::Remainder<ImplementationBase, Tx, Ty>{}(remainder_ctx, x, y);
			    }

			    //! Computes the nearest integer value to arg (in floating-point format), rounding halfway cases away from
			    //! zero, regardless of the current rounding mode.
			    //!
			    //! \tparam T The type of the object specializing Round.
			    //! \tparam TArg The arg type.
			    //! \param round_ctx The object specializing Round.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto round(T const& round_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathRound, T>;
			        return trait::Round<ImplementationBase, TArg>{}(round_ctx, arg);
			    }
			    //! Computes the nearest integer value to arg (in integer format), rounding halfway cases away from zero,
			    //! regardless of the current rounding mode.
			    //!
			    //! \tparam T The type of the object specializing Round.
			    //! \tparam TArg The arg type.
			    //! \param lround_ctx The object specializing Round.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto lround(T const& lround_ctx, TArg const& arg) -> long int
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathRound, T>;
			        return trait::Lround<ImplementationBase, TArg>{}(lround_ctx, arg);
			    }
			    //! Computes the nearest integer value to arg (in integer format), rounding halfway cases away from zero,
			    //! regardless of the current rounding mode.
			    //!
			    //! \tparam T The type of the object specializing Round.
			    //! \tparam TArg The arg type.
			    //! \param llround_ctx The object specializing Round.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto llround(T const& llround_ctx, TArg const& arg) -> long long int
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathRound, T>;
			        return trait::Llround<ImplementationBase, TArg>{}(llround_ctx, arg);
			    }

			    //! Computes the rsqrt.
			    //!
			    //! Valid real arguments are positive. For other values the result
			    //! may depend on the backend and compilation options, will likely
			    //! be NaN.
			    //!
			    //! \tparam T The type of the object specializing Rsqrt.
			    //! \tparam TArg The arg type.
			    //! \param rsqrt_ctx The object specializing Rsqrt.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto rsqrt(T const& rsqrt_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathRsqrt, T>;
			        return trait::Rsqrt<ImplementationBase, TArg>{}(rsqrt_ctx, arg);
			    }

			    //! Computes the sine (measured in radians).
			    //!
			    //! \tparam T The type of the object specializing Sin.
			    //! \tparam TArg The arg type.
			    //! \param sin_ctx The object specializing Sin.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto sin(T const& sin_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathSin, T>;
			        return trait::Sin<ImplementationBase, TArg>{}(sin_ctx, arg);
			    }

			    //! Computes the hyperbolic sine (measured in radians).
			    //!
			    //! \tparam T The type of the object specializing Sin.
			    //! \tparam TArg The arg type.
			    //! \param sinh_ctx The object specializing Sin.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto sinh(T const& sinh_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathSinh, T>;
			        return trait::Sinh<ImplementationBase, TArg>{}(sinh_ctx, arg);
			    }

			    //! Computes the sine and cosine (measured in radians).
			    //!
			    //! \tparam T The type of the object specializing SinCos.
			    //! \tparam TArg The arg type.
			    //! \param sincos_ctx The object specializing SinCos.
			    //! \param arg The arg.
			    //! \param result_sin result of sine
			    //! \param result_cos result of cosine
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto sincos(T const& sincos_ctx, TArg const& arg, TArg& result_sin, TArg& result_cos) -> void
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathSinCos, T>;
			        trait::SinCos<ImplementationBase, TArg>{}(sincos_ctx, arg, result_sin, result_cos);
			    }


			    //! Computes the square root of arg.
			    //!
			    //! Valid real arguments are non-negative. For other values the result
			    //! may depend on the backend and compilation options, will likely
			    //! be NaN.
			    //!
			    //! \tparam T The type of the object specializing Sqrt.
			    //! \tparam TArg The arg type.
			    //! \param sqrt_ctx The object specializing Sqrt.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto sqrt(T const& sqrt_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathSqrt, T>;
			        return trait::Sqrt<ImplementationBase, TArg>{}(sqrt_ctx, arg);
			    }

			    //! Computes the tangent (measured in radians).
			    //!
			    //! \tparam T The type of the object specializing Tan.
			    //! \tparam TArg The arg type.
			    //! \param tan_ctx The object specializing Tan.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto tan(T const& tan_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathTan, T>;
			        return trait::Tan<ImplementationBase, TArg>{}(tan_ctx, arg);
			    }

			    //! Computes the hyperbolic tangent (measured in radians).
			    //!
			    //! \tparam T The type of the object specializing Tanh.
			    //! \tparam TArg The arg type.
			    //! \param tanh_ctx The object specializing Tanh.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto tanh(T const& tanh_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathTanh, T>;
			        return trait::Tanh<ImplementationBase, TArg>{}(tanh_ctx, arg);
			    }

			    //! Computes the nearest integer not greater in magnitude than arg.
			    //!
			    //! \tparam T The type of the object specializing Trunc.
			    //! \tparam TArg The arg type.
			    //! \param trunc_ctx The object specializing Trunc.
			    //! \param arg The arg.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename T, typename TArg>
			    ALPAKA_FN_HOST_ACC auto trunc(T const& trunc_ctx, TArg const& arg)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMathTrunc, T>;
			        return trait::Trunc<ImplementationBase, TArg>{}(trunc_ctx, arg);
			    }
			} // namespace alpaka::math
			// ==
			// == ./include/alpaka/math/Traits.hpp ==
			// ============================================================================


		namespace alpaka::math
		{
		    //! The standard library abs, implementation covered by the general template.
		    class AbsStdLib : public concepts::Implements<ConceptMathAbs, AbsStdLib>
		    {
		    };

		    //! The standard library acos, implementation covered by the general template.
		    class AcosStdLib : public concepts::Implements<ConceptMathAcos, AcosStdLib>
		    {
		    };

		    //! The standard library acos, implementation covered by the general template.
		    class AcoshStdLib : public concepts::Implements<ConceptMathAcosh, AcoshStdLib>
		    {
		    };

		    //! The standard library arg, implementation covered by the general template.
		    class ArgStdLib : public concepts::Implements<ConceptMathArg, ArgStdLib>
		    {
		    };

		    //! The standard library asin, implementation covered by the general template.
		    class AsinStdLib : public concepts::Implements<ConceptMathAsin, AsinStdLib>
		    {
		    };

		    //! The standard library asinh, implementation covered by the general template.
		    class AsinhStdLib : public concepts::Implements<ConceptMathAsinh, AsinhStdLib>
		    {
		    };

		    //! The standard library atan, implementation covered by the general template.
		    class AtanStdLib : public concepts::Implements<ConceptMathAtan, AtanStdLib>
		    {
		    };

		    //! The standard library atanh, implementation covered by the general template.
		    class AtanhStdLib : public concepts::Implements<ConceptMathAtanh, AtanhStdLib>
		    {
		    };

		    //! The standard library atan2, implementation covered by the general template.
		    class Atan2StdLib : public concepts::Implements<ConceptMathAtan2, Atan2StdLib>
		    {
		    };

		    //! The standard library cbrt, implementation covered by the general template.
		    class CbrtStdLib : public concepts::Implements<ConceptMathCbrt, CbrtStdLib>
		    {
		    };

		    //! The standard library ceil, implementation covered by the general template.
		    class CeilStdLib : public concepts::Implements<ConceptMathCeil, CeilStdLib>
		    {
		    };

		    //! The standard library conj, implementation covered by the general template.
		    class ConjStdLib : public concepts::Implements<ConceptMathConj, ConjStdLib>
		    {
		    };

		    //! The standard library cos, implementation covered by the general template.
		    class CosStdLib : public concepts::Implements<ConceptMathCos, CosStdLib>
		    {
		    };

		    //! The standard library cosh, implementation covered by the general template.
		    class CoshStdLib : public concepts::Implements<ConceptMathCosh, CoshStdLib>
		    {
		    };

		    //! The standard library erf, implementation covered by the general template.
		    class ErfStdLib : public concepts::Implements<ConceptMathErf, ErfStdLib>
		    {
		    };

		    //! The standard library exp, implementation covered by the general template.
		    class ExpStdLib : public concepts::Implements<ConceptMathExp, ExpStdLib>
		    {
		    };

		    //! The standard library floor, implementation covered by the general template.
		    class FloorStdLib : public concepts::Implements<ConceptMathFloor, FloorStdLib>
		    {
		    };

		    //! The standard library fmod, implementation covered by the general template.
		    class FmodStdLib : public concepts::Implements<ConceptMathFmod, FmodStdLib>
		    {
		    };

		    //! The standard library isfinite, implementation covered by the general template.
		    class IsfiniteStdLib : public concepts::Implements<ConceptMathIsfinite, IsfiniteStdLib>
		    {
		    };

		    //! The standard library isinf, implementation covered by the general template.
		    class IsinfStdLib : public concepts::Implements<ConceptMathIsinf, IsinfStdLib>
		    {
		    };

		    //! The standard library isnan, implementation covered by the general template.
		    class IsnanStdLib : public concepts::Implements<ConceptMathIsnan, IsnanStdLib>
		    {
		    };

		    //! The standard library log, implementation covered by the general template.
		    class LogStdLib : public concepts::Implements<ConceptMathLog, LogStdLib>
		    {
		    };

		    //! The standard library max.
		    class MaxStdLib : public concepts::Implements<ConceptMathMax, MaxStdLib>
		    {
		    };

		    //! The standard library min.
		    class MinStdLib : public concepts::Implements<ConceptMathMin, MinStdLib>
		    {
		    };

		    //! The standard library pow, implementation covered by the general template.
		    class PowStdLib : public concepts::Implements<ConceptMathPow, PowStdLib>
		    {
		    };

		    //! The standard library remainder, implementation covered by the general template.
		    class RemainderStdLib : public concepts::Implements<ConceptMathRemainder, RemainderStdLib>
		    {
		    };

		    //! The standard library round, implementation covered by the general template.
		    class RoundStdLib : public concepts::Implements<ConceptMathRound, RoundStdLib>
		    {
		    };

		    //! The standard library rsqrt, implementation covered by the general template.
		    class RsqrtStdLib : public concepts::Implements<ConceptMathRsqrt, RsqrtStdLib>
		    {
		    };

		    //! The standard library sin, implementation covered by the general template.
		    class SinStdLib : public concepts::Implements<ConceptMathSin, SinStdLib>
		    {
		    };

		    //! The standard library sinh, implementation covered by the general template.
		    class SinhStdLib : public concepts::Implements<ConceptMathSinh, SinhStdLib>
		    {
		    };

		    //! The standard library sincos, implementation covered by the general template.
		    class SinCosStdLib : public concepts::Implements<ConceptMathSinCos, SinCosStdLib>
		    {
		    };

		    //! The standard library sqrt, implementation covered by the general template.
		    class SqrtStdLib : public concepts::Implements<ConceptMathSqrt, SqrtStdLib>
		    {
		    };

		    //! The standard library tan, implementation covered by the general template.
		    class TanStdLib : public concepts::Implements<ConceptMathTan, TanStdLib>
		    {
		    };

		    //! The standard library tanh, implementation covered by the general template.
		    class TanhStdLib : public concepts::Implements<ConceptMathTanh, TanhStdLib>
		    {
		    };

		    //! The standard library trunc, implementation covered by the general template.
		    class TruncStdLib : public concepts::Implements<ConceptMathTrunc, TruncStdLib>
		    {
		    };

		    //! The standard library math trait specializations.
		    class MathStdLib
		        : public AbsStdLib
		        , public AcosStdLib
		        , public AcoshStdLib
		        , public ArgStdLib
		        , public AsinStdLib
		        , public AsinhStdLib
		        , public AtanStdLib
		        , public AtanhStdLib
		        , public Atan2StdLib
		        , public CbrtStdLib
		        , public CeilStdLib
		        , public ConjStdLib
		        , public CosStdLib
		        , public CoshStdLib
		        , public ErfStdLib
		        , public ExpStdLib
		        , public FloorStdLib
		        , public FmodStdLib
		        , public LogStdLib
		        , public MaxStdLib
		        , public MinStdLib
		        , public PowStdLib
		        , public RemainderStdLib
		        , public RoundStdLib
		        , public RsqrtStdLib
		        , public SinStdLib
		        , public SinhStdLib
		        , public SinCosStdLib
		        , public SqrtStdLib
		        , public TanStdLib
		        , public TanhStdLib
		        , public TruncStdLib
		        , public IsnanStdLib
		        , public IsinfStdLib
		        , public IsfiniteStdLib
		    {
		    };

		    namespace trait
		    {
		        //! The standard library max trait specialization.
		        template<typename Tx, typename Ty>
		        struct Max<MaxStdLib, Tx, Ty, std::enable_if_t<std::is_arithmetic_v<Tx> && std::is_arithmetic_v<Ty>>>
		        {
		            ALPAKA_FN_HOST auto operator()(MaxStdLib const& /* max_ctx */, Tx const& x, Ty const& y)
		            {
		                using std::fmax;
		                using std::max;

		                if constexpr(std::is_integral_v<Tx> && std::is_integral_v<Ty>)
		                    return max(x, y);
		                else if constexpr(
		                    is_decayed_v<
		                        Tx,
		                        float> || is_decayed_v<Ty, float> || is_decayed_v<Tx, double> || is_decayed_v<Ty, double>)
		                    return fmax(x, y);
		                else
		                    static_assert(!sizeof(Tx), "Unsupported data type");

		                ALPAKA_UNREACHABLE(std::common_type_t<Tx, Ty>{});
		            }
		        };

		        //! The standard library min trait specialization.
		        template<typename Tx, typename Ty>
		        struct Min<MinStdLib, Tx, Ty, std::enable_if_t<std::is_arithmetic_v<Tx> && std::is_arithmetic_v<Ty>>>
		        {
		            ALPAKA_FN_HOST auto operator()(MinStdLib const& /* min_ctx */, Tx const& x, Ty const& y)
		            {
		                using std::fmin;
		                using std::min;

		                if constexpr(std::is_integral_v<Tx> && std::is_integral_v<Ty>)
		                    return min(x, y);
		                else if constexpr(
		                    is_decayed_v<
		                        Tx,
		                        float> || is_decayed_v<Ty, float> || is_decayed_v<Tx, double> || is_decayed_v<Ty, double>)
		                    return fmin(x, y);
		                else
		                    static_assert(!sizeof(Tx), "Unsupported data type");

		                ALPAKA_UNREACHABLE(std::common_type_t<Tx, Ty>{});
		            }
		        };
		    } // namespace trait

		} // namespace alpaka::math
		// ==
		// == ./include/alpaka/math/MathStdLib.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/mem/fence/MemFenceOmp2Blocks.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan, Bernhard Manfred Gruber, Andrea Bocci
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/mem/fence/Traits.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			namespace alpaka
			{
			    struct ConceptMemFence
			    {
			    };

			    namespace memory_scope
			    {
			        //! Memory fences are observed by all threads in the same block.
			        struct Block
			        {
			        };

			        //! Memory fences are observed by all threads in the same grid.
			        struct Grid
			        {
			        };

			        //! Memory fences are observed by all threads on the device.
			        struct Device
			        {
			        };
			    } // namespace memory_scope

			    //! The memory fence trait.
			    namespace trait
			    {
			        //! The mem_fence trait.
			        template<typename TMemFence, typename TMemScope, typename TSfinae = void>
			        struct MemFence;
			    } // namespace trait

			    //! Issues memory fence instructions.
			    //
			    // Issues a memory fence instruction for a given memory scope (\a memory_scope::Block or \a memory_scope::Device).
			    // This guarantees the following:
			    // * All \a LOAD instructions preceeding the fence will always occur before the LOAD instructions following the
			    //   fence (\a LoadLoad coherence)
			    // * All \a STORE instructions preceeding the fence will always occur before the STORE instructions following the
			    //   fence (\a LoadStore coherence). The pre-fence STORE results will be propagated to the other threads in the
			    //   scope at an unknown point in time.
			    //
			    // Note that there are no further guarantees, especially with regard to \a LoadStore ordering. Users should not
			    // mistake this as a synchronization function between threads (please use syncBlockThreads() instead).
			    //
			    //! \tparam TMemFence The memory fence implementation type.
			    //! \tparam TMemScope The memory scope type.
			    //! \param fence The memory fence implementation.
			    //! \param scope The memory scope.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TMemFence, typename TMemScope>
			    ALPAKA_FN_ACC auto mem_fence(TMemFence const& fence, TMemScope const& scope) -> void
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptMemFence, TMemFence>;
			        trait::MemFence<ImplementationBase, TMemScope>::mem_fence(fence, scope);
			    }
			} // namespace alpaka
			// ==
			// == ./include/alpaka/mem/fence/Traits.hpp ==
			// ============================================================================


		#ifdef ALPAKA_ACC_CPU_B_OMP2_T_SEQ_ENABLED

		#    if _OPENMP < 200203
		#        error If ALPAKA_ACC_CPU_B_OMP2_T_SEQ_ENABLED is set, the compiler has to support OpenMP 2.0 or higher!
		#    endif

		namespace alpaka
		{
		    //! The CPU OpenMP 2.0 block memory fence.
		    class MemFenceOmp2Blocks : public concepts::Implements<ConceptMemFence, MemFenceOmp2Blocks>
		    {
		    };

		    namespace trait
		    {
		        template<>
		        struct MemFence<MemFenceOmp2Blocks, memory_scope::Block>
		        {
		            static auto mem_fence(MemFenceOmp2Blocks const&, memory_scope::Block const&)
		            {
		                // Only one thread per block allowed -> no memory fence required on block level
		            }
		        };

		        template<>
		        struct MemFence<MemFenceOmp2Blocks, memory_scope::Grid>
		        {
		            static auto mem_fence(MemFenceOmp2Blocks const&, memory_scope::Grid const&)
		            {
		#    pragma omp flush
		            }
		        };

		        template<>
		        struct MemFence<MemFenceOmp2Blocks, memory_scope::Device>
		        {
		            static auto mem_fence(MemFenceOmp2Blocks const&, memory_scope::Device const&)
		            {
		#    pragma omp flush
		            }
		        };
		    } // namespace trait
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/mem/fence/MemFenceOmp2Blocks.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/rand/RandStdLib.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, René Widera, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/rand/TinyMT/Engine.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
				// ============================================================================
				// == ./include/alpaka/rand/TinyMT/tinymt32.h ==
				// ==
				/* Copyright 2011 - 2023 Mutsuo Saito, Makoto Matsumoto, Axel Hübl, Benjamin Worpitz, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: BSD-3-Clause
				 */
				// clang-format off
				#ifndef TINYMT32_H
				#define TINYMT32_H
				/**
				 * @file tinymt32.h
				 *
				 * @brief Tiny Mersenne Twister only 127 bit internal state
				 *
				 * @author Mutsuo Saito (Hiroshima University)
				 * @author Makoto Matsumoto (University of Tokyo)
				 *
				 * Copyright (C) 2011 Mutsuo Saito, Makoto Matsumoto,
				 * Hiroshima University and The University of Tokyo.
				 * All rights reserved.
				 *
				 * The 3-clause BSD License is applied to this software, see
				 * LICENSE.txt
				 */

				// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

				// #include <cstdint>    // amalgamate: file already included
				/* work-around for glibc < 2.18 according to bug
				 * https://sourceware.org/bugzilla/show_bug.cgi?id=15366
				 */
				#ifndef UINT32_MAX
				#   define UINT32_MAX ((uint32_t)-1u)
				#endif
				#ifndef UINT32_C
				#   define UINT32_C(value) uint_least32_t(value)
				#endif
				#include <cinttypes>

				#if BOOST_COMP_CLANG
				#   pragma clang diagnostic push
				#   pragma clang diagnostic ignored "-Wold-style-cast"
				#endif
				#if BOOST_COMP_GNUC
				#   pragma GCC diagnostic push
				#   pragma GCC diagnostic ignored "-Wold-style-cast"
				#endif
				#if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
				    #pragma warning(push)
				    #pragma warning(disable: 4100)  // tinymt32.h(60): warning C4100: 'random': unreferenced formal parameter
				#endif

				#define TINYMT32_MEXP 127
				#define TINYMT32_SH0 1
				#define TINYMT32_SH1 10
				#define TINYMT32_SH8 8
				#define TINYMT32_MASK UINT32_C(0x7fffffff)
				#define TINYMT32_MUL (1.0f / 16777216.0f)

				#if defined(__cplusplus)
				extern "C" {
				#endif

				/**
				 * tinymt32 internal state vector and parameters
				 */
				struct TINYMT32_T {
				    uint32_t status[4];
				    uint32_t mat1;
				    uint32_t mat2;
				    uint32_t tmat;
				};

				typedef struct TINYMT32_T tinymt32_t;

				inline void tinymt32_init(tinymt32_t * random, uint32_t seed);
				inline void tinymt32_init_by_array(tinymt32_t * random, uint32_t init_key[],
				                            int key_length);

				#if defined(__GNUC__)
				/**
				 * This function always returns 127
				 * @param random not used
				 * @return always 127
				 */
				inline static int tinymt32_get_mexp(
				    tinymt32_t * random  __attribute__((unused))) {
				    return TINYMT32_MEXP;
				}
				#else
				inline static int tinymt32_get_mexp(tinymt32_t * random) {
				    return TINYMT32_MEXP;
				}
				#endif

				/**
				 * This function changes internal state of tinymt32.
				 * Users should not call this function directly.
				 * @param random tinymt internal status
				 */
				inline static void tinymt32_next_state(tinymt32_t * random) {
				    uint32_t x;
				    uint32_t y;

				    y = random->status[3];
				    x = (random->status[0] & TINYMT32_MASK)
				        ^ random->status[1]
				        ^ random->status[2];
				    x ^= (x << TINYMT32_SH0);
				    y ^= (y >> TINYMT32_SH0) ^ x;
				    random->status[0] = random->status[1];
				    random->status[1] = random->status[2];
				    random->status[2] = x ^ (y << TINYMT32_SH1);
				    random->status[3] = y;
				    int32_t const a = -((int32_t)(y & 1)) & (int32_t)random->mat1;
				    int32_t const b = -((int32_t)(y & 1)) & (int32_t)random->mat2;
				    random->status[1] ^= (uint32_t)a;
				    random->status[2] ^= (uint32_t)b;
				}

				/**
				 * This function outputs 32-bit unsigned integer from internal state.
				 * Users should not call this function directly.
				 * @param random tinymt internal status
				 * @return 32-bit unsigned pseudorandom number
				 */
				inline static uint32_t tinymt32_temper(tinymt32_t * random) {
				    uint32_t t0, t1;
				    t0 = random->status[3];
				#if defined(LINEARITY_CHECK)
				    t1 = random->status[0]
				        ^ (random->status[2] >> TINYMT32_SH8);
				#else
				    t1 = random->status[0]
				        + (random->status[2] >> TINYMT32_SH8);
				#endif
				    t0 ^= t1;
				    if ((t1 & 1) != 0) {
				        t0 ^= random->tmat;
				    }
				    return t0;
				}

				/**
				 * This function outputs floating point number from internal state.
				 * Users should not call this function directly.
				 * @param random tinymt internal status
				 * @return floating point number r (1.0 <= r < 2.0)
				 */
				inline static float tinymt32_temper_conv(tinymt32_t * random) {
				    uint32_t t0, t1;
				    union {
				        uint32_t u;
				        float f;
				    } conv;

				    t0 = random->status[3];
				#if defined(LINEARITY_CHECK)
				    t1 = random->status[0]
				        ^ (random->status[2] >> TINYMT32_SH8);
				#else
				    t1 = random->status[0]
				        + (random->status[2] >> TINYMT32_SH8);
				#endif
				    t0 ^= t1;
				    if ((t1 & 1) != 0) {
				        conv.u  = ((t0 ^ random->tmat) >> 9) | UINT32_C(0x3f800000);
				    } else {
				        conv.u  = (t0 >> 9) | UINT32_C(0x3f800000);
				    }
				    return conv.f;
				}

				/**
				 * This function outputs floating point number from internal state.
				 * Users should not call this function directly.
				 * @param random tinymt internal status
				 * @return floating point number r (1.0 < r < 2.0)
				 */
				inline static float tinymt32_temper_conv_open(tinymt32_t * random) {
				    uint32_t t0, t1;
				    union {
				        uint32_t u;
				        float f;
				    } conv;

				    t0 = random->status[3];
				#if defined(LINEARITY_CHECK)
				    t1 = random->status[0]
				        ^ (random->status[2] >> TINYMT32_SH8);
				#else
				    t1 = random->status[0]
				        + (random->status[2] >> TINYMT32_SH8);
				#endif
				    t0 ^= t1;
				    if ((t1 & 1) != 0) {
				        conv.u  = ((t0 ^ random->tmat) >> 9) | UINT32_C(0x3f800001);
				    } else {
				        conv.u  = (t0 >> 9) | UINT32_C(0x3f800001);
				    }
				    return conv.f;
				}

				/**
				 * This function outputs 32-bit unsigned integer from internal state.
				 * @param random tinymt internal status
				 * @return 32-bit unsigned integer r (0 <= r < 2^32)
				 */
				inline static uint32_t tinymt32_generate_uint32(tinymt32_t * random) {
				    tinymt32_next_state(random);
				    return tinymt32_temper(random);
				}

				/**
				 * This function outputs floating point number from internal state.
				 * This function is implemented using multiplying by (1 / 2^24).
				 * floating point multiplication is faster than using union trick in
				 * my Intel CPU.
				 * @param random tinymt internal status
				 * @return floating point number r (0.0 <= r < 1.0)
				 */
				inline static float tinymt32_generate_float(tinymt32_t * random) {
				    tinymt32_next_state(random);
				    return (float)(tinymt32_temper(random) >> 8) * TINYMT32_MUL;
				}

				/**
				 * This function outputs floating point number from internal state.
				 * This function is implemented using union trick.
				 * @param random tinymt internal status
				 * @return floating point number r (1.0 <= r < 2.0)
				 */
				inline static float tinymt32_generate_float12(tinymt32_t * random) {
				    tinymt32_next_state(random);
				    return tinymt32_temper_conv(random);
				}

				/**
				 * This function outputs floating point number from internal state.
				 * This function is implemented using union trick.
				 * @param random tinymt internal status
				 * @return floating point number r (0.0 <= r < 1.0)
				 */
				inline static float tinymt32_generate_float01(tinymt32_t * random) {
				    tinymt32_next_state(random);
				    return tinymt32_temper_conv(random) - 1.0f;
				}

				/**
				 * This function outputs floating point number from internal state.
				 * This function may return 1.0 and never returns 0.0.
				 * @param random tinymt internal status
				 * @return floating point number r (0.0 < r <= 1.0)
				 */
				inline static float tinymt32_generate_floatOC(tinymt32_t * random) {
				    tinymt32_next_state(random);
				    return 1.0f - tinymt32_generate_float(random);
				}

				/**
				 * This function outputs floating point number from internal state.
				 * This function returns neither 0.0 nor 1.0.
				 * @param random tinymt internal status
				 * @return floating point number r (0.0 < r < 1.0)
				 */
				inline static float tinymt32_generate_floatOO(tinymt32_t * random) {
				    tinymt32_next_state(random);
				    return tinymt32_temper_conv_open(random) - 1.0f;
				}

				/**
				 * This function outputs double precision floating point number from
				 * internal state. The returned value has 32-bit precision.
				 * In other words, this function makes one double precision floating point
				 * number from one 32-bit unsigned integer.
				 * @param random tinymt internal status
				 * @return floating point number r (0.0 <= r < 1.0)
				 */
				inline static double tinymt32_generate_32double(tinymt32_t * random) {
				    tinymt32_next_state(random);
				    return tinymt32_temper(random) * (1.0 / 4294967296.0);
				}

				#if defined(__cplusplus)
				}
				#endif

				#define MIN_LOOP 8
				#define PRE_LOOP 8

				/**
				 * This function represents a function used in the initialization
				 * by init_by_array
				 * @param x 32-bit integer
				 * @return 32-bit integer
				 */
				static uint32_t ini_func1(uint32_t x) {
				    return (x ^ (x >> 27)) * UINT32_C(1664525);
				}

				/**
				 * This function represents a function used in the initialization
				 * by init_by_array
				 * @param x 32-bit integer
				 * @return 32-bit integer
				 */
				static uint32_t ini_func2(uint32_t x) {
				    return (x ^ (x >> 27)) * UINT32_C(1566083941);
				}

				/**
				 * This function certificate the period of 2^127-1.
				 * @param random tinymt state vector.
				 */
				static void period_certification(tinymt32_t * random) {
				    if ((random->status[0] & TINYMT32_MASK) == 0 &&
				        random->status[1] == 0 &&
				        random->status[2] == 0 &&
				        random->status[3] == 0) {
				        random->status[0] = 'T';
				        random->status[1] = 'I';
				        random->status[2] = 'N';
				        random->status[3] = 'Y';
				    }
				}

				/**
				 * This function initializes the internal state array with a 32-bit
				 * unsigned integer seed.
				 * @param random tinymt state vector.
				 * @param seed a 32-bit unsigned integer used as a seed.
				 */
				void tinymt32_init(tinymt32_t * random, uint32_t seed) {
				    random->status[0] = seed;
				    random->status[1] = random->mat1;
				    random->status[2] = random->mat2;
				    random->status[3] = random->tmat;
				    for (unsigned int i = 1; i < MIN_LOOP; i++) {
				        random->status[i & 3] ^= i + UINT32_C(1812433253)
				            * (random->status[(i - 1) & 3]
				               ^ (random->status[(i - 1) & 3] >> 30));
				    }
				    period_certification(random);
				    for (unsigned int i = 0; i < PRE_LOOP; i++) {
				        tinymt32_next_state(random);
				    }
				}

				/**
				 * This function initializes the internal state array,
				 * with an array of 32-bit unsigned integers used as seeds
				 * @param random tinymt state vector.
				 * @param init_key the array of 32-bit integers, used as a seed.
				 * @param key_length the length of init_key.
				 */
				void tinymt32_init_by_array(tinymt32_t * random, uint32_t init_key[],
				                            int key_length) {
				    const unsigned int lag = 1;
				    const unsigned int mid = 1;
				    const unsigned int size = 4;
				    unsigned int i, j;
				    unsigned int count;
				    uint32_t r;
				    uint32_t * st = &random->status[0];

				    st[0] = 0;
				    st[1] = random->mat1;
				    st[2] = random->mat2;
				    st[3] = random->tmat;
				    if (key_length + 1 > MIN_LOOP) {
				        count = (unsigned int)key_length + 1;
				    } else {
				        count = MIN_LOOP;
				    }
				    r = ini_func1(st[0] ^ st[mid % size]
				                  ^ st[(size - 1) % size]);
				    st[mid % size] += r;
				    r += (unsigned int)key_length;
				    st[(mid + lag) % size] += r;
				    st[0] = r;
				    count--;
				    for (i = 1, j = 0; (j < count) && (j < (unsigned int)key_length); j++) {
				        r = ini_func1(st[i % size]
				                      ^ st[(i + mid) % size]
				                      ^ st[(i + size - 1) % size]);
				        st[(i + mid) % size] += r;
				        r += init_key[j] + i;
				        st[(i + mid + lag) % size] += r;
				        st[i % size] = r;
				        i = (i + 1) % size;
				    }
				    for (; j < count; j++) {
				        r = ini_func1(st[i % size]
				                      ^ st[(i + mid) % size]
				                      ^ st[(i + size - 1) % size]);
				        st[(i + mid) % size] += r;
				        r += i;
				        st[(i + mid + lag) % size] += r;
				        st[i % size] = r;
				        i = (i + 1) % size;
				    }
				    for (j = 0; j < size; j++) {
				        r = ini_func2(st[i % size]
				                      + st[(i + mid) % size]
				                      + st[(i + size - 1) % size]);
				        st[(i + mid) % size] ^= r;
				        r -= i;
				        st[(i + mid + lag) % size] ^= r;
				        st[i % size] = r;
				        i = (i + 1) % size;
				    }
				    period_certification(random);
				    for (i = 0; i < PRE_LOOP; i++) {
				        tinymt32_next_state(random);
				    }
				}

				#undef MIN_LOOP
				#undef PRE_LOOP

				#if BOOST_COMP_CLANG
				#   pragma clang diagnostic pop
				#endif
				#if BOOST_COMP_GNUC
				#   pragma GCC diagnostic pop
				#endif
				#if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
				#   pragma warning(pop)
				#endif

				#endif
				// ==
				// == ./include/alpaka/rand/TinyMT/tinymt32.h ==
				// ============================================================================


			// #include <cstdint>    // amalgamate: file already included

			namespace alpaka::rand::engine::cpu
			{
			    //! Implementation of std::UniformRandomBitGenerator for TinyMT32
			    struct TinyMTengine
			    {
			        using result_type = std::uint32_t;

			        static constexpr auto default_seed() -> result_type
			        {
			            return 42u;
			        }

			        void seed(result_type value = default_seed())
			        {
			            // parameters from TinyMT/jump/sample.c
			            prng.mat1 = 0x8f7011ee;
			            prng.mat2 = 0xfc78ff1f;
			            prng.tmat = 0x3793fdff;

			            tinymt32_init(&prng, value);
			        }

			        TinyMTengine(std::uint32_t const& seedValue)
			        {
			            seed(seedValue);
			        }

			        TinyMTengine()
			        {
			            seed(default_seed());
			        }

			        auto operator()() -> result_type
			        {
			            return tinymt32_generate_uint32(&prng);
			        }

			        static constexpr auto min() -> result_type
			        {
			            return 0u;
			        }

			        static constexpr auto max() -> result_type
			        {
			            return UINT32_MAX;
			        }

			        void discard(unsigned long long) // z
			        {
			            // not implemented
			            // tinymt32_jump( &prng, z, z );
			        }

			        tinymt32_t prng;
			    };
			} // namespace alpaka::rand::engine::cpu
			// ==
			// == ./include/alpaka/rand/TinyMT/Engine.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/rand/Traits.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			// #include <cstdint>    // amalgamate: file already included
			// #include <type_traits>    // amalgamate: file already included

			namespace alpaka::rand
			{
			    struct ConceptRand
			    {
			    };

			    //! The random number generator distribution specifics.
			    namespace distribution
			    {
			        //! The random number generator distribution trait.
			        namespace trait
			        {
			            //! The random number float normal distribution get trait.
			            template<typename TRand, typename T, typename TSfinae = void>
			            struct CreateNormalReal;

			            //! The random number float uniform distribution get trait.
			            template<typename TRand, typename T, typename TSfinae = void>
			            struct CreateUniformReal;

			            //! The random number integer uniform distribution get trait.
			            template<typename TRand, typename T, typename TSfinae = void>
			            struct CreateUniformUint;
			        } // namespace trait

			        //! \return A normal float distribution with mean 0.0f and standard deviation 1.0f.
			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename T, typename TRand>
			        ALPAKA_FN_HOST_ACC auto createNormalReal(TRand const& rand)
			        {
			            static_assert(std::is_floating_point_v<T>, "The value type T has to be a floating point type!");

			            using ImplementationBase = concepts::ImplementationBase<ConceptRand, TRand>;
			            return trait::CreateNormalReal<ImplementationBase, T>::createNormalReal(rand);
			        }
			        //! \return A uniform floating point distribution [0.0, 1.0).
			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename T, typename TRand>
			        ALPAKA_FN_HOST_ACC auto createUniformReal(TRand const& rand)
			        {
			            static_assert(std::is_floating_point_v<T>, "The value type T has to be a floating point type!");

			            using ImplementationBase = concepts::ImplementationBase<ConceptRand, TRand>;
			            return trait::CreateUniformReal<ImplementationBase, T>::createUniformReal(rand);
			        }
			        //! \return A uniform integer distribution [0, UINT_MAX].
			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename T, typename TRand>
			        ALPAKA_FN_HOST_ACC auto createUniformUint(TRand const& rand)
			        {
			            static_assert(
			                std::is_integral_v<T> && std::is_unsigned_v<T>,
			                "The value type T has to be a unsigned integral type!");

			            using ImplementationBase = concepts::ImplementationBase<ConceptRand, TRand>;
			            return trait::CreateUniformUint<ImplementationBase, T>::createUniformUint(rand);
			        }
			    } // namespace distribution

			    //! The random number generator engine specifics.
			    namespace engine
			    {
			        //! The random number generator engine trait.
			        namespace trait
			        {
			            //! The random number default generator engine get trait.
			            template<typename TRand, typename TSfinae = void>
			            struct CreateDefault;
			        } // namespace trait
			        //! \return A default random number generator engine. Its type is guaranteed to be trivially copyable.
			        //!         Except HIP accelerator for HIP versions below 5.2 as its internal state was not trivially copyable.
			        //!         The limitation was discussed in PR #1778.
			        ALPAKA_NO_HOST_ACC_WARNING
			        template<typename TRand>
			        ALPAKA_FN_HOST_ACC auto createDefault(
			            TRand const& rand,
			            std::uint32_t const& seed = 0,
			            std::uint32_t const& subsequence = 0,
			            std::uint32_t const& offset = 0)
			        {
			            using ImplementationBase = concepts::ImplementationBase<ConceptRand, TRand>;
			            return trait::CreateDefault<ImplementationBase>::createDefault(rand, seed, subsequence, offset);
			        }
			    } // namespace engine
			} // namespace alpaka::rand
			// ==
			// == ./include/alpaka/rand/Traits.hpp ==
			// ============================================================================


		// #include <cstdint>    // amalgamate: file already included
		// #include <limits>    // amalgamate: file already included
		#include <random>
		// #include <type_traits>    // amalgamate: file already included

		namespace alpaka::rand
		{
		    //! "Tiny" state mersenne twister implementation
		    class TinyMersenneTwister : public concepts::Implements<ConceptRand, TinyMersenneTwister>
		    {
		    };
		    using RandStdLib = TinyMersenneTwister;

		    //! The standard library mersenne twister implementation.
		    class MersenneTwister : public concepts::Implements<ConceptRand, MersenneTwister>
		    {
		    };

		    //! The standard library rand device implementation.
		    class RandomDevice : public concepts::Implements<ConceptRand, RandomDevice>
		    {
		    };

		    namespace engine::cpu
		    {
		        //! The standard library mersenne twister random number generator.
		        //!
		        //! size of state: 19937 bytes
		        class MersenneTwister
		        {
		            std::mt19937 state;

		        public:
		            MersenneTwister() = default;

		            ALPAKA_FN_HOST MersenneTwister(
		                std::uint32_t const& seed,
		                std::uint32_t const& subsequence = 0,
		                std::uint32_t const& offset = 0)
		                : // NOTE: XOR the seed and the subsequence to generate a unique seed.
		                state((seed ^ subsequence) + offset)
		            {
		            }

		            // STL UniformRandomBitGenerator concept interface
		            using result_type = std::mt19937::result_type;
		            ALPAKA_FN_HOST constexpr static auto min() -> result_type
		            {
		                return std::mt19937::min();
		            }
		            ALPAKA_FN_HOST constexpr static auto max() -> result_type
		            {
		                return std::mt19937::max();
		            }
		            ALPAKA_FN_HOST auto operator()() -> result_type
		            {
		                return state();
		            }
		        };

		        //! "Tiny" state mersenne twister implementation
		        //!
		        //! repository: github.com/MersenneTwister-Lab/TinyMT
		        //!
		        //! license: 3-clause BSD
		        //!
		        //! @author Mutsuo Saito (Hiroshima University)Tokio University.
		        //! @author Makoto Matsumoto (The University of Tokyo)
		        //!
		        //! size of state: 28 bytes (127 bits?!)
		        class TinyMersenneTwister
		        {
		            TinyMTengine state;

		        public:
		            TinyMersenneTwister() = default;

		            ALPAKA_FN_HOST TinyMersenneTwister(
		                std::uint32_t const& seed,
		                std::uint32_t const& subsequence = 0,
		                std::uint32_t const& offset = 0)
		                : // NOTE: XOR the seed and the subsequence to generate a unique seed.
		                state((seed ^ subsequence) + offset)
		            {
		            }

		            // STL UniformRandomBitGenerator concept interface
		            using result_type = TinyMTengine::result_type;
		            ALPAKA_FN_HOST constexpr static auto min() -> result_type
		            {
		                return TinyMTengine::min();
		            }
		            ALPAKA_FN_HOST constexpr static auto max() -> result_type
		            {
		                return TinyMTengine::max();
		            }
		            ALPAKA_FN_HOST auto operator()() -> result_type
		            {
		                return state();
		            }
		        };

		        //! The standard library's random device based on the local entropy pool.
		        //!
		        //! Warning: the entropy pool on many devices degrates quickly and performance
		        //!          will drop significantly when this point occures.
		        //!
		        //! size of state: 1 byte
		        class RandomDevice
		        {
		            std::random_device state;

		        public:
		            RandomDevice() = default;

		            ALPAKA_FN_HOST RandomDevice(std::uint32_t const&, std::uint32_t const& = 0, std::uint32_t const& = 0)
		            {
		            }

		            // STL UniformRandomBitGenerator concept interface
		            using result_type = std::random_device::result_type;
		            ALPAKA_FN_HOST constexpr static auto min() -> result_type
		            {
		                return std::random_device::min();
		            }
		            ALPAKA_FN_HOST constexpr static auto max() -> result_type
		            {
		                return std::random_device::max();
		            }
		            ALPAKA_FN_HOST auto operator()() -> result_type
		            {
		                return state();
		            }
		        };
		    } // namespace engine::cpu

		    namespace distribution::cpu
		    {
		        //! The CPU random number normal distribution.
		        template<typename T>
		        struct NormalReal
		        {
		            template<typename TEngine>
		            ALPAKA_FN_HOST auto operator()(TEngine& engine) -> T
		            {
		                return m_dist(engine);
		            }

		        private:
		            std::normal_distribution<T> m_dist;
		        };

		        //! The CPU random number uniform distribution.
		        template<typename T>
		        struct UniformReal
		        {
		            template<typename TEngine>
		            ALPAKA_FN_HOST auto operator()(TEngine& engine) -> T
		            {
		                return m_dist(engine);
		            }

		        private:
		            std::uniform_real_distribution<T> m_dist;
		        };

		        //! The CPU random number normal distribution.
		        template<typename T>
		        struct UniformUint
		        {
		            template<typename TEngine>
		            ALPAKA_FN_HOST auto operator()(TEngine& engine) -> T
		            {
		                return m_dist(engine);
		            }

		        private:
		            std::uniform_int_distribution<T> m_dist{
		                0, // For signed integer: std::numeric_limits<T>::lowest()
		                std::numeric_limits<T>::max()};
		        };
		    } // namespace distribution::cpu

		    namespace distribution::trait
		    {
		        //! The CPU device random number float normal distribution get trait specialization.
		        template<typename T>
		        struct CreateNormalReal<RandStdLib, T, std::enable_if_t<std::is_floating_point_v<T>>>
		        {
		            ALPAKA_FN_HOST static auto createNormalReal(RandStdLib const& /* rand */) -> cpu::NormalReal<T>
		            {
		                return {};
		            }
		        };
		        //! The CPU device random number float uniform distribution get trait specialization.
		        template<typename T>
		        struct CreateUniformReal<RandStdLib, T, std::enable_if_t<std::is_floating_point_v<T>>>
		        {
		            ALPAKA_FN_HOST static auto createUniformReal(RandStdLib const& /* rand */) -> cpu::UniformReal<T>
		            {
		                return {};
		            }
		        };
		        //! The CPU device random number integer uniform distribution get trait specialization.
		        template<typename T>
		        struct CreateUniformUint<RandStdLib, T, std::enable_if_t<std::is_integral_v<T>>>
		        {
		            ALPAKA_FN_HOST static auto createUniformUint(RandStdLib const& /* rand */) -> cpu::UniformUint<T>
		            {
		                return {};
		            }
		        };
		    } // namespace distribution::trait

		    namespace engine::trait
		    {
		        //! The CPU device random number default generator get trait specialization.
		        template<>
		        struct CreateDefault<TinyMersenneTwister>
		        {
		            ALPAKA_FN_HOST static auto createDefault(
		                TinyMersenneTwister const& /* rand */,
		                std::uint32_t const& seed = 0,
		                std::uint32_t const& subsequence = 0,
		                std::uint32_t const& offset = 0) -> cpu::TinyMersenneTwister
		            {
		                return {seed, subsequence, offset};
		            }
		        };

		        template<>
		        struct CreateDefault<MersenneTwister>
		        {
		            ALPAKA_FN_HOST static auto createDefault(
		                MersenneTwister const& /* rand */,
		                std::uint32_t const& seed = 0,
		                std::uint32_t const& subsequence = 0,
		                std::uint32_t const& offset = 0) -> cpu::MersenneTwister
		            {
		                return {seed, subsequence, offset};
		            }
		        };

		        template<>
		        struct CreateDefault<RandomDevice>
		        {
		            ALPAKA_FN_HOST static auto createDefault(
		                RandomDevice const& /* rand */,
		                std::uint32_t const& seed = 0,
		                std::uint32_t const& subsequence = 0,
		                std::uint32_t const& offset = 0) -> cpu::RandomDevice
		            {
		                return {seed, subsequence, offset};
		            }
		        };
		    } // namespace engine::trait
		} // namespace alpaka::rand
		// ==
		// == ./include/alpaka/rand/RandStdLib.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/warp/WarpSingleThread.hpp ==
		// ==
		/* Copyright 2022 Sergei Bastrakov, David M. Rogers, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
			// ============================================================================
			// == ./include/alpaka/warp/Traits.hpp ==
			// ==
			/* Copyright 2022 Sergei Bastrakov, David M. Rogers, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			// #include <cstdint>    // amalgamate: file already included
			// #include <type_traits>    // amalgamate: file already included

			namespace alpaka::warp
			{
			    struct ConceptWarp
			    {
			    };

			    //! The warp traits.
			    namespace trait
			    {
			        //! The warp size trait.
			        template<typename TWarp, typename TSfinae = void>
			        struct GetSize;

			        //! The all warp vote trait.
			        template<typename TWarp, typename TSfinae = void>
			        struct All;

			        //! The any warp vote trait.
			        template<typename TWarp, typename TSfinae = void>
			        struct Any;

			        //! The ballot warp vote trait.
			        template<typename TWarp, typename TSfinae = void>
			        struct Ballot;

			        //! The shfl warp swizzling trait.
			        template<typename TWarp, typename TSfinae = void>
			        struct Shfl;

			        //! The active mask trait.
			        template<typename TWarp, typename TSfinae = void>
			        struct Activemask;
			    } // namespace trait

			    //! Returns warp size.
			    //!
			    //! \tparam TWarp The warp implementation type.
			    //! \param warp The warp implementation.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TWarp>
			    ALPAKA_FN_ACC auto getSize(TWarp const& warp) -> std::int32_t
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptWarp, TWarp>;
			        return trait::GetSize<ImplementationBase>::getSize(warp);
			    }

			    //! Returns a 32- or 64-bit unsigned integer (depending on the
			    //! accelerator) whose Nth bit is set if and only if the Nth thread
			    //! of the warp is active.
			    //!
			    //! Note: decltype for return type is required there, otherwise
			    //! compilcation with a CPU and a GPU accelerator enabled fails as it
			    //! tries to call device function from a host-device one. The reason
			    //! is unclear, but likely related to deducing the return type.
			    //!
			    //! Note:
			    //! * The programmer must ensure that all threads calling this function are executing
			    //!   the same line of code. In particular it is not portable to write
			    //!   if(a) {activemask} else {activemask}.
			    //!
			    //! \tparam TWarp The warp implementation type.
			    //! \param warp The warp implementation.
			    //! \return 32-bit or 64-bit unsigned type depending on the accelerator.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TWarp>
			    ALPAKA_FN_ACC auto activemask(TWarp const& warp)
			        -> decltype(trait::Activemask<concepts::ImplementationBase<ConceptWarp, TWarp>>::activemask(warp))
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptWarp, TWarp>;
			        return trait::Activemask<ImplementationBase>::activemask(warp);
			    }

			    //! Evaluates predicate for all active threads of the warp and returns
			    //! non-zero if and only if predicate evaluates to non-zero for all of them.
			    //!
			    //! It follows the logic of __all(predicate) in CUDA before version 9.0 and HIP,
			    //! the operation is applied for all active threads.
			    //! The modern CUDA counterpart would be __all_sync(__activemask(), predicate).
			    //!
			    //! Note:
			    //! * The programmer must ensure that all threads calling this function are executing
			    //!   the same line of code. In particular it is not portable to write
			    //!   if(a) {all} else {all}.
			    //!
			    //! \tparam TWarp The warp implementation type.
			    //! \param warp The warp implementation.
			    //! \param predicate The predicate value for current thread.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TWarp>
			    ALPAKA_FN_ACC auto all(TWarp const& warp, std::int32_t predicate) -> std::int32_t
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptWarp, TWarp>;
			        return trait::All<ImplementationBase>::all(warp, predicate);
			    }

			    //! Evaluates predicate for all active threads of the warp and returns
			    //! non-zero if and only if predicate evaluates to non-zero for any of them.
			    //!
			    //! It follows the logic of __any(predicate) in CUDA before version 9.0 and HIP,
			    //! the operation is applied for all active threads.
			    //! The modern CUDA counterpart would be __any_sync(__activemask(), predicate).
			    //!
			    //! Note:
			    //! * The programmer must ensure that all threads calling this function are executing
			    //!   the same line of code. In particular it is not portable to write
			    //!   if(a) {any} else {any}.
			    //!
			    //! \tparam TWarp The warp implementation type.
			    //! \param warp The warp implementation.
			    //! \param predicate The predicate value for current thread.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TWarp>
			    ALPAKA_FN_ACC auto any(TWarp const& warp, std::int32_t predicate) -> std::int32_t
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptWarp, TWarp>;
			        return trait::Any<ImplementationBase>::any(warp, predicate);
			    }

			    //! Evaluates predicate for all non-exited threads in a warp and returns
			    //! a 32- or 64-bit unsigned integer (depending on the accelerator)
			    //! whose Nth bit is set if and only if predicate evaluates to non-zero
			    //! for the Nth thread of the warp and the Nth thread is active.
			    //!
			    //! It follows the logic of __ballot(predicate) in CUDA before version 9.0 and HIP,
			    //! the operation is applied for all active threads.
			    //! The modern CUDA counterpart would be __ballot_sync(__activemask(), predicate).
			    //! Return type is 64-bit to fit all platforms.
			    //!
			    //! Note:
			    //! * The programmer must ensure that all threads calling this function are executing
			    //!   the same line of code. In particular it is not portable to write
			    //!   if(a) {ballot} else {ballot}.
			    //!
			    //! \tparam TWarp The warp implementation type.
			    //! \param warp The warp implementation.
			    //! \param predicate The predicate value for current thread.
			    //! \return 32-bit or 64-bit unsigned type depending on the accelerator.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TWarp>
			    ALPAKA_FN_ACC auto ballot(TWarp const& warp, std::int32_t predicate)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptWarp, TWarp>;
			        return trait::Ballot<ImplementationBase>::ballot(warp, predicate);
			    }

			    //! Exchange data between threads within a warp.
			    //!
			    //! Effectively executes:
			    //!
			    //!     __shared__ int32_t values[warpsize];
			    //!     values[threadIdx.x] = value;
			    //!     __syncthreads();
			    //!     return values[(srcLane + width*floor(threadIdx.x/width))%width];
			    //!
			    //! However, it does not use shared memory.
			    //!
			    //! Notes:
			    //! * The programmer must ensure that all threads calling this
			    //!   function (and the srcLane) are executing the same line of code.
			    //!   In particular it is not portable to write if(a) {shfl} else {shfl}.
			    //!
			    //! * Commonly used with width = warpsize (the default), (returns values[srcLane])
			    //!
			    //! * Width must be a power of 2.
			    //!
			    //! \tparam TWarp   warp implementation type
			    //! \param  warp    warp implementation
			    //! \param  value   value to broadcast (only meaningful from threadIdx == srcLane)
			    //! \param  srcLane source lane sending value
			    //! \param  width   number of threads receiving a single value
			    //! \return val from the thread index srcLane.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TWarp>
			    ALPAKA_FN_ACC auto shfl(TWarp const& warp, std::int32_t value, std::int32_t srcLane, std::int32_t width = 0)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptWarp, TWarp>;
			        return trait::Shfl<ImplementationBase>::shfl(warp, value, srcLane, width ? width : getSize(warp));
			    }

			    //! shfl for float vals
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TWarp>
			    ALPAKA_FN_ACC auto shfl(TWarp const& warp, float value, std::int32_t srcLane, std::int32_t width = 0)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptWarp, TWarp>;
			        return trait::Shfl<ImplementationBase>::shfl(warp, value, srcLane, width ? width : getSize(warp));
			    }
			} // namespace alpaka::warp
			// ==
			// == ./include/alpaka/warp/Traits.hpp ==
			// ============================================================================


		// #include <cstdint>    // amalgamate: file already included

		namespace alpaka::warp
		{
		    //! The single-threaded warp to emulate it on CPUs.
		    class WarpSingleThread : public concepts::Implements<ConceptWarp, WarpSingleThread>
		    {
		    };

		    namespace trait
		    {
		        template<>
		        struct GetSize<WarpSingleThread>
		        {
		            static auto getSize(warp::WarpSingleThread const& /*warp*/)
		            {
		                return 1;
		            }
		        };

		        template<>
		        struct Activemask<WarpSingleThread>
		        {
		            static auto activemask(warp::WarpSingleThread const& /*warp*/)
		            {
		                return 1u;
		            }
		        };

		        template<>
		        struct All<WarpSingleThread>
		        {
		            static auto all(warp::WarpSingleThread const& /*warp*/, std::int32_t predicate)
		            {
		                return predicate;
		            }
		        };

		        template<>
		        struct Any<WarpSingleThread>
		        {
		            static auto any(warp::WarpSingleThread const& /*warp*/, std::int32_t predicate)
		            {
		                return predicate;
		            }
		        };

		        template<>
		        struct Ballot<WarpSingleThread>
		        {
		            static auto ballot(warp::WarpSingleThread const& /*warp*/, std::int32_t predicate)
		            {
		                return predicate ? 1u : 0u;
		            }
		        };

		        template<>
		        struct Shfl<WarpSingleThread>
		        {
		            static auto shfl(
		                warp::WarpSingleThread const& /*warp*/,
		                std::int32_t val,
		                std::int32_t /*srcLane*/,
		                std::int32_t /*width*/)
		            {
		                return val;
		            }

		            static auto shfl(
		                warp::WarpSingleThread const& /*warp*/,
		                float val,
		                std::int32_t /*srcLane*/,
		                std::int32_t /*width*/)
		            {
		                return val;
		            }
		        };
		    } // namespace trait
		} // namespace alpaka::warp
		// ==
		// == ./include/alpaka/warp/WarpSingleThread.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/workdiv/WorkDivMembers.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Matthias Werner, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
		// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/workdiv/Traits.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included
			// #include <utility>    // amalgamate: file already included

			namespace alpaka
			{
			    struct ConceptWorkDiv
			    {
			    };

			    //! The work division trait.
			    namespace trait
			    {
			        //! The work div trait.
			        template<typename TWorkDiv, typename TOrigin, typename TUnit, typename TSfinae = void>
			        struct GetWorkDiv;
			    } // namespace trait

			    //! Get the extent requested.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TOrigin, typename TUnit, typename TWorkDiv>
			    ALPAKA_FN_HOST_ACC auto getWorkDiv(TWorkDiv const& workDiv) -> Vec<Dim<TWorkDiv>, Idx<TWorkDiv>>
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptWorkDiv, TWorkDiv>;
			        return trait::GetWorkDiv<ImplementationBase, TOrigin, TUnit>::getWorkDiv(workDiv);
			    }

			    namespace trait
			    {
			        //! The work div grid thread extent trait specialization.
			        template<typename TWorkDiv>
			        struct GetWorkDiv<TWorkDiv, origin::Grid, unit::Threads>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static auto getWorkDiv(TWorkDiv const& workDiv)
			            {
			                return alpaka::getWorkDiv<origin::Grid, unit::Blocks>(workDiv)
			                       * alpaka::getWorkDiv<origin::Block, unit::Threads>(workDiv);
			            }
			        };
			        //! The work div grid element extent trait specialization.
			        template<typename TWorkDiv>
			        struct GetWorkDiv<TWorkDiv, origin::Grid, unit::Elems>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static auto getWorkDiv(TWorkDiv const& workDiv)
			            {
			                return alpaka::getWorkDiv<origin::Grid, unit::Threads>(workDiv)
			                       * alpaka::getWorkDiv<origin::Thread, unit::Elems>(workDiv);
			            }
			        };
			        //! The work div block element extent trait specialization.
			        template<typename TWorkDiv>
			        struct GetWorkDiv<TWorkDiv, origin::Block, unit::Elems>
			        {
			            ALPAKA_NO_HOST_ACC_WARNING
			            ALPAKA_FN_HOST_ACC static auto getWorkDiv(TWorkDiv const& workDiv)
			            {
			                return alpaka::getWorkDiv<origin::Block, unit::Threads>(workDiv)
			                       * alpaka::getWorkDiv<origin::Thread, unit::Elems>(workDiv);
			            }
			        };
			    } // namespace trait
			} // namespace alpaka
			// ==
			// == ./include/alpaka/workdiv/Traits.hpp ==
			// ============================================================================


		#include <iosfwd>

		namespace alpaka
		{
		    //! A basic class holding the work division as grid block extent, block thread and thread element extent.
		    template<typename TDim, typename TIdx>
		    class WorkDivMembers : public concepts::Implements<ConceptWorkDiv, WorkDivMembers<TDim, TIdx>>
		    {
		    public:
		        ALPAKA_FN_HOST_ACC WorkDivMembers() = delete;
		        ALPAKA_NO_HOST_ACC_WARNING
		        template<typename TGridBlockExtent, typename TBlockThreadExtent, typename TThreadElemExtent>
		        ALPAKA_FN_HOST_ACC explicit WorkDivMembers(
		            TGridBlockExtent const& gridBlockExtent = TGridBlockExtent(),
		            TBlockThreadExtent const& blockThreadExtent = TBlockThreadExtent(),
		            TThreadElemExtent const& threadElemExtent = TThreadElemExtent())
		            : m_gridBlockExtent(getExtentVecEnd<TDim>(gridBlockExtent))
		            , m_blockThreadExtent(getExtentVecEnd<TDim>(blockThreadExtent))
		            , m_threadElemExtent(getExtentVecEnd<TDim>(threadElemExtent))
		        {
		        }
		        ALPAKA_NO_HOST_ACC_WARNING
		        ALPAKA_FN_HOST_ACC WorkDivMembers(WorkDivMembers const& other)
		            : m_gridBlockExtent(other.m_gridBlockExtent)
		            , m_blockThreadExtent(other.m_blockThreadExtent)
		            , m_threadElemExtent(other.m_threadElemExtent)
		        {
		        }
		        ALPAKA_NO_HOST_ACC_WARNING
		        template<typename TWorkDiv>
		        ALPAKA_FN_HOST_ACC explicit WorkDivMembers(TWorkDiv const& other)
		            : m_gridBlockExtent(subVecEnd<TDim>(getWorkDiv<Grid, Blocks>(other)))
		            , m_blockThreadExtent(subVecEnd<TDim>(getWorkDiv<Block, Threads>(other)))
		            , m_threadElemExtent(subVecEnd<TDim>(getWorkDiv<Thread, Elems>(other)))
		        {
		        }

		        WorkDivMembers(WorkDivMembers&&) = default;
		        auto operator=(WorkDivMembers const&) -> WorkDivMembers& = default;
		        auto operator=(WorkDivMembers&&) -> WorkDivMembers& = default;
		        ALPAKA_NO_HOST_ACC_WARNING
		        template<typename TWorkDiv>
		        ALPAKA_FN_HOST_ACC auto operator=(TWorkDiv const& other) -> WorkDivMembers<TDim, TIdx>&
		        {
		            m_gridBlockExtent = subVecEnd<TDim>(getWorkDiv<Grid, Blocks>(other));
		            m_blockThreadExtent = subVecEnd<TDim>(getWorkDiv<Block, Threads>(other));
		            m_threadElemExtent = subVecEnd<TDim>(getWorkDiv<Thread, Elems>(other));
		            return *this;
		        }

		        ALPAKA_FN_HOST_ACC friend constexpr auto operator==(WorkDivMembers const& a, WorkDivMembers const& b) -> bool
		        {
		            return a.m_gridBlockExtent == b.m_gridBlockExtent && a.m_blockThreadExtent == b.m_blockThreadExtent
		                   && a.m_threadElemExtent == b.m_threadElemExtent;
		        }

		        ALPAKA_FN_HOST friend auto operator<<(std::ostream& os, WorkDivMembers const& workDiv) -> std::ostream&
		        {
		            return os << "{gridBlockExtent: " << workDiv.m_gridBlockExtent
		                      << ", blockThreadExtent: " << workDiv.m_blockThreadExtent
		                      << ", threadElemExtent: " << workDiv.m_threadElemExtent << "}";
		        }

		    public:
		        Vec<TDim, TIdx> m_gridBlockExtent;
		        Vec<TDim, TIdx> m_blockThreadExtent;
		        Vec<TDim, TIdx> m_threadElemExtent;
		    };

		    namespace trait
		    {
		        //! The WorkDivMembers dimension get trait specialization.
		        template<typename TDim, typename TIdx>
		        struct DimType<WorkDivMembers<TDim, TIdx>>
		        {
		            using type = TDim;
		        };

		        //! The WorkDivMembers idx type trait specialization.
		        template<typename TDim, typename TIdx>
		        struct IdxType<WorkDivMembers<TDim, TIdx>>
		        {
		            using type = TIdx;
		        };

		        //! The WorkDivMembers grid block extent trait specialization.
		        template<typename TDim, typename TIdx>
		        struct GetWorkDiv<WorkDivMembers<TDim, TIdx>, origin::Grid, unit::Blocks>
		        {
		            //! \return The number of blocks in each dimension of the grid.
		            ALPAKA_NO_HOST_ACC_WARNING
		            ALPAKA_FN_HOST_ACC static auto getWorkDiv(WorkDivMembers<TDim, TIdx> const& workDiv) -> Vec<TDim, TIdx>
		            {
		                return workDiv.m_gridBlockExtent;
		            }
		        };

		        //! The WorkDivMembers block thread extent trait specialization.
		        template<typename TDim, typename TIdx>
		        struct GetWorkDiv<WorkDivMembers<TDim, TIdx>, origin::Block, unit::Threads>
		        {
		            //! \return The number of threads in each dimension of a block.
		            ALPAKA_NO_HOST_ACC_WARNING
		            ALPAKA_FN_HOST_ACC static auto getWorkDiv(WorkDivMembers<TDim, TIdx> const& workDiv) -> Vec<TDim, TIdx>
		            {
		                return workDiv.m_blockThreadExtent;
		            }
		        };

		        //! The WorkDivMembers thread element extent trait specialization.
		        template<typename TDim, typename TIdx>
		        struct GetWorkDiv<WorkDivMembers<TDim, TIdx>, origin::Thread, unit::Elems>
		        {
		            //! \return The number of elements in each dimension of a thread.
		            ALPAKA_NO_HOST_ACC_WARNING
		            ALPAKA_FN_HOST_ACC static auto getWorkDiv(WorkDivMembers<TDim, TIdx> const& workDiv) -> Vec<TDim, TIdx>
		            {
		                return workDiv.m_threadElemExtent;
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/workdiv/WorkDivMembers.hpp ==
		// ============================================================================


	// Specialized traits.
		// ============================================================================
		// == ./include/alpaka/acc/Traits.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
			// ============================================================================
			// == ./include/alpaka/acc/AccDevProps.hpp ==
			// ==
			/* Copyright 2020 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

			// #include <string>    // amalgamate: file already included
			#include <vector>

			namespace alpaka
			{
			    //! The acceleration properties on a device.
			    //
			    // \TODO:
			    //  TIdx m_maxClockFrequencyHz;            //!< Maximum clock frequency of the device in Hz.
			    template<typename TDim, typename TIdx>
			    struct AccDevProps
			    {
			        static_assert(
			            sizeof(TIdx) >= sizeof(int),
			            "Index type is not supported, consider using int or a larger type.");
			        ALPAKA_FN_HOST AccDevProps(
			            TIdx const& multiProcessorCount,
			            Vec<TDim, TIdx> const& gridBlockExtentMax,
			            TIdx const& gridBlockCountMax,
			            Vec<TDim, TIdx> const& blockThreadExtentMax,
			            TIdx const& blockThreadCountMax,
			            Vec<TDim, TIdx> const& threadElemExtentMax,
			            TIdx const& threadElemCountMax,
			            size_t const& sharedMemSizeBytes)
			            : m_gridBlockExtentMax(gridBlockExtentMax)
			            , m_blockThreadExtentMax(blockThreadExtentMax)
			            , m_threadElemExtentMax(threadElemExtentMax)
			            , m_gridBlockCountMax(gridBlockCountMax)
			            , m_blockThreadCountMax(blockThreadCountMax)
			            , m_threadElemCountMax(threadElemCountMax)
			            , m_multiProcessorCount(multiProcessorCount)
			            , m_sharedMemSizeBytes(sharedMemSizeBytes)
			        {
			        }

			        // NOTE: The members have been reordered from the order in the constructor because gcc is buggy for some TDim
			        // and TIdx and generates invalid assembly.
			        Vec<TDim, TIdx> m_gridBlockExtentMax; //!< The maximum number of blocks in each dimension of the grid.
			        Vec<TDim, TIdx> m_blockThreadExtentMax; //!< The maximum number of threads in each dimension of a block.
			        Vec<TDim, TIdx> m_threadElemExtentMax; //!< The maximum number of elements in each dimension of a thread.

			        TIdx m_gridBlockCountMax; //!< The maximum number of blocks in a grid.
			        TIdx m_blockThreadCountMax; //!< The maximum number of threads in a block.
			        TIdx m_threadElemCountMax; //!< The maximum number of elements in a threads.

			        TIdx m_multiProcessorCount; //!< The number of multiprocessors.
			        size_t m_sharedMemSizeBytes; //!< The size of shared memory per block
			    };
			} // namespace alpaka
			// ==
			// == ./include/alpaka/acc/AccDevProps.hpp ==
			// ============================================================================

		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/dev/Traits.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber, Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			// #include <cstddef>    // amalgamate: file already included
			// #include <string>    // amalgamate: file already included
			// #include <vector>    // amalgamate: file already included

			namespace alpaka
			{
			    //! The device traits.
			    namespace trait
			    {
			        //! The device type trait.
			        template<typename T, typename TSfinae = void>
			        struct DevType;

			        //! The device get trait.
			        template<typename T, typename TSfinae = void>
			        struct GetDev;

			        //! The device name get trait.
			        template<typename TDev, typename TSfinae = void>
			        struct GetName;

			        //! The device memory size get trait.
			        template<typename TDev, typename TSfinae = void>
			        struct GetMemBytes;

			        //! The device free memory size get trait.
			        template<typename T, typename TSfinae = void>
			        struct GetFreeMemBytes;

			        //! The device warp size get trait.
			        template<typename T, typename TSfinae = void>
			        struct GetWarpSizes;

			        //! The device reset trait.
			        template<typename T, typename TSfinae = void>
			        struct Reset;
			    } // namespace trait

			    //! The device type trait alias template to remove the ::type.
			    template<typename T>
			    using Dev = typename trait::DevType<T>::type;

			    struct ConceptGetDev;

			    struct ConceptDev;

			    //! True if TDev is a device, i.e. if it implements the ConceptDev concept.
			    template<typename TDev>
			    inline constexpr bool isDevice = concepts::ImplementsConcept<ConceptDev, TDev>::value;

			    //! \return The device this object is bound to.
			    template<typename T>
			    ALPAKA_FN_HOST auto getDev(T const& t)
			    {
			        using ImplementationBase = concepts::ImplementationBase<ConceptGetDev, T>;
			        return trait::GetDev<ImplementationBase>::getDev(t);
			    }

			    //! \return The device name.
			    template<typename TDev>
			    ALPAKA_FN_HOST auto getName(TDev const& dev) -> std::string
			    {
			        return trait::GetName<TDev>::getName(dev);
			    }

			    //! \return The memory on the device in Bytes. Returns 0 if querying memory
			    //!  is not supported.
			    template<typename TDev>
			    ALPAKA_FN_HOST auto getMemBytes(TDev const& dev) -> std::size_t
			    {
			        return trait::GetMemBytes<TDev>::getMemBytes(dev);
			    }

			    //! \return The free memory on the device in Bytes.
			    //
			    //! \note Do not use this query if getMemBytes returned 0.
			    template<typename TDev>
			    ALPAKA_FN_HOST auto getFreeMemBytes(TDev const& dev) -> std::size_t
			    {
			        return trait::GetFreeMemBytes<TDev>::getFreeMemBytes(dev);
			    }

			    //! \return The supported warp sizes on the device in number of threads.
			    template<typename TDev>
			    ALPAKA_FN_HOST auto getWarpSizes(TDev const& dev) -> std::vector<std::size_t>
			    {
			        return trait::GetWarpSizes<TDev>::getWarpSizes(dev);
			    }

			    //! Resets the device.
			    //! What this method does is dependent on the accelerator.
			    template<typename TDev>
			    ALPAKA_FN_HOST auto reset(TDev const& dev) -> void
			    {
			        trait::Reset<TDev>::reset(dev);
			    }

			    namespace trait
			    {
			        //! Get device type
			        template<typename TDev>
			        struct DevType<TDev, std::enable_if_t<concepts::ImplementsConcept<ConceptDev, TDev>::value>>
			        {
			            using type = typename concepts::ImplementationBase<ConceptDev, TDev>;
			        };
			    } // namespace trait
			} // namespace alpaka
			// ==
			// == ./include/alpaka/dev/Traits.hpp ==
			// ============================================================================

		// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/kernel/Traits.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, René Widera, Sergei Bastrakov, Jan Stephan, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Debug.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/core/OmpSchedule.hpp ==
				// ==
				/* Copyright 2022 Sergei Bastrakov, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

				#ifdef _OPENMP
				#    include <omp.h>
				#endif

				// #include <cstdint>    // amalgamate: file already included


				namespace alpaka::omp
				{
				    //! Representation of OpenMP schedule information: kind and chunk size. This class can be used regardless of
				    //! whether OpenMP is enabled.
				    struct Schedule
				    {
				        //! Schedule kinds corresponding to arguments of OpenMP schedule clause
				        //!
				        //! Kinds also present in omp_sched_t enum have the same integer values.
				        //! It is enum, not enum class, for shorter usage as omp::Schedule::[kind] and to keep interface of 0.6.0.
				        enum Kind
				        {
				            // Corresponds to not setting schedule
				            NoSchedule,
				            Static = 1u,
				            Dynamic = 2u,
				            Guided = 3u,
				            // Auto supported since OpenMP 3.0
				#if defined _OPENMP && _OPENMP >= 200805
				            Auto = 4u,
				#endif
				            Runtime = 5u
				        };

				        //! Schedule kind.
				        Kind kind;

				        //! Chunk size. Same as in OpenMP, value 0 corresponds to default chunk size. Using int and not a
				        //! fixed-width type to match OpenMP API.
				        int chunkSize;

				        //! Create a schedule with the given kind and chunk size
				        ALPAKA_FN_HOST constexpr Schedule(Kind myKind = NoSchedule, int myChunkSize = 0)
				            : kind(myKind)
				            , chunkSize(myChunkSize)
				        {
				        }
				    };

				    //! Get the OpenMP schedule that is applied when the runtime schedule is used.
				    //!
				    //! For OpenMP >= 3.0 returns the value of the internal control variable run-sched-var.
				    //! Without OpenMP or with OpenMP < 3.0, returns the default schedule.
				    //!
				    //! \return Schedule object.
				    ALPAKA_FN_HOST inline auto getSchedule()
				    {
				        // Getting a runtime schedule requires OpenMP 3.0 or newer
				#if defined _OPENMP && _OPENMP >= 200805
				        omp_sched_t ompKind;
				        int chunkSize = 0;
				        omp_get_schedule(&ompKind, &chunkSize);
				        return Schedule{static_cast<Schedule::Kind>(ompKind), chunkSize};
				#else
				        return Schedule{};
				#endif
				    }

				    //! Set the OpenMP schedule that is applied when the runtime schedule is used for future parallel regions.
				    //!
				    //! For OpenMP >= 3.0 sets the value of the internal control variable run-sched-var according to the given
				    //! schedule. Without OpenMP or with OpenMP < 3.0, does nothing.
				    //!
				    //! Note that calling from inside a parallel region does not have an immediate effect.
				    ALPAKA_FN_HOST inline void setSchedule(Schedule schedule)
				    {
				        if((schedule.kind != Schedule::NoSchedule) && (schedule.kind != Schedule::Runtime))
				        {
				#if defined _OPENMP && _OPENMP >= 200805
				            omp_set_schedule(static_cast<omp_sched_t>(schedule.kind), schedule.chunkSize);
				#endif
				        }
				    }
				} // namespace alpaka::omp
				// ==
				// == ./include/alpaka/core/OmpSchedule.hpp ==
				// ============================================================================

			// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/queue/Traits.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/wait/Traits.hpp ==
					// ==
					/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
					// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

					namespace alpaka
					{
					    struct ConceptCurrentThreadWaitFor
					    {
					    };

					    //! The wait traits.
					    namespace trait
					    {
					        //! The thread wait trait.
					        template<typename TAwaited, typename TSfinae = void>
					        struct CurrentThreadWaitFor;

					        //! The waiter wait trait.
					        template<typename TWaiter, typename TAwaited, typename TSfinae = void>
					        struct WaiterWaitFor;
					    } // namespace trait

					    //! Waits the thread for the completion of the given awaited action to complete.
					    template<typename TAwaited>
					    ALPAKA_FN_HOST auto wait(TAwaited const& awaited) -> void
					    {
					        using ImplementationBase = concepts::ImplementationBase<ConceptCurrentThreadWaitFor, TAwaited>;
					        trait::CurrentThreadWaitFor<ImplementationBase>::currentThreadWaitFor(awaited);
					    }

					    //! The waiter waits for the given awaited action to complete.
					    template<typename TWaiter, typename TAwaited>
					    ALPAKA_FN_HOST auto wait(TWaiter& waiter, TAwaited const& awaited) -> void
					    {
					        trait::WaiterWaitFor<TWaiter, TAwaited>::waiterWaitFor(waiter, awaited);
					    }
					} // namespace alpaka
					// ==
					// == ./include/alpaka/wait/Traits.hpp ==
					// ============================================================================


				// #include <type_traits>    // amalgamate: file already included
				// #include <utility>    // amalgamate: file already included

				namespace alpaka
				{
				    struct ConceptQueue;

				    //! True if TQueue is a queue, i.e. if it implements the ConceptQueue concept.
				    template<typename TQueue>
				    inline constexpr bool isQueue = concepts::ImplementsConcept<ConceptQueue, TQueue>::value;

				    //! The queue traits.
				    namespace trait
				    {
				        //! The queue enqueue trait.
				        template<typename TQueue, typename TTask, typename TSfinae = void>
				        struct Enqueue;

				        //! The queue empty trait.
				        template<typename TQueue, typename TSfinae = void>
				        struct Empty;

				        //! Queue for an accelerator
				        template<typename TAcc, typename TProperty, typename TSfinae = void>
				        struct QueueType;
				    } // namespace trait

				    //! Queues the given task in the given queue.
				    //!
				    //! Special Handling for events:
				    //!   If the event has previously been queued, then this call will overwrite any existing state of the event.
				    //!   Any subsequent calls which examine the status of event will only examine the completion of this most recent
				    //!   call to enqueue.
				    template<typename TQueue, typename TTask>
				    ALPAKA_FN_HOST auto enqueue(TQueue& queue, TTask&& task) -> void
				    {
				        trait::Enqueue<TQueue, std::decay_t<TTask>>::enqueue(queue, std::forward<TTask>(task));
				    }

				    //! Tests if the queue is empty (all ops in the given queue have been completed).
				    //!
				    //! \warning This function is allowed to return false negatives. An empty queue can reported as
				    //! non empty because the status information are not fully propagated by the used alpaka backend.
				    //! \return true queue is empty else false.
				    template<typename TQueue>
				    ALPAKA_FN_HOST auto empty(TQueue const& queue) -> bool
				    {
				        using ImplementationBase = concepts::ImplementationBase<ConceptQueue, TQueue>;
				        return trait::Empty<ImplementationBase>::empty(queue);
				    }

				    //! Queue based on the environment and a property
				    //!
				    //! \tparam TEnv Environment type, e.g.  accelerator, device or a platform.
				    //!              trait::QueueType must be specialized for TEnv
				    //! \tparam TProperty Property to define the behavior of TEnv.
				    template<typename TEnv, typename TProperty>
				    using Queue = typename trait::QueueType<TEnv, TProperty>::type;
				} // namespace alpaka
				// ==
				// == ./include/alpaka/queue/Traits.hpp ==
				// ============================================================================

			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded
			// #include "alpaka/workdiv/Traits.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			//! The alpaka accelerator library.
			namespace alpaka
			{
			    //! The kernel traits.
			    namespace trait
			    {
			        //! The kernel execution task creation trait.
			        template<
			            typename TAcc,
			            typename TWorkDiv,
			            typename TKernelFnObj,
			            typename... TArgs/*,
			            typename TSfinae = void*/>
			        struct CreateTaskKernel;

			        //! The trait for getting the size of the block shared dynamic memory of a kernel.
			        //!
			        //! \tparam TKernelFnObj The kernel function object.
			        //! \tparam TAcc The accelerator.
			        //!
			        //! The default implementation returns 0.
			        template<typename TKernelFnObj, typename TAcc, typename TSfinae = void>
			        struct BlockSharedMemDynSizeBytes
			        {
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic push
			#    pragma clang diagnostic ignored                                                                                  \
			        "-Wdocumentation" // clang does not support the syntax for variadic template arguments "args,..."
			#endif
			            //! \param kernelFnObj The kernel object for which the block shared memory size should be calculated.
			            //! \param blockThreadExtent The block thread extent.
			            //! \param threadElemExtent The thread element extent.
			            //! \tparam TArgs The kernel invocation argument types pack.
			            //! \param args,... The kernel invocation arguments.
			            //! \return The size of the shared memory allocated for a block in bytes.
			            //! The default version always returns zero.
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic pop
			#endif
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TDim, typename... TArgs>
			            ALPAKA_FN_HOST_ACC static auto getBlockSharedMemDynSizeBytes(
			                [[maybe_unused]] TKernelFnObj const& kernelFnObj,
			                [[maybe_unused]] Vec<TDim, Idx<TAcc>> const& blockThreadExtent,
			                [[maybe_unused]] Vec<TDim, Idx<TAcc>> const& threadElemExtent,
			                [[maybe_unused]] TArgs const&... args) -> std::size_t
			            {
			                return 0u;
			            }
			        };

			        //! The trait for getting the schedule to use when a kernel is run using the CpuOmp2Blocks accelerator.
			        //!
			        //! Has no effect on other accelerators.
			        //!
			        //! A user could either specialize this trait for their kernel, or define a public static member
			        //! ompScheduleKind of type alpaka::omp::Schedule, and additionally also int member ompScheduleChunkSize. In
			        //! the latter case, alpaka never odr-uses these members.
			        //!
			        //! In case schedule kind and chunk size are compile-time constants, setting then inside kernel may benefit
			        //! performance.
			        //!
			        //! \tparam TKernelFnObj The kernel function object.
			        //! \tparam TAcc The accelerator.
			        //!
			        //! The default implementation behaves as if the trait was not specialized.
			        template<typename TKernelFnObj, typename TAcc, typename TSfinae = void>
			        struct OmpSchedule
			        {
			        private:
			            //! Type returned when the trait is not specialized
			            struct TraitNotSpecialized
			            {
			            };

			        public:
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic push
			#    pragma clang diagnostic ignored                                                                                  \
			        "-Wdocumentation" // clang does not support the syntax for variadic template arguments "args,..."
			#endif
			            //! \param kernelFnObj The kernel object for which the schedule should be returned.
			            //! \param blockThreadExtent The block thread extent.
			            //! \param threadElemExtent The thread element extent.
			            //! \tparam TArgs The kernel invocation argument types pack.
			            //! \param args,... The kernel invocation arguments.
			            //! \return The OpenMP schedule information as an alpaka::omp::Schedule object,
			            //!         returning an object of any other type is treated as if the trait is not specialized.
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic pop
			#endif
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TDim, typename... TArgs>
			            ALPAKA_FN_HOST static auto getOmpSchedule(
			                [[maybe_unused]] TKernelFnObj const& kernelFnObj,
			                [[maybe_unused]] Vec<TDim, Idx<TAcc>> const& blockThreadExtent,
			                [[maybe_unused]] Vec<TDim, Idx<TAcc>> const& threadElemExtent,
			                [[maybe_unused]] TArgs const&... args) -> TraitNotSpecialized
			            {
			                return TraitNotSpecialized{};
			            }
			        };
			    } // namespace trait

			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic push
			#    pragma clang diagnostic ignored                                                                                  \
			        "-Wdocumentation" // clang does not support the syntax for variadic template arguments "args,..."
			#endif
			    //! \tparam TAcc The accelerator type.
			    //! \param kernelFnObj The kernel object for which the block shared memory size should be calculated.
			    //! \param blockThreadExtent The block thread extent.
			    //! \param threadElemExtent The thread element extent.
			    //! \param args,... The kernel invocation arguments.
			    //! \return The size of the shared memory allocated for a block in bytes.
			    //! The default implementation always returns zero.
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic pop
			#endif
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<typename TAcc, typename TKernelFnObj, typename TDim, typename... TArgs>
			    ALPAKA_FN_HOST_ACC auto getBlockSharedMemDynSizeBytes(
			        TKernelFnObj const& kernelFnObj,
			        Vec<TDim, Idx<TAcc>> const& blockThreadExtent,
			        Vec<TDim, Idx<TAcc>> const& threadElemExtent,
			        TArgs const&... args) -> std::size_t
			    {
			        return trait::BlockSharedMemDynSizeBytes<TKernelFnObj, TAcc>::getBlockSharedMemDynSizeBytes(
			            kernelFnObj,
			            blockThreadExtent,
			            threadElemExtent,
			            args...);
			    }

			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic push
			#    pragma clang diagnostic ignored                                                                                  \
			        "-Wdocumentation" // clang does not support the syntax for variadic template arguments "args,..."
			#endif
			    //! \tparam TAcc The accelerator type.
			    //! \param kernelFnObj The kernel object for which the block shared memory size should be calculated.
			    //! \param blockThreadExtent The block thread extent.
			    //! \param threadElemExtent The thread element extent.
			    //! \param args,... The kernel invocation arguments.
			    //! \return The OpenMP schedule information as an alpaka::omp::Schedule object if the kernel specialized the
			    //!         OmpSchedule trait, an object of another type if the kernel didn't specialize the trait.
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic pop
			#endif
			    template<typename TAcc, typename TKernelFnObj, typename TDim, typename... TArgs>
			    ALPAKA_FN_HOST auto getOmpSchedule(
			        TKernelFnObj const& kernelFnObj,
			        Vec<TDim, Idx<TAcc>> const& blockThreadExtent,
			        Vec<TDim, Idx<TAcc>> const& threadElemExtent,
			        TArgs const&... args)
			    {
			        return trait::OmpSchedule<TKernelFnObj, TAcc>::getOmpSchedule(
			            kernelFnObj,
			            blockThreadExtent,
			            threadElemExtent,
			            args...);
			    }

			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic push
			#    pragma clang diagnostic ignored                                                                                  \
			        "-Wdocumentation" // clang does not support the syntax for variadic template arguments "args,..."
			#endif

			    namespace detail
			    {
			        //! Check that the return of TKernelFnObj is void
			        template<typename TAcc, typename TSfinae = void>
			        struct CheckFnReturnType
			        {
			            template<typename TKernelFnObj, typename... TArgs>
			            void operator()(TKernelFnObj const&, TArgs const&...)
			            {
			                using Result = std::invoke_result_t<TKernelFnObj, TAcc const&, TArgs const&...>;
			                static_assert(std::is_same_v<Result, void>, "The TKernelFnObj is required to return void!");
			            }
			        };
			    } // namespace detail
			    //! Creates a kernel execution task.
			    //!
			    //! \tparam TAcc The accelerator type.
			    //! \param workDiv The index domain work division.
			    //! \param kernelFnObj The kernel function object which should be executed.
			    //! \param args,... The kernel invocation arguments.
			    //! \return The kernel execution task.
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic pop
			#endif
			    template<typename TAcc, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
			    ALPAKA_FN_HOST auto createTaskKernel(TWorkDiv const& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
			    {
			        // check for void return type
			        detail::CheckFnReturnType<TAcc>{}(kernelFnObj, args...);

			#if BOOST_COMP_NVCC
			        static_assert(
			            std::is_trivially_copyable_v<TKernelFnObj> || __nv_is_extended_device_lambda_closure_type(TKernelFnObj)
			                || __nv_is_extended_host_device_lambda_closure_type(TKernelFnObj),
			            "Kernels must be trivially copyable or an extended CUDA lambda expression!");
			#else
			        static_assert(std::is_trivially_copyable_v<TKernelFnObj>, "Kernels must be trivially copyable!");
			#endif
			        static_assert(
			            (std::is_trivially_copyable_v<std::decay_t<TArgs>> && ...),
			            "Kernel arguments must be trivially copyable!");
			        static_assert(
			            Dim<std::decay_t<TWorkDiv>>::value == Dim<TAcc>::value,
			            "The dimensions of TAcc and TWorkDiv have to be identical!");
			        static_assert(
			            std::is_same_v<Idx<std::decay_t<TWorkDiv>>, Idx<TAcc>>,
			            "The idx type of TAcc and the idx type of TWorkDiv have to be identical!");

			#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			        std::cout << __func__ << " workDiv: " << workDiv
			                  << ", kernelFnObj: " << core::demangled<decltype(kernelFnObj)> << std::endl;
			#endif
			        return trait::CreateTaskKernel<TAcc, TWorkDiv, TKernelFnObj, TArgs...>::createTaskKernel(
			            workDiv,
			            kernelFnObj,
			            std::forward<TArgs>(args)...);
			    }

			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic push
			#    pragma clang diagnostic ignored                                                                                  \
			        "-Wdocumentation" // clang does not support the syntax for variadic template arguments "args,..."
			#endif
			    //! Executes the given kernel in the given queue.
			    //!
			    //! \tparam TAcc The accelerator type.
			    //! \param queue The queue to enqueue the view copy task into.
			    //! \param workDiv The index domain work division.
			    //! \param kernelFnObj The kernel function object which should be executed.
			    //! \param args,... The kernel invocation arguments.
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic pop
			#endif
			    template<typename TAcc, typename TQueue, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
			    ALPAKA_FN_HOST auto exec(TQueue& queue, TWorkDiv const& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
			        -> void
			    {
			        enqueue(queue, createTaskKernel<TAcc>(workDiv, kernelFnObj, std::forward<TArgs>(args)...));
			    }
			} // namespace alpaka
			// ==
			// == ./include/alpaka/kernel/Traits.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/pltf/Traits.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included
			// #include <vector>    // amalgamate: file already included

			namespace alpaka
			{
			    struct ConceptPltf
			    {
			    };

			    //! True if TPltf is a platform, i.e. if it implements the ConceptPltf concept.
			    template<typename TPltf>
			    inline constexpr bool isPlatform = concepts::ImplementsConcept<ConceptPltf, TPltf>::value;

			    //! The platform traits.
			    namespace trait
			    {
			        //! The platform type trait.
			        template<typename T, typename TSfinae = void>
			        struct PltfType;

			        template<typename TPltf>
			        struct PltfType<TPltf, std::enable_if_t<concepts::ImplementsConcept<ConceptPltf, TPltf>::value>>
			        {
			            using type = typename concepts::ImplementationBase<ConceptDev, TPltf>;
			        };

			        //! The device count get trait.
			        template<typename T, typename TSfinae = void>
			        struct GetDevCount;

			        //! The device get trait.
			        template<typename T, typename TSfinae = void>
			        struct GetDevByIdx;
			    } // namespace trait

			    //! The platform type trait alias template to remove the ::type.
			    template<typename T>
			    using Pltf = typename trait::PltfType<T>::type;

			    //! \return The device identified by its index.
			    template<typename TPltf>
			    ALPAKA_FN_HOST auto getDevCount()
			    {
			        return trait::GetDevCount<Pltf<TPltf>>::getDevCount();
			    }

			    //! \return The device identified by its index.
			    template<typename TPltf>
			    ALPAKA_FN_HOST auto getDevByIdx(std::size_t const& devIdx)
			    {
			        return trait::GetDevByIdx<Pltf<TPltf>>::getDevByIdx(devIdx);
			    }

			    //! \return All the devices available on this accelerator.
			    template<typename TPltf>
			    ALPAKA_FN_HOST auto getDevs() -> std::vector<Dev<Pltf<TPltf>>>
			    {
			        std::vector<Dev<Pltf<TPltf>>> devs;

			        std::size_t const devCount(getDevCount<Pltf<TPltf>>());
			        devs.reserve(devCount);
			        for(std::size_t devIdx(0); devIdx < devCount; ++devIdx)
			        {
			            devs.push_back(getDevByIdx<Pltf<TPltf>>(devIdx));
			        }

			        return devs;
			    }

			    namespace trait
			    {
			        template<typename TPltf, typename TProperty>
			        struct QueueType<TPltf, TProperty, std::enable_if_t<concepts::ImplementsConcept<ConceptPltf, TPltf>::value>>
			        {
			            using type = typename QueueType<typename alpaka::trait::DevType<TPltf>::type, TProperty>::type;
			        };
			    } // namespace trait
			} // namespace alpaka
			// ==
			// == ./include/alpaka/pltf/Traits.hpp ==
			// ============================================================================

		// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded

		// #include <string>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included
		#include <typeinfo>

		namespace alpaka
		{
		    struct ConceptAcc
		    {
		    };

		    //! True if TAcc is an accelerator, i.e. if it implements the ConceptAcc concept.
		    template<typename TAcc>
		    inline constexpr bool isAccelerator = concepts::ImplementsConcept<ConceptAcc, TAcc>::value;

		    //! The accelerator traits.
		    namespace trait
		    {
		        //! The accelerator type trait.
		        template<typename T, typename TSfinae = void>
		        struct AccType;

		        //! The device properties get trait.
		        template<typename TAcc, typename TSfinae = void>
		        struct GetAccDevProps;

		        //! The accelerator name trait.
		        //!
		        //! The default implementation returns the mangled class name.
		        template<typename TAcc, typename TSfinae = void>
		        struct GetAccName
		        {
		            ALPAKA_FN_HOST static auto getAccName() -> std::string
		            {
		                return core::demangled<TAcc>;
		            }
		        };
		    } // namespace trait

		    //! The accelerator type trait alias template to remove the ::type.
		    template<typename T>
		    using Acc = typename trait::AccType<T>::type;

		    //! \return The acceleration properties on the given device.
		    template<typename TAcc, typename TDev>
		    ALPAKA_FN_HOST auto getAccDevProps(TDev const& dev) -> AccDevProps<Dim<TAcc>, Idx<TAcc>>
		    {
		        using ImplementationBase = concepts::ImplementationBase<ConceptAcc, TAcc>;
		        return trait::GetAccDevProps<ImplementationBase>::getAccDevProps(dev);
		    }

		    //! \return The accelerator name
		    //!
		    //! \tparam TAcc The accelerator type.
		    template<typename TAcc>
		    ALPAKA_FN_HOST auto getAccName() -> std::string
		    {
		        return trait::GetAccName<TAcc>::getAccName();
		    }

		    namespace trait
		    {
		        template<typename TAcc, typename TProperty>
		        struct QueueType<TAcc, TProperty, std::enable_if_t<concepts::ImplementsConcept<ConceptAcc, TAcc>::value>>
		        {
		            using type = typename QueueType<typename alpaka::trait::PltfType<TAcc>::type, TProperty>::type;
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/acc/Traits.hpp ==
		// ============================================================================

	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
		// ============================================================================
		// == ./include/alpaka/acc/Tag.hpp ==
		// ==
		/* Copyright 2023 Simeon Ehrig, Jan Stephan
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

		// #include <iostream>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included

		#define CREATE_ACC_TAG(tag_name)                                                                                      \
		    struct tag_name                                                                                                   \
		    {                                                                                                                 \
		        static std::string get_name()                                                                                 \
		        {                                                                                                             \
		            return #tag_name;                                                                                         \
		        }                                                                                                             \
		    }

		namespace alpaka
		{
		    CREATE_ACC_TAG(TagCpuOmp2Blocks);
		    CREATE_ACC_TAG(TagCpuOmp2Threads);
		    CREATE_ACC_TAG(TagCpuSerial);
		    CREATE_ACC_TAG(TagCpuSyclIntel);
		    CREATE_ACC_TAG(TagCpuTbbBlocks);
		    CREATE_ACC_TAG(TagCpuThreads);
		    CREATE_ACC_TAG(TagFpgaSyclIntel);
		    CREATE_ACC_TAG(TagGenericSycl);
		    CREATE_ACC_TAG(TagGpuCudaRt);
		    CREATE_ACC_TAG(TagGpuHipRt);
		    CREATE_ACC_TAG(TagGpuSyclIntel);

		    namespace trait
		    {
		        template<typename TAcc>
		        struct AccToTag;

		        template<typename TAcc>
		        using AccToTagType = typename AccToTag<TAcc>::type;

		        template<typename TTag, typename TDim, typename TIdx>
		        struct TagToAcc;

		        template<typename TTag, typename TDim, typename TIdx>
		        using TagToAccType = typename TagToAcc<TTag, TDim, TIdx>::type;
		    } // namespace trait

		    template<typename TAcc, typename... TTag>
		    inline constexpr bool accMatchesTags = (std::is_same_v<alpaka::trait::AccToTagType<TAcc>, TTag> || ...);
		} // namespace alpaka
		// ==
		// == ./include/alpaka/acc/Tag.hpp ==
		// ============================================================================

	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/dev/DevCpu.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber,
		 * Antonio Di Pilato
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/dev/common/QueueRegistry.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jeffrey Kelling
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

			#include <deque>
			// #include <functional>    // amalgamate: file already included
			#include <memory>
			// #include <mutex>    // amalgamate: file already included

			namespace alpaka::detail
			{
			    //! The CPU device implementation.
			    template<typename TQueue>
			    struct QueueRegistry
			    {
			        ALPAKA_FN_HOST auto getAllExistingQueues() const -> std::vector<std::shared_ptr<TQueue>>
			        {
			            std::vector<std::shared_ptr<TQueue>> vspQueues;

			            std::lock_guard<std::mutex> lk(m_Mutex);
			            vspQueues.reserve(std::size(m_queues));

			            for(auto it = std::begin(m_queues); it != std::end(m_queues);)
			            {
			                auto spQueue = it->lock();
			                if(spQueue)
			                {
			                    vspQueues.emplace_back(std::move(spQueue));
			                    ++it;
			                }
			                else
			                {
			                    it = m_queues.erase(it);
			                }
			            }
			            return vspQueues;
			        }

			        //! Registers the given queue on this device.
			        //! NOTE: Every queue has to be registered for correct functionality of device wait operations!
			        ALPAKA_FN_HOST auto registerQueue(std::shared_ptr<TQueue> const& spQueue) const -> void
			        {
			            std::lock_guard<std::mutex> lk(m_Mutex);

			            // Register this queue on the device.
			            m_queues.push_back(spQueue);
			        }

			    private:
			        std::mutex mutable m_Mutex;
			        std::deque<std::weak_ptr<TQueue>> mutable m_queues;
			    };
			} // namespace alpaka::detail
			// ==
			// == ./include/alpaka/dev/common/QueueRegistry.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/dev/cpu/SysInfo.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Daniel Vollmer, Erik Zenker, René Widera, Bernhard Manfred Gruber, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

			#if BOOST_OS_WINDOWS || BOOST_OS_CYGWIN
			#    ifndef NOMINMAX
			#        define NOMINMAX
			#    endif
			#    ifndef WIN32_LEAN_AND_MEAN
			#        define WIN32_LEAN_AND_MEAN
			#    endif
			// We could use some more macros to reduce the number of sub-headers included, but this would restrict user code.
			#    include <windows.h>
			#elif BOOST_OS_UNIX || BOOST_OS_MACOS
			#    include <sys/param.h>
			#    include <sys/types.h>
			#    include <unistd.h>

			// #    include <cstdint>    // amalgamate: file already included
			#    if BOOST_OS_BSD || BOOST_OS_MACOS
			#        include <sys/sysctl.h>
			#    endif
			#endif

			#if BOOST_OS_LINUX
			#    include <fstream>
			#endif

			#include <cstring>
			#include <stdexcept>
			// #include <string>    // amalgamate: file already included

			#if BOOST_ARCH_X86
			#    if BOOST_COMP_GNUC || BOOST_COMP_CLANG || BOOST_COMP_PGI
			#        include <cpuid.h>
			#    elif BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
			// #        include <intrin.h>    // amalgamate: file already included
			#    endif
			#endif

			namespace alpaka::cpu::detail
			{
			    constexpr int NO_CPUID = 0;
			    constexpr int UNKNOWN_CPU = 0;
			    constexpr int UNKNOWN_COMPILER = 1;
			#if BOOST_ARCH_X86
			#    if BOOST_COMP_GNUC || BOOST_COMP_CLANG || BOOST_COMP_PGI
			    inline auto cpuid(std::uint32_t level, std::uint32_t subfunction, std::uint32_t ex[4]) -> void
			    {
			        __cpuid_count(level, subfunction, ex[0], ex[1], ex[2], ex[3]);
			    }

			#    elif BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
			    inline auto cpuid(std::uint32_t level, std::uint32_t subfunction, std::uint32_t ex[4]) -> void
			    {
			        __cpuidex(reinterpret_cast<int*>(ex), level, subfunction);
			    }
			#    else
			    inline auto cpuid(std::uint32_t, std::uint32_t, std::uint32_t ex[4]) -> void
			    {
			        ex[0] = ex[2] = ex[3] = NO_CPUID;
			        ex[1] = UNKNOWN_COMPILER;
			    }
			#    endif
			#else
			    inline auto cpuid(std::uint32_t, std::uint32_t, std::uint32_t ex[4]) -> void
			    {
			        ex[0] = ex[2] = ex[3] = NO_CPUID;
			        ex[1] = UNKNOWN_CPU;
			    }
			#endif
			    //! \return The name of the CPU the code is running on.
			    inline auto getCpuName() -> std::string
			    {
			        // Get extended ids.
			        std::uint32_t ex[4] = {0};
			        cpuid(0x80000000, 0, ex);
			        std::uint32_t const nExIds(ex[0]);

			        if(!nExIds)
			        {
			            switch(ex[1])
			            {
			            case UNKNOWN_COMPILER:
			                return "<unknown: compiler>";
			            case UNKNOWN_CPU:
			                return "<unknown: CPU>";
			            default:
			                return "<unknown>";
			            }
			        }
			#if BOOST_ARCH_X86
			        // Get the information associated with each extended ID.
			        char cpuBrandString[0x40] = {0};
			        for(std::uint32_t i(0x80000000); i <= nExIds; ++i)
			        {
			            cpuid(i, 0, ex);

			            // Interpret CPU brand string and cache information.
			            if(i == 0x80000002)
			            {
			                std::memcpy(cpuBrandString, ex, sizeof(ex));
			            }
			            else if(i == 0x80000003)
			            {
			                std::memcpy(cpuBrandString + 16, ex, sizeof(ex));
			            }
			            else if(i == 0x80000004)
			            {
			                std::memcpy(cpuBrandString + 32, ex, sizeof(ex));
			            }
			        }
			        return std::string(cpuBrandString);
			#else
			        return std::string("unknown");
			#endif
			    }

			    //! \return Pagesize in bytes used by the system.
			    inline size_t getPageSize()
			    {
			#if BOOST_OS_WINDOWS || BOOST_OS_CYGWIN
			        SYSTEM_INFO si;
			        GetSystemInfo(&si);
			        return si.dwPageSize;
			#elif BOOST_OS_UNIX || BOOST_OS_MACOS
			#    if defined(_SC_PAGESIZE)
			        return static_cast<std::size_t>(sysconf(_SC_PAGESIZE));
			#    else
			        // this is legacy and only used as fallback
			        return = static_cast<size_t>(getpagesize());
			#    endif
			#else
			#    error "getPageSize not implemented for this system!"
			        return 0;
			#endif
			    }

			    //! \return The total number of bytes of global memory.
			    //! Adapted from David Robert Nadeau:
			    //! http://nadeausoftware.com/articles/2012/09/c_c_tip_how_get_physical_memory_size_system
			    inline auto getTotalGlobalMemSizeBytes() -> std::size_t
			    {
			#if BOOST_OS_WINDOWS
			        MEMORYSTATUSEX status;
			        status.dwLength = sizeof(status);
			        GlobalMemoryStatusEx(&status);
			        return static_cast<std::size_t>(status.ullTotalPhys);

			#elif BOOST_OS_CYGWIN
			        // New 64-bit MEMORYSTATUSEX isn't available.
			        MEMORYSTATUS status;
			        status.dwLength = sizeof(status);
			        GlobalMemoryStatus(&status);
			        return static_cast<std::size_t>(status.dwTotalPhys);

			#elif BOOST_OS_UNIX || BOOST_OS_MACOS
			        // Unix : Prefer sysctl() over sysconf() except sysctl() with HW_REALMEM and HW_PHYSMEM which are not
			        // always reliable
			#    if defined(CTL_HW) && (defined(HW_MEMSIZE) || defined(HW_PHYSMEM64))
			        int mib[2]
			            = { CTL_HW,
			#        if defined(HW_MEMSIZE) // OSX
			                HW_MEMSIZE
			#        elif defined(HW_PHYSMEM64) // NetBSD, OpenBSD.
			                HW_PHYSMEM64
			#        endif
			              };
			        std::uint64_t size(0);
			        std::size_t sizeLen{sizeof(size)};
			        if(sysctl(mib, 2, &size, &sizeLen, nullptr, 0) < 0)
			            throw std::logic_error("getTotalGlobalMemSizeBytes failed calling sysctl!");
			        return static_cast<std::size_t>(size);

			#    elif defined(_SC_AIX_REALMEM) // AIX.
			        return static_cast<std::size_t>(sysconf(_SC_AIX_REALMEM)) * static_cast<std::size_t>(1024);

			#    elif defined(_SC_PHYS_PAGES) // Linux, FreeBSD, OpenBSD, Solaris.
			        return static_cast<std::size_t>(sysconf(_SC_PHYS_PAGES)) * getPageSize();

			#    elif defined(CTL_HW)                                                                                             \
			        && (defined(HW_PHYSMEM) || defined(HW_REALMEM)) // FreeBSD, DragonFly BSD, NetBSD, OpenBSD, and OSX.
			        int mib[2]
			            = { CTL_HW,
			#        if defined(HW_REALMEM) // FreeBSD.
			                HW_REALMEM
			#        elif defined(HW_PYSMEM) // Others.
			                HW_PHYSMEM
			#        endif
			              };
			        std::uint32_t size(0);
			        std::size_t const sizeLen{sizeof(size)};
			        if(sysctl(mib, 2, &size, &sizeLen, nullptr, 0) < 0)
			            throw std::logic_error("getTotalGlobalMemSizeBytes failed calling sysctl!");
			        return static_cast<std::size_t>(size);
			#    endif

			#else
			#    error "getTotalGlobalMemSizeBytes not implemented for this system!"
			#endif
			    }

			    //! \return The free number of bytes of global memory.
			    //! \throws std::logic_error if not implemented on the system and std::runtime_error on other errors.
			    inline auto getFreeGlobalMemSizeBytes() -> std::size_t
			    {
			#if BOOST_OS_WINDOWS
			        MEMORYSTATUSEX status;
			        status.dwLength = sizeof(status);
			        GlobalMemoryStatusEx(&status);
			        return static_cast<std::size_t>(status.ullAvailPhys);
			#elif BOOST_OS_LINUX
			#    if defined(_SC_AVPHYS_PAGES)
			        return static_cast<std::size_t>(sysconf(_SC_AVPHYS_PAGES)) * getPageSize();
			#    else
			        // this is legacy and only used as fallback
			        return static_cast<std::size_t>(get_avphys_pages()) * getPageSize();
			#    endif
			#elif BOOST_OS_MACOS
			        int free_pages = 0;
			        std::size_t len = sizeof(free_pages);
			        if(sysctlbyname("vm.page_free_count", &free_pages, &len, nullptr, 0) < 0)
			        {
			            throw std::logic_error("getFreeGlobalMemSizeBytes failed calling sysctl(vm.page_free_count)!");
			        }

			        return static_cast<std::size_t>(free_pages) * getPageSize();
			#else
			#    error "getFreeGlobalMemSizeBytes not implemented for this system!"
			#endif
			    }

			} // namespace alpaka::cpu::detail
			// ==
			// == ./include/alpaka/dev/cpu/SysInfo.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/mem/buf/Traits.hpp ==
			// ==
			/* Copyright 2023 Alexander Matthes, Benjamin Worpitz, Andrea Bocci, Bernhard Manfred Gruber, Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/mem/view/Traits.hpp ==
				// ==
				/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Andrea Bocci, Jan Stephan, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Unreachable.hpp"    // amalgamate: file already expanded
				// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/elem/Traits.hpp ==
					// ==
					/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include <type_traits>    // amalgamate: file already included

					namespace alpaka
					{
					    //! The element trait.
					    namespace trait
					    {
					        //! The element type trait.
					        template<typename TView, typename TSfinae = void>
					        struct ElemType;
					    } // namespace trait

					    //! The element type trait alias template to remove the ::type.
					    template<typename TView>
					    using Elem = std::remove_volatile_t<typename trait::ElemType<TView>::type>;

					    // Trait specializations for unsigned integral types.
					    namespace trait
					    {
					        //! The fundamental type elem type trait specialization.
					        template<typename T>
					        struct ElemType<T, std::enable_if_t<std::is_fundamental_v<T>>>
					        {
					            using type = T;
					        };
					    } // namespace trait
					} // namespace alpaka
					// ==
					// == ./include/alpaka/elem/Traits.hpp ==
					// ============================================================================

				// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/meta/Fold.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/meta/Integral.hpp ==
					// ==
					/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include <type_traits>    // amalgamate: file already included

					namespace alpaka::meta
					{
					    //! The trait is true if all values of TSubset are contained in TSuperset.
					    template<typename TSuperset, typename TSubset>
					    using IsIntegralSuperset = std::integral_constant<
					        bool,
					        std::is_integral_v<TSuperset> && std::is_integral_v<TSubset>
					            && (
					                // If the signdness is equal, the sizes have to be greater or equal to be a superset.
					                ((std::is_unsigned_v<TSuperset> == std::is_unsigned_v<TSubset>)
					                 && (sizeof(TSuperset) >= sizeof(TSubset)))
					                // If the signdness is non-equal, the superset has to have at least one bit more.
					                || ((std::is_unsigned_v<TSuperset> != std::is_unsigned_v<TSubset>)
					                    && (sizeof(TSuperset) > sizeof(TSubset))))>;

					    //! The type that has the higher max value.
					    template<typename T0, typename T1>
					    using HigherMax = std::conditional_t<
					        (sizeof(T0) > sizeof(T1)),
					        T0,
					        std::conditional_t<((sizeof(T0) == sizeof(T1)) && std::is_unsigned_v<T0> && std::is_signed_v<T1>), T0, T1>>;

					    //! The type that has the lower max value.
					    template<typename T0, typename T1>
					    using LowerMax = std::conditional_t<
					        (sizeof(T0) < sizeof(T1)),
					        T0,
					        std::conditional_t<((sizeof(T0) == sizeof(T1)) && std::is_signed_v<T0> && std::is_unsigned_v<T1>), T0, T1>>;

					    //! The type that has the higher min value. If both types have the same min value, the type with the wider
					    //! range is chosen.
					    template<typename T0, typename T1>
					    using HigherMin = std::conditional_t<
					        (std::is_unsigned_v<T0> == std::is_unsigned_v<T1>),
					        std::conditional_t<
					            std::is_unsigned_v<T0>,
					            std::conditional_t<(sizeof(T0) < sizeof(T1)), T1, T0>,
					            std::conditional_t<(sizeof(T0) < sizeof(T1)), T0, T1>>,
					        std::conditional_t<std::is_unsigned_v<T0>, T0, T1>>;

					    //! The type that has the lower min value. If both types have the same min value, the type with the wider range
					    //! is chosen.
					    template<typename T0, typename T1>
					    using LowerMin = std::conditional_t<
					        (std::is_unsigned_v<T0> == std::is_unsigned_v<T1>),
					        std::conditional_t<(sizeof(T0) > sizeof(T1)), T0, T1>,
					        std::conditional_t<std::is_signed_v<T0>, T0, T1>>;
					} // namespace alpaka::meta
					// ==
					// == ./include/alpaka/meta/Integral.hpp ==
					// ============================================================================

				// #include "alpaka/offset/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

				// #include <array>    // amalgamate: file already included
				// #include <iosfwd>    // amalgamate: file already included
				// #include <type_traits>    // amalgamate: file already included
				// #include <vector>    // amalgamate: file already included
				#ifdef ALPAKA_USE_MDSPAN
				#    include <experimental/mdspan>
				#endif

				namespace alpaka
				{
				    //! The view traits.
				    namespace trait
				    {
				        //! The native pointer get trait.
				        template<typename TView, typename TSfinae = void>
				        struct GetPtrNative;

				        //! The pointer on device get trait.
				        template<typename TView, typename TDev, typename TSfinae = void>
				        struct GetPtrDev;

				        //! The pitch in bytes.
				        //! This is the distance in bytes in the linear memory between two consecutive elements in the next higher
				        //! dimension (TIdx-1).
				        //!
				        //! The default implementation uses the extent to calculate the pitch.
				        template<typename TIdx, typename TView, typename TSfinae = void>
				        struct GetPitchBytes
				        {
				            using ViewIdx = Idx<TView>;

				            ALPAKA_FN_HOST static auto getPitchBytes(TView const& view) -> ViewIdx
				            {
				                return getPitchBytesDefault(view);
				            }

				        private:
				            static auto getPitchBytesDefault(TView const& view) -> ViewIdx
				            {
				                constexpr auto idx = TIdx::value;
				                constexpr auto viewDim = Dim<TView>::value;
				                if constexpr(idx < viewDim - 1)
				                    return getExtent<idx>(view) * GetPitchBytes<DimInt<idx + 1>, TView>::getPitchBytes(view);
				                else if constexpr(idx == viewDim - 1)
				                    return getExtent<viewDim - 1>(view) * static_cast<ViewIdx>(sizeof(Elem<TView>));
				                else
				                    return static_cast<ViewIdx>(sizeof(Elem<TView>));
				                ALPAKA_UNREACHABLE({});
				            }
				        };

				        //! The memory set task trait.
				        //!
				        //! Fills the view with data.
				        template<typename TDim, typename TDev, typename TSfinae = void>
				        struct CreateTaskMemset;

				        //! The memory copy task trait.
				        //!
				        //! Copies memory from one view into another view possibly on a different device.
				        template<typename TDim, typename TDevDst, typename TDevSrc, typename TSfinae = void>
				        struct CreateTaskMemcpy;

				        //! The static device memory view creation trait.
				        template<typename TDev, typename TSfinae = void>
				        struct CreateStaticDevMemView;

				        //! The device memory view creation trait.
				        template<typename TDev, typename TSfinae = void>
				        struct CreateViewPlainPtr;

				        //! The sub view creation trait.
				        template<typename TDev, typename TSfinae = void>
				        struct CreateSubView;
				    } // namespace trait

				    //! Gets the native pointer of the memory view.
				    //!
				    //! \param view The memory view.
				    //! \return The native pointer.
				    template<typename TView>
				    ALPAKA_FN_HOST auto getPtrNative(TView const& view) -> Elem<TView> const*
				    {
				        return trait::GetPtrNative<TView>::getPtrNative(view);
				    }

				    //! Gets the native pointer of the memory view.
				    //!
				    //! \param view The memory view.
				    //! \return The native pointer.
				    template<typename TView>
				    ALPAKA_FN_HOST auto getPtrNative(TView& view) -> Elem<TView>*
				    {
				        return trait::GetPtrNative<TView>::getPtrNative(view);
				    }

				    //! Gets the pointer to the view on the given device.
				    //!
				    //! \param view The memory view.
				    //! \param dev The device.
				    //! \return The pointer on the device.
				    template<typename TView, typename TDev>
				    ALPAKA_FN_HOST auto getPtrDev(TView const& view, TDev const& dev) -> Elem<TView> const*
				    {
				        return trait::GetPtrDev<TView, TDev>::getPtrDev(view, dev);
				    }

				    //! Gets the pointer to the view on the given device.
				    //!
				    //! \param view The memory view.
				    //! \param dev The device.
				    //! \return The pointer on the device.
				    template<typename TView, typename TDev>
				    ALPAKA_FN_HOST auto getPtrDev(TView& view, TDev const& dev) -> Elem<TView>*
				    {
				        return trait::GetPtrDev<TView, TDev>::getPtrDev(view, dev);
				    }

				    //! \return The pitch in bytes. This is the distance in bytes between two consecutive elements in the given
				    //! dimension.
				    template<std::size_t Tidx, typename TView>
				    ALPAKA_FN_HOST auto getPitchBytes(TView const& view) -> Idx<TView>
				    {
				        return trait::GetPitchBytes<DimInt<Tidx>, TView>::getPitchBytes(view);
				    }

				    //! Create a memory set task.
				    //!
				    //! \param view The memory view to fill.
				    //! \param byte Value to set for each element of the specified view.
				    //! \param extent The extent of the view to fill.
				    template<typename TExtent, typename TViewFwd>
				    ALPAKA_FN_HOST auto createTaskMemset(TViewFwd&& view, std::uint8_t const& byte, TExtent const& extent)
				    {
				        using TView = std::remove_reference_t<TViewFwd>;
				        static_assert(!std::is_const_v<TView>, "The view must not be const!");
				        static_assert(
				            Dim<TView>::value == Dim<TExtent>::value,
				            "The view and the extent are required to have the same dimensionality!");
				        static_assert(
				            meta::IsIntegralSuperset<Idx<TView>, Idx<TExtent>>::value,
				            "The view and the extent must have compatible index types!");

				        return trait::CreateTaskMemset<Dim<TView>, Dev<TView>>::createTaskMemset(
				            std::forward<TViewFwd>(view),
				            byte,
				            extent);
				    }

				    //! Sets the bytes of the memory of view, described by extent, to the given value.
				    //!
				    //! \param queue The queue to enqueue the view fill task into.
				    //! \param[in,out] view The memory view to fill. May be a temporary object.
				    //! \param byte Value to set for each element of the specified view.
				    //! \param extent The extent of the view to fill.
				    template<typename TExtent, typename TViewFwd, typename TQueue>
				    ALPAKA_FN_HOST auto memset(TQueue& queue, TViewFwd&& view, std::uint8_t const& byte, TExtent const& extent) -> void
				    {
				        enqueue(queue, createTaskMemset(std::forward<TViewFwd>(view), byte, extent));
				    }

				    //! Sets each byte of the memory of the entire view to the given value.
				    //!
				    //! \param queue The queue to enqueue the view fill task into.
				    //! \param[in,out] view The memory view to fill. May be a temporary object.
				    //! \param byte Value to set for each element of the specified view.
				    template<typename TViewFwd, typename TQueue>
				    ALPAKA_FN_HOST auto memset(TQueue& queue, TViewFwd&& view, std::uint8_t const& byte) -> void
				    {
				        enqueue(queue, createTaskMemset(std::forward<TViewFwd>(view), byte, getExtentVec(view)));
				    }

				    //! Creates a memory copy task.
				    //!
				    //! \param viewDst The destination memory view.
				    //! \param viewSrc The source memory view.
				    //! \param extent The extent of the view to copy.
				    template<typename TExtent, typename TViewSrc, typename TViewDstFwd>
				    ALPAKA_FN_HOST auto createTaskMemcpy(TViewDstFwd&& viewDst, TViewSrc const& viewSrc, TExtent const& extent)
				    {
				        using TViewDst = std::remove_reference_t<TViewDstFwd>;
				        using SrcElem = Elem<TViewSrc>;
				        using DstElem = Elem<TViewDst>;
				        using ExtentIdx = Idx<TExtent>;
				        using DstIdx = Idx<TViewDst>;
				        using SrcIdx = Idx<TViewSrc>;

				        static_assert(!std::is_const_v<TViewDst>, "The destination view must not be const!");
				        static_assert(!std::is_const_v<DstElem>, "The destination view's element type must not be const!");
				        static_assert(
				            Dim<TViewDst>::value == Dim<TViewSrc>::value,
				            "The source and the destination view must have the same dimensionality!");
				        static_assert(
				            Dim<TViewDst>::value == Dim<TExtent>::value,
				            "The destination view and the extent must have the same dimensionality!");
				        static_assert(
				            std::is_same_v<DstElem, std::remove_const_t<SrcElem>>,
				            "The source and destination view must have the same element type!");
				        static_assert(
				            meta::IsIntegralSuperset<DstIdx, ExtentIdx>::value,
				            "The destination view and the extent are required to have compatible index types!");
				        static_assert(
				            meta::IsIntegralSuperset<SrcIdx, ExtentIdx>::value,
				            "The source view and the extent are required to have compatible index types!");

				        return trait::CreateTaskMemcpy<Dim<TViewDst>, Dev<TViewDst>, Dev<TViewSrc>>::createTaskMemcpy(
				            std::forward<TViewDstFwd>(viewDst),
				            viewSrc,
				            extent);
				    }

				    //! Copies memory from a part of viewSrc to viewDst, described by extent. Possibly copies between different memory
				    //! spaces.
				    //!
				    //! \param queue The queue to enqueue the view copy task into.
				    //! \param[in,out] viewDst The destination memory view. May be a temporary object.
				    //! \param viewSrc The source memory view. May be a temporary object.
				    //! \param extent The extent of the view to copy.
				    template<typename TExtent, typename TViewSrc, typename TViewDstFwd, typename TQueue>
				    ALPAKA_FN_HOST auto memcpy(TQueue& queue, TViewDstFwd&& viewDst, TViewSrc const& viewSrc, TExtent const& extent)
				        -> void
				    {
				        enqueue(queue, createTaskMemcpy(std::forward<TViewDstFwd>(viewDst), viewSrc, extent));
				    }

				    //! Copies the entire memory of viewSrc to viewDst. Possibly copies between different memory
				    //! spaces.
				    //!
				    //! \param queue The queue to enqueue the view copy task into.
				    //! \param[in,out] viewDst The destination memory view. May be a temporary object.
				    //! \param viewSrc The source memory view. May be a temporary object.
				    template<typename TViewSrc, typename TViewDstFwd, typename TQueue>
				    ALPAKA_FN_HOST auto memcpy(TQueue& queue, TViewDstFwd&& viewDst, TViewSrc const& viewSrc) -> void
				    {
				        enqueue(queue, createTaskMemcpy(std::forward<TViewDstFwd>(viewDst), viewSrc, getExtentVec(viewSrc)));
				    }

				    namespace detail
				    {
				        template<typename TDim, typename TView>
				        struct Print
				        {
				            ALPAKA_FN_HOST static auto print(
				                TView const& view,
				                Elem<TView> const* const ptr,
				                Vec<Dim<TView>, Idx<TView>> const& extent,
				                std::ostream& os,
				                std::string const& elementSeparator,
				                std::string const& rowSeparator,
				                std::string const& rowPrefix,
				                std::string const& rowSuffix) -> void
				            {
				                os << rowPrefix;

				                auto const pitch(getPitchBytes<TDim::value + 1u>(view));
				                auto const lastIdx(extent[TDim::value] - 1u);
				                for(auto i(decltype(lastIdx)(0)); i <= lastIdx; ++i)
				                {
				                    Print<DimInt<TDim::value + 1u>, TView>::print(
				                        view,
				                        reinterpret_cast<Elem<TView> const*>(reinterpret_cast<std::uint8_t const*>(ptr) + i * pitch),
				                        extent,
				                        os,
				                        elementSeparator,
				                        rowSeparator,
				                        rowPrefix,
				                        rowSuffix);

				                    // While we are not at the end of a row, add the row separator.
				                    if(i != lastIdx)
				                    {
				                        os << rowSeparator;
				                    }
				                }

				                os << rowSuffix;
				            }
				        };
				        template<typename TView>
				        struct Print<DimInt<Dim<TView>::value - 1u>, TView>
				        {
				            ALPAKA_FN_HOST static auto print(
				                TView const& /* view */,
				                Elem<TView> const* const ptr,
				                Vec<Dim<TView>, Idx<TView>> const& extent,
				                std::ostream& os,
				                std::string const& elementSeparator,
				                std::string const& /* rowSeparator */,
				                std::string const& rowPrefix,
				                std::string const& rowSuffix) -> void
				            {
				                os << rowPrefix;

				                auto const lastIdx(extent[Dim<TView>::value - 1u] - 1u);
				                for(auto i(decltype(lastIdx)(0)); i <= lastIdx; ++i)
				                {
				                    // Add the current element.
				                    os << *(ptr + i);

				                    // While we are not at the end of a line, add the element separator.
				                    if(i != lastIdx)
				                    {
				                        os << elementSeparator;
				                    }
				                }

				                os << rowSuffix;
				            }
				        };
				    } // namespace detail
				    //! Prints the content of the view to the given queue.
				    // \TODO: Add precision flag.
				    // \TODO: Add column alignment flag.
				    template<typename TView>
				    ALPAKA_FN_HOST auto print(
				        TView const& view,
				        std::ostream& os,
				        std::string const& elementSeparator = ", ",
				        std::string const& rowSeparator = "\n",
				        std::string const& rowPrefix = "[",
				        std::string const& rowSuffix = "]") -> void
				    {
				        detail::Print<DimInt<0u>, TView>::print(
				            view,
				            getPtrNative(view),
				            getExtentVec(view),
				            os,
				            elementSeparator,
				            rowSeparator,
				            rowPrefix,
				            rowSuffix);
				    }

				    namespace detail
				    {
				        //! A class with a create method that returns the pitch for each index.
				        template<std::size_t Tidx>
				        struct CreatePitchBytes
				        {
				            template<typename TView>
				            ALPAKA_FN_HOST static auto create(TView const& view) -> Idx<TView>
				            {
				                return getPitchBytes<Tidx>(view);
				            }
				        };

				        //! Calculate the pitches purely from the extents.
				        template<typename TElem, typename TDim, typename TIdx>
				        ALPAKA_FN_HOST inline auto calculatePitchesFromExtents(Vec<TDim, TIdx> const& extent)
				        {
				            Vec<TDim, TIdx> pitchBytes(Vec<TDim, TIdx>::all(0));
				            if constexpr(TDim::value > 0)
				            {
				                pitchBytes[TDim::value - 1u] = extent[TDim::value - 1u] * static_cast<TIdx>(sizeof(TElem));
				                for(TIdx i = TDim::value - 1u; i > static_cast<TIdx>(0u); --i)
				                {
				                    pitchBytes[i - 1] = extent[i - 1] * pitchBytes[i];
				                }
				            }
				            return pitchBytes;
				        }
				    } // namespace detail
				    //! \return The pitch vector.
				    template<typename TView>
				    auto getPitchBytesVec(TView const& view = TView()) -> Vec<Dim<TView>, Idx<TView>>
				    {
				        return createVecFromIndexedFn<Dim<TView>, detail::CreatePitchBytes>(view);
				    }

				    //! \return The pitch but only the last N elements.
				    template<typename TDim, typename TView>
				    ALPAKA_FN_HOST auto getPitchBytesVecEnd(TView const& view = TView()) -> Vec<TDim, Idx<TView>>
				    {
				        using IdxOffset = std::integral_constant<
				            std::intmax_t,
				            static_cast<std::intmax_t>(Dim<TView>::value) - static_cast<std::intmax_t>(TDim::value)>;
				        return createVecFromIndexedFnOffset<TDim, detail::CreatePitchBytes, IdxOffset>(view);
				    }

				    //! \return A view to static device memory.
				    template<typename TElem, typename TDev, typename TExtent>
				    auto createStaticDevMemView(TElem* pMem, TDev const& dev, TExtent const& extent)
				    {
				        return trait::CreateStaticDevMemView<TDev>::createStaticDevMemView(pMem, dev, extent);
				    }

				    //! Creates a view to a device pointer
				    //!
				    //! \param dev Device from where pMem can be accessed.
				    //! \param pMem Pointer to memory. The pointer must be accessible from the given device.
				    //! \param extent Number of elements represented by the pMem.
				    //!               Using a multi dimensional extent will result in a multi dimension view to the memory represented
				    //!               by pMem.
				    //! \return A view to device memory.
				    template<typename TDev, typename TElem, typename TExtent>
				    auto createView(TDev const& dev, TElem* pMem, TExtent const& extent)
				    {
				        using Dim = alpaka::Dim<TExtent>;
				        using Idx = alpaka::Idx<TExtent>;
				        auto const extentVec = Vec<Dim, Idx>(extent);
				        return trait::CreateViewPlainPtr<TDev>::createViewPlainPtr(
				            dev,
				            pMem,
				            extentVec,
				            detail::calculatePitchesFromExtents<TElem>(extentVec));
				    }

				    //! Creates a view to a device pointer
				    //!
				    //! \param dev Device from where pMem can be accessed.
				    //! \param pMem Pointer to memory. The pointer must be accessible from the given device.
				    //! \param extent Number of elements represented by the pMem.
				    //!               Using a multi dimensional extent will result in a multi dimension view to the memory represented
				    //!               by pMem.
				    //! \param pitch Pitch in bytes for each dimension. Dimensionality must be equal to extent.
				    //! \return A view to device memory.
				    template<typename TDev, typename TElem, typename TExtent, typename TPitch>
				    auto createView(TDev const& dev, TElem* pMem, TExtent const& extent, TPitch const& pitch)
				    {
				        return trait::CreateViewPlainPtr<TDev>::createViewPlainPtr(dev, pMem, extent, pitch);
				    }

				    //! Creates a view to a contiguous container of device-accessible memory.
				    //!
				    //! \param dev Device from which the container can be accessed.
				    //! \param con Contiguous container. The container must provide a `data()` method. The data held by the container
				    //!            must be accessible from the given device. The `GetExtent` trait must be defined for the container.
				    //! \return A view to device memory.
				    template<typename TDev, typename TContainer>
				    auto createView(TDev const& dev, TContainer& con)
				    {
				        return createView(dev, std::data(con), getExtentVec(con));
				    }

				    //! Creates a view to a contiguous container of device-accessible memory.
				    //!
				    //! \param dev Device from which the container can be accessed.
				    //! \param con Contiguous container. The container must provide a `data()` method. The data held by the container
				    //!            must be accessible from the given device. The `GetExtent` trait must be defined for the container.
				    //! \param extent Number of elements held by the container. Using a multi-dimensional extent will result in a
				    //!               multi-dimensional view to the memory represented by the container.
				    //! \return A view to device memory.
				    template<typename TDev, typename TContainer, typename TExtent>
				    auto createView(TDev const& dev, TContainer& con, TExtent const& extent)
				    {
				        return createView(dev, std::data(con), extent);
				    }

				    //! Creates a sub view to an existing view.
				    //!
				    //! \param view The view this view is a sub-view of.
				    //! \param extent Number of elements the resulting view holds.
				    //! \param offset Number of elements skipped in view for the new origin of the resulting view.
				    //! \return A sub view to a existing view.
				    template<typename TView, typename TExtent, typename TOffsets>
				    auto createSubView(TView& view, TExtent const& extent, TOffsets const& offset = TExtent())
				    {
				        return trait::CreateSubView<typename trait::DevType<TView>::type>::createSubView(view, extent, offset);
				    }

				#ifdef ALPAKA_USE_MDSPAN
				    namespace experimental
				    {
				        // import mdspan into alpaka::experimental namespace. see: https://eel.is/c++draft/mdspan.syn
				        using std::experimental::default_accessor;
				        using std::experimental::dextents;
				        using std::experimental::extents;
				        using std::experimental::layout_left;
				        using std::experimental::layout_right;
				        using std::experimental::layout_stride;
				        using std::experimental::mdspan;
				        // import submdspan as well, which is not standardized yet
				        using std::experimental::full_extent;
				        using std::experimental::submdspan;

				        namespace traits
				        {
				            namespace detail
				            {
				                template<typename ElementType>
				                struct ByteIndexedAccessor
				                {
				                    using offset_policy = ByteIndexedAccessor;
				                    using element_type = ElementType;
				                    using reference = ElementType&;

				                    using data_handle_type
				                        = std::conditional_t<std::is_const_v<ElementType>, std::byte const*, std::byte*>;

				                    constexpr ByteIndexedAccessor() noexcept = default;

				                    ALPAKA_FN_HOST_ACC
				                    constexpr data_handle_type offset(data_handle_type p, size_t i) const noexcept
				                    {
				                        return p + i;
				                    }

				                    ALPAKA_FN_HOST_ACC
				                    constexpr reference access(data_handle_type p, size_t i) const noexcept
				                    {
				                        assert(i % alignof(ElementType) == 0);
				#    if BOOST_COMP_GNUC
				#        pragma GCC diagnostic push
				#        pragma GCC diagnostic ignored "-Wcast-align"
				#    endif
				                        return *reinterpret_cast<ElementType*>(p + i);
				#    if BOOST_COMP_GNUC
				#        pragma GCC diagnostic pop
				#    endif
				                    }
				                };

				                template<typename TView, std::size_t... Is>
				                ALPAKA_FN_HOST auto makeExtents(TView const& view, std::index_sequence<Is...>)
				                {
				                    return std::experimental::dextents<Idx<TView>, Dim<TView>::value>{getExtent<Is>(view)...};
				                }

				                template<typename TView, std::size_t... Is>
				                ALPAKA_FN_HOST auto makeStrides(TView const& view, std::index_sequence<Is...>)
				                {
				                    constexpr auto fastestIndexPitch = static_cast<Idx<TView>>(sizeof(Elem<TView>));
				                    [[maybe_unused]] constexpr auto dim = Dim<TView>::value;
				                    // alpaka pitches are right-shifted by 1. We skip getPitchBytes<0> (the size in bytes of the entire
				                    // buffer) and append the element size last
				                    return std::array<Idx<TView>, dim>{
				                        (Is < dim - 1 ? getPitchBytes<Is + 1>(view) : fastestIndexPitch)...};
				                }
				            } // namespace detail

				            //! Customization point for getting an mdspan from a view.
				            template<typename TView, typename TSfinae = void>
				            struct GetMdSpan
				            {
				                ALPAKA_FN_HOST static auto getMdSpan(TView& view)
				                {
				                    constexpr auto dim = Dim<TView>::value;
				                    using Element = Elem<TView>;
				                    auto extents = detail::makeExtents(view, std::make_index_sequence<dim>{});
				                    auto* ptr = reinterpret_cast<std::byte*>(getPtrNative(view));
				                    auto const strides = detail::makeStrides(view, std::make_index_sequence<dim>{});
				                    layout_stride::mapping<decltype(extents)> m{extents, strides};
				                    return mdspan<Element, decltype(extents), layout_stride, detail::ByteIndexedAccessor<Element>>{
				                        ptr,
				                        m};
				                }

				                ALPAKA_FN_HOST static auto getMdSpanTransposed(TView& view)
				                {
				                    constexpr auto dim = Dim<TView>::value;
				                    using Element = Elem<TView>;
				                    auto extents = detail::makeExtents(view, std::make_index_sequence<dim>{});
				                    auto* ptr = reinterpret_cast<std::byte*>(getPtrNative(view));
				                    auto strides = detail::makeStrides(view, std::make_index_sequence<dim>{});
				                    std::reverse(begin(strides), end(strides));
				                    layout_stride::mapping<decltype(extents)> m{extents, strides};
				                    return mdspan<Element, decltype(extents), layout_stride, detail::ByteIndexedAccessor<Element>>{
				                        ptr,
				                        m};
				                }
				            };
				        } // namespace traits

				        //! Gets a std::mdspan from the given view. The memory layout is determined by the pitches of the view.
				        template<typename TView>
				        ALPAKA_FN_HOST auto getMdSpan(TView& view)
				        {
				            return traits::GetMdSpan<TView>::getMdSpan(view);
				        }

				        //! Gets a std::mdspan from the given view. The memory layout is determined by the reversed pitches of the
				        //! view. This effectively also reverses the extents of the view. In order words, if you create a transposed
				        //! mdspan on a 10x5 element view, the mdspan will have an iteration space of 5x10.
				        template<typename TView>
				        ALPAKA_FN_HOST auto getMdSpanTransposed(TView& view)
				        {
				            return traits::GetMdSpan<TView>::getMdSpanTransposed(view);
				        }
				    } // namespace experimental
				#endif
				} // namespace alpaka
				// ==
				// == ./include/alpaka/mem/view/Traits.hpp ==
				// ============================================================================


			namespace alpaka
			{
			    //! The CPU device handle.
			    class DevCpu;

			    //! The buffer traits.
			    namespace trait
			    {
			        //! The memory buffer type trait.
			        template<typename TDev, typename TElem, typename TDim, typename TIdx, typename TSfinae = void>
			        struct BufType;

			        //! The memory allocator trait.
			        template<typename TElem, typename TDim, typename TIdx, typename TDev, typename TSfinae = void>
			        struct BufAlloc;

			        //! The stream-ordered memory allocator trait.
			        template<typename TElem, typename TDim, typename TIdx, typename TDev, typename TSfinae = void>
			        struct AsyncBufAlloc;

			        //! The stream-ordered memory allocation capability trait.
			        template<typename TDim, typename TDev>
			        struct HasAsyncBufSupport : public std::false_type
			        {
			        };

			        //! The pinned/mapped memory allocator trait.
			        template<typename TPltf, typename TElem, typename TDim, typename TIdx>
			        struct BufAllocMapped;

			        //! The pinned/mapped memory allocation capability trait.
			        template<typename TPltf>
			        struct HasMappedBufSupport : public std::false_type
			        {
			        };
			    } // namespace trait

			    //! The memory buffer type trait alias template to remove the ::type.
			    template<typename TDev, typename TElem, typename TDim, typename TIdx>
			    using Buf = typename trait::BufType<alpaka::Dev<TDev>, TElem, TDim, TIdx>::type;

			    //! Allocates memory on the given device.
			    //!
			    //! \tparam TElem The element type of the returned buffer.
			    //! \tparam TIdx The linear index type of the buffer.
			    //! \tparam TExtent The extent type of the buffer.
			    //! \tparam TDev The type of device the buffer is allocated on.
			    //! \param dev The device to allocate the buffer on.
			    //! \param extent The extent of the buffer.
			    //! \return The newly allocated buffer.
			    template<typename TElem, typename TIdx, typename TExtent, typename TDev>
			    ALPAKA_FN_HOST auto allocBuf(TDev const& dev, TExtent const& extent = TExtent())
			    {
			        return trait::BufAlloc<TElem, Dim<TExtent>, TIdx, TDev>::allocBuf(dev, extent);
			    }

			    //! Allocates stream-ordered memory on the given device.
			    //!
			    //! \tparam TElem The element type of the returned buffer.
			    //! \tparam TIdx The linear index type of the buffer.
			    //! \tparam TExtent The extent type of the buffer.
			    //! \tparam TQueue The type of queue used to order the buffer allocation.
			    //! \param queue The queue used to order the buffer allocation.
			    //! \param extent The extent of the buffer.
			    //! \return The newly allocated buffer.
			    template<typename TElem, typename TIdx, typename TExtent, typename TQueue>
			    ALPAKA_FN_HOST auto allocAsyncBuf(TQueue queue, TExtent const& extent = TExtent())
			    {
			        return trait::AsyncBufAlloc<TElem, Dim<TExtent>, TIdx, alpaka::Dev<TQueue>>::allocAsyncBuf(queue, extent);
			    }

			    /* TODO: Remove this pragma block once support for clang versions <= 13 is removed. These versions are unable to
			       figure out that the template parameters are attached to a C++17 inline variable. */
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic push
			#    pragma clang diagnostic ignored "-Wdocumentation"
			#endif
			    //! Checks if the given device can allocate a stream-ordered memory buffer of the given dimensionality.
			    //!
			    //! \tparam TDev The type of device to allocate the buffer on.
			    //! \tparam TDim The dimensionality of the buffer to allocate.
			    template<typename TDev, typename TDim>
			    constexpr inline bool hasAsyncBufSupport = trait::HasAsyncBufSupport<TDim, TDev>::value;
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic pop
			#endif

			    //! If supported, allocates stream-ordered memory on the given queue and the associated device.
			    //! Otherwise, allocates regular memory on the device associated to the queue.
			    //! Please note that stream-ordered and regular memory have different semantics:
			    //! this function is provided for convenience in the cases where the difference is not relevant,
			    //! and the stream-ordered memory is only used as a performance optimisation.
			    //!
			    //! \tparam TElem The element type of the returned buffer.
			    //! \tparam TIdx The linear index type of the buffer.
			    //! \tparam TExtent The extent type of the buffer.
			    //! \tparam TQueue The type of queue used to order the buffer allocation.
			    //! \param queue The queue used to order the buffer allocation.
			    //! \param extent The extent of the buffer.
			    //! \return The newly allocated buffer.
			    template<typename TElem, typename TIdx, typename TExtent, typename TQueue>
			    ALPAKA_FN_HOST auto allocAsyncBufIfSupported(TQueue queue, TExtent const& extent = TExtent())
			    {
			        if constexpr(hasAsyncBufSupport<alpaka::Dev<TQueue>, Dim<TExtent>>)
			        {
			            return allocAsyncBuf<TElem, TIdx>(queue, extent);
			        }
			        else
			        {
			            return allocBuf<TElem, TIdx>(getDev(queue), extent);
			        }

			        ALPAKA_UNREACHABLE(allocBuf<TElem, TIdx>(getDev(queue), extent));
			    }

			    //! Allocates pinned/mapped host memory, accessible by all devices in the given platform.
			    //!
			    //! \tparam TPltf The platform from which the buffer is accessible.
			    //! \tparam TElem The element type of the returned buffer.
			    //! \tparam TIdx The linear index type of the buffer.
			    //! \tparam TExtent The extent type of the buffer.
			    //! \param host The host device to allocate the buffer on.
			    //! \param extent The extent of the buffer.
			    //! \return The newly allocated buffer.
			    template<typename TPltf, typename TElem, typename TIdx, typename TExtent>
			    ALPAKA_FN_HOST auto allocMappedBuf(DevCpu const& host, TExtent const& extent = TExtent())
			    {
			        return trait::BufAllocMapped<TPltf, TElem, Dim<TExtent>, TIdx>::allocMappedBuf(host, extent);
			    }

			    /* TODO: Remove this pragma block once support for clang versions <= 13 is removed. These versions are unable to
			       figure out that the template parameters are attached to a C++17 inline variable. */
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic push
			#    pragma clang diagnostic ignored "-Wdocumentation"
			#endif
			    //! Checks if the host can allocate a pinned/mapped host memory, accessible by all devices in the given platform.
			    //!
			    //! \tparam TPltf The platform from which the buffer is accessible.
			    template<typename TPltf>
			    constexpr inline bool hasMappedBufSupport = trait::HasMappedBufSupport<TPltf>::value;
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic pop
			#endif

			    //! If supported, allocates pinned/mapped host memory, accessible by all devices in the given platform.
			    //! Otherwise, allocates regular host memory.
			    //! Please note that pinned/mapped and regular memory may have different semantics:
			    //! this function is provided for convenience in the cases where the difference is not relevant,
			    //! and the pinned/mapped memory is only used as a performance optimisation.
			    //!
			    //! \tparam TPltf The platform from which the buffer is accessible.
			    //! \tparam TElem The element type of the returned buffer.
			    //! \tparam TIdx The linear index type of the buffer.
			    //! \tparam TExtent The extent type of the buffer.
			    //! \param host The host device to allocate the buffer on.
			    //! \param extent The extent of the buffer.
			    //! \return The newly allocated buffer.
			    template<typename TPltf, typename TElem, typename TIdx, typename TExtent>
			    ALPAKA_FN_HOST auto allocMappedBufIfSupported(DevCpu const& host, TExtent const& extent = TExtent())
			    {
			        if constexpr(hasMappedBufSupport<TPltf>)
			        {
			            return allocMappedBuf<TPltf, TElem, TIdx>(host, extent);
			        }
			        else
			        {
			            return allocBuf<TElem, TIdx>(host, extent);
			        }

			        ALPAKA_UNREACHABLE(allocBuf<TElem, TIdx>(host, extent));
			    }
			} // namespace alpaka
			// ==
			// == ./include/alpaka/mem/buf/Traits.hpp ==
			// ============================================================================

		// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/queue/Properties.hpp ==
			// ==
			/* Copyright 2020 Rene Widera, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			namespace alpaka
			{
			    //! Properties to define queue behavior
			    namespace property
			    {
			        //! The caller is waiting until the enqueued task is finished
			        struct Blocking;

			        //! The caller is NOT waiting until the enqueued task is finished
			        struct NonBlocking;
			    } // namespace property
			    using namespace property;
			} // namespace alpaka
			// ==
			// == ./include/alpaka/queue/Properties.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/queue/QueueGenericThreadsBlocking.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/event/Traits.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
				// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded

				namespace alpaka
				{
				    //! The event management traits.
				    namespace trait
				    {
				        //! The event type trait.
				        template<typename T, typename TSfinae = void>
				        struct EventType;

				        //! The event tester trait.
				        template<typename TEvent, typename TSfinae = void>
				        struct IsComplete;
				    } // namespace trait

				    //! The event type trait alias template to remove the ::type.
				    template<typename T>
				    using Event = typename trait::EventType<T>::type;

				    //! Tests if the given event has already been completed.
				    //!
				    //! \warning This function is allowed to return false negatives. An already completed event can reported as
				    //! uncompleted because the status information are not fully propagated by the used alpaka backend.
				    //! \return true event is finished/complete else false.
				    template<typename TEvent>
				    ALPAKA_FN_HOST auto isComplete(TEvent const& event) -> bool
				    {
				        return trait::IsComplete<TEvent>::isComplete(event);
				    }
				} // namespace alpaka
				// ==
				// == ./include/alpaka/event/Traits.hpp ==
				// ============================================================================

			// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/queue/cpu/IGenericThreadsQueue.hpp ==
				// ==
				/* Copyright 2020 Axel Huebl, Benjamin Worpitz, Matthias Werner, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

				namespace alpaka
				{
				    template<typename TDev>
				    class EventGenericThreads;

				#if BOOST_COMP_CLANG
				// avoid diagnostic warning: "has no out-of-line virtual method definitions; its vtable will be emitted in every
				// translation unit [-Werror,-Wweak-vtables]" https://stackoverflow.com/a/29288300
				#    pragma clang diagnostic push
				#    pragma clang diagnostic ignored "-Wweak-vtables"
				#endif

				    //! The CPU queue interface
				    template<typename TDev>
				    class IGenericThreadsQueue
				    {
				    public:
				        //! enqueue the event
				        virtual void enqueue(EventGenericThreads<TDev>&) = 0;
				        //! waiting for the event
				        virtual void wait(EventGenericThreads<TDev> const&) = 0;
				        virtual ~IGenericThreadsQueue() = default;
				    };
				#if BOOST_COMP_CLANG
				#    pragma clang diagnostic pop
				#endif
				} // namespace alpaka
				// ==
				// == ./include/alpaka/queue/cpu/IGenericThreadsQueue.hpp ==
				// ============================================================================

			// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

			// #include <atomic>    // amalgamate: file already included
			// #include <memory>    // amalgamate: file already included
			// #include <mutex>    // amalgamate: file already included

			namespace alpaka
			{
			    template<typename TDev>
			    class EventGenericThreads;

			    namespace generic
			    {
			        namespace detail
			        {
			#if BOOST_COMP_CLANG
			// avoid diagnostic warning: "has no out-of-line virtual method definitions; its vtable will be emitted in every
			// translation unit [-Werror,-Wweak-vtables]" https://stackoverflow.com/a/29288300
			#    pragma clang diagnostic push
			#    pragma clang diagnostic ignored "-Wweak-vtables"
			#endif
			            //! The CPU device queue implementation.
			            template<typename TDev>
			            class QueueGenericThreadsBlockingImpl final : public IGenericThreadsQueue<TDev>
			#if BOOST_COMP_CLANG
			#    pragma clang diagnostic pop
			#endif
			            {
			            public:
			                explicit QueueGenericThreadsBlockingImpl(TDev dev) noexcept
			                    : m_dev(std::move(dev))
			                    , m_bCurrentlyExecutingTask(false)
			                {
			                }
			                QueueGenericThreadsBlockingImpl(QueueGenericThreadsBlockingImpl<TDev> const&) = delete;
			                auto operator=(QueueGenericThreadsBlockingImpl<TDev> const&)
			                    -> QueueGenericThreadsBlockingImpl<TDev>& = delete;

			                void enqueue(EventGenericThreads<TDev>& ev) final
			                {
			                    alpaka::enqueue(*this, ev);
			                }

			                void wait(EventGenericThreads<TDev> const& ev) final
			                {
			                    alpaka::wait(*this, ev);
			                }

			            public:
			                TDev const m_dev; //!< The device this queue is bound to.
			                std::mutex mutable m_mutex;
			                std::atomic<bool> m_bCurrentlyExecutingTask;
			            };
			        } // namespace detail
			    } // namespace generic

			    //! The CPU device queue.
			    template<typename TDev>
			    class QueueGenericThreadsBlocking final
			        : public concepts::Implements<ConceptCurrentThreadWaitFor, QueueGenericThreadsBlocking<TDev>>
			        , public concepts::Implements<ConceptQueue, QueueGenericThreadsBlocking<TDev>>
			        , public concepts::Implements<ConceptGetDev, QueueGenericThreadsBlocking<TDev>>
			    {
			    public:
			        explicit QueueGenericThreadsBlocking(TDev const& dev)
			            : m_spQueueImpl(std::make_shared<generic::detail::QueueGenericThreadsBlockingImpl<TDev>>(dev))
			        {
			            ALPAKA_DEBUG_FULL_LOG_SCOPE;

			            dev.registerQueue(m_spQueueImpl);
			        }
			        auto operator==(QueueGenericThreadsBlocking<TDev> const& rhs) const -> bool
			        {
			            return (m_spQueueImpl == rhs.m_spQueueImpl);
			        }
			        auto operator!=(QueueGenericThreadsBlocking<TDev> const& rhs) const -> bool
			        {
			            return !((*this) == rhs);
			        }

			    public:
			        std::shared_ptr<generic::detail::QueueGenericThreadsBlockingImpl<TDev>> m_spQueueImpl;
			    };

			    namespace trait
			    {
			        //! The CPU blocking device queue device type trait specialization.
			        template<typename TDev>
			        struct DevType<QueueGenericThreadsBlocking<TDev>>
			        {
			            using type = TDev;
			        };
			        //! The CPU blocking device queue device get trait specialization.
			        template<typename TDev>
			        struct GetDev<QueueGenericThreadsBlocking<TDev>>
			        {
			            ALPAKA_FN_HOST static auto getDev(QueueGenericThreadsBlocking<TDev> const& queue) -> TDev
			            {
			                return queue.m_spQueueImpl->m_dev;
			            }
			        };

			        //! The CPU blocking device queue event type trait specialization.
			        template<typename TDev>
			        struct EventType<QueueGenericThreadsBlocking<TDev>>
			        {
			            using type = EventGenericThreads<TDev>;
			        };

			        //! The CPU blocking device queue enqueue trait specialization.
			        //! This default implementation for all tasks directly invokes the function call operator of the task.
			        template<typename TDev, typename TTask>
			        struct Enqueue<QueueGenericThreadsBlocking<TDev>, TTask>
			        {
			            ALPAKA_FN_HOST static auto enqueue(QueueGenericThreadsBlocking<TDev>& queue, TTask const& task) -> void
			            {
			                std::lock_guard<std::mutex> lk(queue.m_spQueueImpl->m_mutex);

			                queue.m_spQueueImpl->m_bCurrentlyExecutingTask = true;

			                task();

			                queue.m_spQueueImpl->m_bCurrentlyExecutingTask = false;
			            }
			        };
			        //! The CPU blocking device queue test trait specialization.
			        template<typename TDev>
			        struct Empty<QueueGenericThreadsBlocking<TDev>>
			        {
			            ALPAKA_FN_HOST static auto empty(QueueGenericThreadsBlocking<TDev> const& queue) -> bool
			            {
			                return !queue.m_spQueueImpl->m_bCurrentlyExecutingTask;
			            }
			        };

			        //! The CPU blocking device queue thread wait trait specialization.
			        //!
			        //! Blocks execution of the calling thread until the queue has finished processing all previously requested
			        //! tasks (kernels, data copies, ...)
			        template<typename TDev>
			        struct CurrentThreadWaitFor<QueueGenericThreadsBlocking<TDev>>
			        {
			            ALPAKA_FN_HOST static auto currentThreadWaitFor(QueueGenericThreadsBlocking<TDev> const& queue) -> void
			            {
			                std::lock_guard<std::mutex> lk(queue.m_spQueueImpl->m_mutex);
			            }
			        };
			    } // namespace trait
			} // namespace alpaka

				// ============================================================================
				// == ./include/alpaka/event/EventGenericThreads.hpp ==
				// ==
				/* Copyright 2023 Axel Hübl, Benjamin Worpitz, Matthias Werner, René Widera, Jan Stephan, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/core/Utility.hpp ==
					// ==
					/* Copyright 2022 Benjamin Worpitz, René Widera, Bernhard Manfred Gruber, Jan Stephan
					 * SPDX-License-Identifier: MPL-2.0
					 */
					// #pragma once
					// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

					// #include <type_traits>    // amalgamate: file already included
					// #include <utility>    // amalgamate: file already included

					namespace alpaka::core
					{
					    //! convert any type to a reference type
					    //
					    // This function is equivalent to std::declval() but can be used
					    // within an alpaka accelerator kernel too.
					    // This function can be used only within std::decltype().
					#if BOOST_LANG_CUDA && BOOST_COMP_CLANG_CUDA || BOOST_COMP_HIP
					    template<class T>
					    ALPAKA_FN_HOST_ACC std::add_rvalue_reference_t<T> declval();
					#else
					    using std::declval;
					#endif

					    /// Returns the ceiling of a / b, as integer.
					    template<typename Integral>
					    [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto divCeil(Integral a, Integral b) -> Integral
					    {
					        return (a + b - 1) / b;
					    }

					    /// Computes the nth power of base, in integers.
					    template<typename Integral>
					    [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto intPow(Integral base, Integral n) -> Integral
					    {
					        if(n == 0)
					            return 1;
					        auto r = base;
					        for(Integral i = 1; i < n; i++)
					            r *= base;
					        return r;
					    }

					    /// Computes the floor of the nth root of value, in integers.
					    template<typename Integral>
					    [[nodiscard]] ALPAKA_FN_HOST_ACC constexpr auto nthRootFloor(Integral value, Integral n) -> Integral
					    {
					        // adapted from: https://en.wikipedia.org/wiki/Integer_square_root
					        Integral L = 0;
					        Integral R = value + 1;
					        while(L != R - 1)
					        {
					            Integral const M = (L + R) / 2;
					            if(intPow(M, n) <= value)
					                L = M;
					            else
					                R = M;
					        }
					        return L;
					    }
					} // namespace alpaka::core
					// ==
					// == ./include/alpaka/core/Utility.hpp ==
					// ============================================================================

				// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/event/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/queue/QueueGenericThreadsBlocking.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/queue/QueueGenericThreadsNonBlocking.hpp ==
					// ==
					/* Copyright 2023 Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber, Jeffrey Kelling
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
						// ============================================================================
						// == ./include/alpaka/core/CallbackThread.hpp ==
						// ==
						/* Copyright 2022 Antonio Di Pilato
						 * SPDX-License-Identifier: MPL-2.0
						 */

						// #pragma once
						#include <condition_variable>
						// #include <functional>    // amalgamate: file already included
						#include <future>
						// #include <iostream>    // amalgamate: file already included
						// #include <mutex>    // amalgamate: file already included
						#include <queue>
						#include <thread>

						namespace alpaka::core
						{
						    class CallbackThread
						    {
						        using Task = std::packaged_task<void()>;

						    public:
						        ~CallbackThread()
						        {
						            {
						                std::unique_lock<std::mutex> lock{m_mutex};
						                m_stop = true;
						                m_cond.notify_one();
						            }

						            if(m_thread.joinable())
						            {
						                if(std::this_thread::get_id() == m_thread.get_id())
						                {
						                    std::cerr << "ERROR in ~CallbackThread: thread joins itself" << std::endl;
						                    std::abort();
						                }
						                m_thread.join();
						            }
						        }

						        // Note: due to different std lib implementations of packaged_task, the lifetime of the passed function either
						        // ends when the packaged_task is destroyed (while the returned future is still alive) or when both are
						        // destroyed. Therefore, ensure that a submitted task does not extend the lifetime of any object that
						        // (transitively) holds the returned future. E.g. don't capture a shared_ptr to an alpaka object that stores
						        // the returned future. This is a cyclic dependency and creates a leak.
						        template<typename NullaryFunction>
						        auto submit(NullaryFunction&& nf) -> std::future<void>
						        {
						            static_assert(
						                std::is_void_v<std::invoke_result_t<NullaryFunction>>,
						                "Submitted function must not have any arguments and return void.");
						            return submit(Task{std::forward<NullaryFunction>(nf)});
						        }

						        auto submit(Task task) -> std::future<void>
						        {
						            auto f = task.get_future();
						            {
						                std::unique_lock<std::mutex> lock{m_mutex};
						                ++m_tasksInProgress;
						                m_tasks.emplace(std::move(task));
						                if(!m_thread.joinable())
						                    startWorkerThread();
						            }
						            m_cond.notify_one();
						            return f;
						        }

						        [[nodiscard]] auto empty() const
						        {
						            return m_tasksInProgress == 0;
						        }

						    private:
						        std::thread m_thread;
						        std::condition_variable m_cond;
						        std::mutex m_mutex;
						        bool m_stop{false};
						        std::queue<Task> m_tasks;
						        std::atomic<int> m_tasksInProgress{0};

						        auto startWorkerThread() -> void
						        {
						            m_thread = std::thread(
						                [this]
						                {
						                    while(true)
						                    {
						                        {
						                            // Do not move the tasks out of the loop else the lifetime could be extended until the
						                            // moment where the callback thread is destructed.
						                            Task task;
						                            {
						                                std::unique_lock<std::mutex> lock{m_mutex};
						                                m_cond.wait(lock, [this] { return m_stop || !m_tasks.empty(); });

						                                if(m_stop && m_tasks.empty())
						                                    break;

						                                task = std::move(m_tasks.front());
						                                m_tasks.pop();
						                            }

						                            task();
						                        }
						                        --m_tasksInProgress;
						                    }
						                });
						        }
						    };
						} // namespace alpaka::core
						// ==
						// == ./include/alpaka/core/CallbackThread.hpp ==
						// ============================================================================

					// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
					// #include "alpaka/event/Traits.hpp"    // amalgamate: file already expanded
					// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
					// #include "alpaka/queue/cpu/IGenericThreadsQueue.hpp"    // amalgamate: file already expanded
					// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

					// #include <future>    // amalgamate: file already included
					// #include <memory>    // amalgamate: file already included
					// #include <mutex>    // amalgamate: file already included
					// #include <thread>    // amalgamate: file already included
					// #include <tuple>    // amalgamate: file already included
					// #include <type_traits>    // amalgamate: file already included

					namespace alpaka
					{
					    template<typename TDev>
					    class EventGenericThreads;

					    namespace generic
					    {
					        namespace detail
					        {
					#if BOOST_COMP_CLANG
					// avoid diagnostic warning: "has no out-of-line virtual method definitions; its vtable will be emitted in every
					// translation unit [-Werror,-Wweak-vtables]" https://stackoverflow.com/a/29288300
					#    pragma clang diagnostic push
					#    pragma clang diagnostic ignored "-Wweak-vtables"
					#endif
					            //! The CPU device queue implementation.
					            template<typename TDev>
					            class QueueGenericThreadsNonBlockingImpl final : public IGenericThreadsQueue<TDev>
					#if BOOST_COMP_CLANG
					#    pragma clang diagnostic pop
					#endif
					            {
					            public:
					                explicit QueueGenericThreadsNonBlockingImpl(TDev dev) : m_dev(std::move(dev))
					                {
					                }
					                QueueGenericThreadsNonBlockingImpl(QueueGenericThreadsNonBlockingImpl<TDev> const&) = delete;
					                QueueGenericThreadsNonBlockingImpl(QueueGenericThreadsNonBlockingImpl<TDev>&&) = delete;
					                auto operator=(QueueGenericThreadsNonBlockingImpl<TDev> const&)
					                    -> QueueGenericThreadsNonBlockingImpl<TDev>& = delete;
					                auto operator=(QueueGenericThreadsNonBlockingImpl&&)
					                    -> QueueGenericThreadsNonBlockingImpl<TDev>& = delete;
					                ~QueueGenericThreadsNonBlockingImpl() override
					                {
					                }

					                void enqueue(EventGenericThreads<TDev>& ev) final
					                {
					                    alpaka::enqueue(*this, ev);
					                }

					                void wait(EventGenericThreads<TDev> const& ev) final
					                {
					                    alpaka::wait(*this, ev);
					                }

					            public:
					                TDev const m_dev; //!< The device this queue is bound to.
					                core::CallbackThread m_workerThread;
					            };
					        } // namespace detail
					    } // namespace generic

					    //! The CPU device queue.
					    template<typename TDev>
					    class QueueGenericThreadsNonBlocking final
					        : public concepts::Implements<ConceptCurrentThreadWaitFor, QueueGenericThreadsNonBlocking<TDev>>
					        , public concepts::Implements<ConceptQueue, QueueGenericThreadsNonBlocking<TDev>>
					        , public concepts::Implements<ConceptGetDev, QueueGenericThreadsNonBlocking<TDev>>
					    {
					    public:
					        explicit QueueGenericThreadsNonBlocking(TDev const& dev)
					            : m_spQueueImpl(std::make_shared<generic::detail::QueueGenericThreadsNonBlockingImpl<TDev>>(dev))
					        {
					            ALPAKA_DEBUG_FULL_LOG_SCOPE;

					            dev.registerQueue(m_spQueueImpl);
					        }
					        auto operator==(QueueGenericThreadsNonBlocking<TDev> const& rhs) const -> bool
					        {
					            return (m_spQueueImpl == rhs.m_spQueueImpl);
					        }
					        auto operator!=(QueueGenericThreadsNonBlocking<TDev> const& rhs) const -> bool
					        {
					            return !((*this) == rhs);
					        }

					    public:
					        std::shared_ptr<generic::detail::QueueGenericThreadsNonBlockingImpl<TDev>> m_spQueueImpl;
					    };

					    namespace trait
					    {
					        //! The CPU non-blocking device queue device type trait specialization.
					        template<typename TDev>
					        struct DevType<QueueGenericThreadsNonBlocking<TDev>>
					        {
					            using type = TDev;
					        };
					        //! The CPU non-blocking device queue device get trait specialization.
					        template<typename TDev>
					        struct GetDev<QueueGenericThreadsNonBlocking<TDev>>
					        {
					            ALPAKA_FN_HOST static auto getDev(QueueGenericThreadsNonBlocking<TDev> const& queue) -> TDev
					            {
					                return queue.m_spQueueImpl->m_dev;
					            }
					        };

					        //! The CPU non-blocking device queue event type trait specialization.
					        template<typename TDev>
					        struct EventType<QueueGenericThreadsNonBlocking<TDev>>
					        {
					            using type = EventGenericThreads<TDev>;
					        };

					        //! The CPU non-blocking device queue enqueue trait specialization.
					        //! This default implementation for all tasks directly invokes the function call operator of the task.
					        template<typename TDev, typename TTask>
					        struct Enqueue<QueueGenericThreadsNonBlocking<TDev>, TTask>
					        {
					            ALPAKA_FN_HOST static auto enqueue(QueueGenericThreadsNonBlocking<TDev>& queue, TTask const& task) -> void
					            {
					                queue.m_spQueueImpl->m_workerThread.submit(task);
					            }
					        };
					        //! The CPU non-blocking device queue test trait specialization.
					        template<typename TDev>
					        struct Empty<QueueGenericThreadsNonBlocking<TDev>>
					        {
					            ALPAKA_FN_HOST static auto empty(QueueGenericThreadsNonBlocking<TDev> const& queue) -> bool
					            {
					                return queue.m_spQueueImpl->m_workerThread.empty();
					            }
					        };
					    } // namespace trait
					} // namespace alpaka

					// #include "alpaka/event/EventGenericThreads.hpp"    // amalgamate: file already expanded
					// ==
					// == ./include/alpaka/queue/QueueGenericThreadsNonBlocking.hpp ==
					// ============================================================================

				// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

				// #include <condition_variable>    // amalgamate: file already included
				// #include <future>    // amalgamate: file already included
				// #include <mutex>    // amalgamate: file already included
				// #include <utility>    // amalgamate: file already included
				#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
				// #    include <iostream>    // amalgamate: file already included
				#endif

				namespace alpaka
				{
				    namespace generic::detail
				    {
				        //! The CPU device event implementation.
				        template<typename TDev>
				        class EventGenericThreadsImpl final
				            : public concepts::Implements<ConceptCurrentThreadWaitFor, EventGenericThreadsImpl<TDev>>
				        {
				        public:
				            EventGenericThreadsImpl(TDev dev) noexcept : m_dev(std::move(dev))
				            {
				            }
				            EventGenericThreadsImpl(EventGenericThreadsImpl<TDev> const&) = delete;
				            auto operator=(EventGenericThreadsImpl<TDev> const&) -> EventGenericThreadsImpl<TDev>& = delete;

				            auto isReady() noexcept -> bool
				            {
				                return (m_LastReadyEnqueueCount == m_enqueueCount);
				            }

				            auto wait(std::size_t const& enqueueCount, std::unique_lock<std::mutex>& lk) const noexcept -> void
				            {
				                ALPAKA_ASSERT(enqueueCount <= m_enqueueCount);

				                while(enqueueCount > m_LastReadyEnqueueCount)
				                {
				                    auto future = m_future;
				                    lk.unlock();
				                    future.get();
				                    lk.lock();
				                }
				            }

				            TDev const m_dev; //!< The device this event is bound to.

				            std::mutex mutable m_mutex; //!< The mutex used to synchronize access to the event.
				            std::shared_future<void> m_future; //!< The future signaling the event completion.
				            std::size_t m_enqueueCount = 0u; //!< The number of times this event has been enqueued.
				            std::size_t m_LastReadyEnqueueCount = 0u; //!< The time this event has been ready the last time.
				                                                      //!< Ready means that the event was not waiting within a queue
				                                                      //!< (not enqueued or already completed). If m_enqueueCount ==
				                                                      //!< m_LastReadyEnqueueCount, the event is currently not enqueued
				        };
				    } // namespace generic::detail

				    //! The CPU device event.
				    template<typename TDev>
				    class EventGenericThreads final
				        : public concepts::Implements<ConceptCurrentThreadWaitFor, EventGenericThreads<TDev>>
				        , public concepts::Implements<ConceptGetDev, EventGenericThreads<TDev>>
				    {
				    public:
				        //! \param bBusyWaiting Unused. EventGenericThreads never does busy waiting.
				        EventGenericThreads(TDev const& dev, [[maybe_unused]] bool bBusyWaiting = true)
				            : m_spEventImpl(std::make_shared<generic::detail::EventGenericThreadsImpl<TDev>>(dev))
				        {
				        }
				        auto operator==(EventGenericThreads<TDev> const& rhs) const -> bool
				        {
				            return (m_spEventImpl == rhs.m_spEventImpl);
				        }
				        auto operator!=(EventGenericThreads<TDev> const& rhs) const -> bool
				        {
				            return !((*this) == rhs);
				        }

				    public:
				        std::shared_ptr<generic::detail::EventGenericThreadsImpl<TDev>> m_spEventImpl;
				    };
				    namespace trait
				    {
				        //! The CPU device event device type trait specialization.
				        template<typename TDev>
				        struct DevType<EventGenericThreads<TDev>>
				        {
				            using type = TDev;
				        };
				        //! The CPU device event device get trait specialization.
				        template<typename TDev>
				        struct GetDev<EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto getDev(EventGenericThreads<TDev> const& event) -> TDev
				            {
				                return event.m_spEventImpl->m_dev;
				            }
				        };

				        //! The CPU device event test trait specialization.
				        template<typename TDev>
				        struct IsComplete<EventGenericThreads<TDev>>
				        {
				            //! \return If the event is not waiting within a queue (not enqueued or already handled).
				            ALPAKA_FN_HOST static auto isComplete(EventGenericThreads<TDev> const& event) -> bool
				            {
				                std::lock_guard<std::mutex> lk(event.m_spEventImpl->m_mutex);

				                return event.m_spEventImpl->isReady();
				            }
				        };

				        //! The CPU non-blocking device queue enqueue trait specialization.
				        template<typename TDev>
				        struct Enqueue<alpaka::generic::detail::QueueGenericThreadsNonBlockingImpl<TDev>, EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto enqueue(
				                [[maybe_unused]] alpaka::generic::detail::QueueGenericThreadsNonBlockingImpl<TDev>& queueImpl,
				                EventGenericThreads<TDev>& event) -> void
				            {
				                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

				                // Copy the shared pointer of the event implementation.
				                // This is forwarded to the lambda that is enqueued into the queue to ensure that the event
				                // implementation is alive as long as it is enqueued.
				                auto spEventImpl = event.m_spEventImpl;

				                // Setting the event state and enqueuing it has to be atomic.
				                std::lock_guard<std::mutex> lk(spEventImpl->m_mutex);

				                ++spEventImpl->m_enqueueCount;

				                auto const enqueueCount = spEventImpl->m_enqueueCount;

				                // Enqueue a task that only resets the events flag if it is completed.
				                spEventImpl->m_future = queueImpl.m_workerThread.submit(
				                    [spEventImpl, enqueueCount]() mutable
				                    {
				                        std::unique_lock<std::mutex> lk2(spEventImpl->m_mutex);

				                        // Nothing to do if it has been re-enqueued to a later position in the queue.
				                        if(enqueueCount == spEventImpl->m_enqueueCount)
				                        {
				                            spEventImpl->m_LastReadyEnqueueCount = spEventImpl->m_enqueueCount;
				                        }
				                        spEventImpl
				                            .reset(); // avoid keeping the event alive as part of the background thread task's future
				                    });
				            }
				        };
				        //! The CPU non-blocking device queue enqueue trait specialization.
				        template<typename TDev>
				        struct Enqueue<QueueGenericThreadsNonBlocking<TDev>, EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto enqueue(
				                QueueGenericThreadsNonBlocking<TDev>& queue,
				                EventGenericThreads<TDev>& event) -> void
				            {
				                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

				                alpaka::enqueue(*queue.m_spQueueImpl, event);
				            }
				        };
				        //! The CPU blocking device queue enqueue trait specialization.
				        template<typename TDev>
				        struct Enqueue<alpaka::generic::detail::QueueGenericThreadsBlockingImpl<TDev>, EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto enqueue(
				                alpaka::generic::detail::QueueGenericThreadsBlockingImpl<TDev>& queueImpl,
				                EventGenericThreads<TDev>& event) -> void
				            {
				                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

				                std::promise<void> promise;
				                {
				                    std::lock_guard<std::mutex> lk(queueImpl.m_mutex);

				                    queueImpl.m_bCurrentlyExecutingTask = true;

				                    auto& eventImpl(*event.m_spEventImpl);

				                    {
				                        // Setting the event state and enqueuing it has to be atomic.
				                        std::lock_guard<std::mutex> evLk(eventImpl.m_mutex);

				                        ++eventImpl.m_enqueueCount;
				                        // NOTE: Difference to non-blocking version: directly set the event state instead of enqueuing.
				                        eventImpl.m_LastReadyEnqueueCount = eventImpl.m_enqueueCount;

				                        eventImpl.m_future = promise.get_future();
				                    }

				                    queueImpl.m_bCurrentlyExecutingTask = false;
				                }
				                promise.set_value();
				            }
				        };
				        //! The CPU blocking device queue enqueue trait specialization.
				        template<typename TDev>
				        struct Enqueue<QueueGenericThreadsBlocking<TDev>, EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto enqueue(
				                QueueGenericThreadsBlocking<TDev>& queue,
				                EventGenericThreads<TDev>& event) -> void
				            {
				                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

				                alpaka::enqueue(*queue.m_spQueueImpl, event);
				            }
				        };
				    } // namespace trait
				    namespace trait
				    {
				        namespace generic
				        {
				            template<typename TDev>
				            ALPAKA_FN_HOST auto currentThreadWaitForDevice(TDev const& dev) -> void
				            {
				                // Get all the queues on the device at the time of invocation.
				                // All queues added afterwards are ignored.
				                auto vQueues = dev.getAllQueues();
				                // Furthermore there should not even be a chance to enqueue something between getting the queues and
				                // adding our wait events!
				                std::vector<EventGenericThreads<TDev>> vEvents;
				                for(auto&& spQueue : vQueues)
				                {
				                    vEvents.emplace_back(dev);
				                    spQueue->enqueue(vEvents.back());
				                }

				                // Now wait for all the events.
				                for(auto&& event : vEvents)
				                {
				                    wait(event);
				                }
				            }
				        } // namespace generic

				        //! The CPU device event thread wait trait specialization.
				        //!
				        //! Waits until the event itself and therefore all tasks preceding it in the queue it is enqueued to have been
				        //! completed. If the event is not enqueued to a queue the method returns immediately.
				        template<typename TDev>
				        struct CurrentThreadWaitFor<EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto currentThreadWaitFor(EventGenericThreads<TDev> const& event) -> void
				            {
				                wait(*event.m_spEventImpl);
				            }
				        };
				        //! The CPU device event implementation thread wait trait specialization.
				        //!
				        //! Waits until the event itself and therefore all tasks preceding it in the queue it is enqueued to have been
				        //! completed. If the event is not enqueued to a queue the method returns immediately.
				        //!
				        //! NOTE: This method is for internal usage only.
				        template<typename TDev>
				        struct CurrentThreadWaitFor<alpaka::generic::detail::EventGenericThreadsImpl<TDev>>
				        {
				            ALPAKA_FN_HOST static auto currentThreadWaitFor(
				                alpaka::generic::detail::EventGenericThreadsImpl<TDev> const& eventImpl) -> void
				            {
				                std::unique_lock<std::mutex> lk(eventImpl.m_mutex);

				                auto const enqueueCount = eventImpl.m_enqueueCount;
				                eventImpl.wait(enqueueCount, lk);
				            }
				        };
				        //! The CPU non-blocking device queue event wait trait specialization.
				        template<typename TDev>
				        struct WaiterWaitFor<
				            alpaka::generic::detail::QueueGenericThreadsNonBlockingImpl<TDev>,
				            EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto waiterWaitFor(
				                alpaka::generic::detail::QueueGenericThreadsNonBlockingImpl<TDev>& queueImpl,
				                EventGenericThreads<TDev> const& event) -> void
				            {
				                // Copy the shared pointer of the event implementation.
				                // This is forwarded to the lambda that is enqueued into the queue to ensure that the event
				                // implementation is alive as long as it is enqueued.
				                auto spEventImpl = event.m_spEventImpl;

				                std::lock_guard<std::mutex> lk(spEventImpl->m_mutex);

				                if(!spEventImpl->isReady())
				                {
				                    auto const enqueueCount = spEventImpl->m_enqueueCount;

				                    // Enqueue a task that waits for the given event.
				                    queueImpl.m_workerThread.submit(
				                        [spEventImpl, enqueueCount]() mutable
				                        {
				                            std::unique_lock<std::mutex> lk2(spEventImpl->m_mutex);
				                            spEventImpl->wait(enqueueCount, lk2);
				                            spEventImpl.reset(); // avoid keeping the event alive as part of the background thread
				                                                 // task's future
				                        });
				                }
				            }
				        };
				        //! The CPU non-blocking device queue event wait trait specialization.
				        template<typename TDev>
				        struct WaiterWaitFor<QueueGenericThreadsNonBlocking<TDev>, EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto waiterWaitFor(
				                QueueGenericThreadsNonBlocking<TDev>& queue,
				                EventGenericThreads<TDev> const& event) -> void
				            {
				                wait(*queue.m_spQueueImpl, event);
				            }
				        };
				        //! The CPU blocking device queue event wait trait specialization.
				        template<typename TDev>
				        struct WaiterWaitFor<alpaka::generic::detail::QueueGenericThreadsBlockingImpl<TDev>, EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto waiterWaitFor(
				                alpaka::generic::detail::QueueGenericThreadsBlockingImpl<TDev>& /* queueImpl */,
				                EventGenericThreads<TDev> const& event) -> void
				            {
				                // NOTE: Difference to non-blocking version: directly wait for event.
				                wait(*event.m_spEventImpl);
				            }
				        };
				        //! The CPU blocking device queue event wait trait specialization.
				        template<typename TDev>
				        struct WaiterWaitFor<QueueGenericThreadsBlocking<TDev>, EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto waiterWaitFor(
				                QueueGenericThreadsBlocking<TDev>& queue,
				                EventGenericThreads<TDev> const& event) -> void
				            {
				                wait(*queue.m_spQueueImpl, event);
				            }
				        };
				        //! The CPU non-blocking device event wait trait specialization.
				        //!
				        //! Any future work submitted in any queue of this device will wait for event to complete before beginning
				        //! execution.
				        template<typename TDev>
				        struct WaiterWaitFor<TDev, EventGenericThreads<TDev>>
				        {
				            ALPAKA_FN_HOST static auto waiterWaitFor(TDev& dev, EventGenericThreads<TDev> const& event) -> void
				            {
				                // Get all the queues on the device at the time of invocation.
				                // All queues added afterwards are ignored.
				                auto vspQueues(dev.getAllQueues());

				                // Let all the queues wait for this event.
				                // Furthermore there should not even be a chance to enqueue something between getting the queues and
				                // adding our wait events!
				                for(auto&& spQueue : vspQueues)
				                {
				                    spQueue->wait(event);
				                }
				            }
				        };

				        //! The CPU non-blocking device queue thread wait trait specialization.
				        //!
				        //! Blocks execution of the calling thread until the queue has finished processing all previously requested
				        //! tasks (kernels, data copies, ...)
				        template<typename TDev>
				        struct CurrentThreadWaitFor<QueueGenericThreadsNonBlocking<TDev>>
				        {
				            ALPAKA_FN_HOST static auto currentThreadWaitFor(QueueGenericThreadsNonBlocking<TDev> const& queue) -> void
				            {
				                EventGenericThreads<TDev> event(getDev(queue));
				                alpaka::enqueue(const_cast<QueueGenericThreadsNonBlocking<TDev>&>(queue), event);
				                wait(event);
				            }
				        };
				    } // namespace trait
				} // namespace alpaka
				// ==
				// == ./include/alpaka/event/EventGenericThreads.hpp ==
				// ============================================================================

			// ==
			// == ./include/alpaka/queue/QueueGenericThreadsBlocking.hpp ==
			// ============================================================================

		// #include "alpaka/queue/QueueGenericThreadsNonBlocking.hpp"    // amalgamate: file already expanded
		// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/queue/cpu/IGenericThreadsQueue.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/traits/Traits.hpp ==
			// ==
			/* Copyright 2022 Antonio Di Pilato
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

			namespace alpaka
			{
			    //! The common trait.
			    namespace trait
			    {
			        //! The native handle trait.
			        template<typename TImpl, typename TSfinae = void>
			        struct NativeHandle
			        {
			            static auto getNativeHandle(TImpl const&)
			            {
			                static_assert(!sizeof(TImpl), "This type does not have a native handle!");
			                return 0;
			            }
			        };
			    } // namespace trait

			    //! Get the native handle of the alpaka object.
			    //! It will return the alpaka object handle if there is any, otherwise it generates a compile time error.
			    template<typename TImpl>
			    ALPAKA_FN_HOST auto getNativeHandle(TImpl const& impl)
			    {
			        return trait::NativeHandle<TImpl>::getNativeHandle(impl);
			    }

			    //! Alias to the type of the native handle.
			    template<typename TImpl>
			    using NativeHandle = decltype(getNativeHandle(std::declval<TImpl>()));
			} // namespace alpaka
			// ==
			// == ./include/alpaka/traits/Traits.hpp ==
			// ============================================================================

		// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

		// #include <algorithm>    // amalgamate: file already included
		// #include <cstddef>    // amalgamate: file already included
		#include <map>
		// #include <memory>    // amalgamate: file already included
		// #include <mutex>    // amalgamate: file already included
		// #include <string>    // amalgamate: file already included
		// #include <vector>    // amalgamate: file already included

		namespace alpaka
		{
		    class DevCpu;
		    namespace cpu
		    {
		        using ICpuQueue = IGenericThreadsQueue<DevCpu>;
		    }
		    namespace trait
		    {
		        template<typename TPltf, typename TSfinae>
		        struct GetDevByIdx;
		    }
		    class PltfCpu;

		    //! The CPU device.
		    namespace cpu::detail
		    {
		        //! The CPU device implementation.
		        using DevCpuImpl = alpaka::detail::QueueRegistry<cpu::ICpuQueue>;
		    } // namespace cpu::detail

		    //! The CPU device handle.
		    class DevCpu
		        : public concepts::Implements<ConceptCurrentThreadWaitFor, DevCpu>
		        , public concepts::Implements<ConceptDev, DevCpu>
		    {
		        friend struct trait::GetDevByIdx<PltfCpu>;

		    protected:
		        DevCpu() : m_spDevCpuImpl(std::make_shared<cpu::detail::DevCpuImpl>())
		        {
		        }

		    public:
		        auto operator==(DevCpu const&) const -> bool
		        {
		            return true;
		        }
		        auto operator!=(DevCpu const& rhs) const -> bool
		        {
		            return !((*this) == rhs);
		        }

		        [[nodiscard]] ALPAKA_FN_HOST auto getAllQueues() const -> std::vector<std::shared_ptr<cpu::ICpuQueue>>
		        {
		            return m_spDevCpuImpl->getAllExistingQueues();
		        }

		        //! Registers the given queue on this device.
		        //! NOTE: Every queue has to be registered for correct functionality of device wait operations!
		        ALPAKA_FN_HOST auto registerQueue(std::shared_ptr<cpu::ICpuQueue> spQueue) const -> void
		        {
		            m_spDevCpuImpl->registerQueue(spQueue);
		        }

		        [[nodiscard]] auto getNativeHandle() const noexcept
		        {
		            return 0;
		        }

		    private:
		        std::shared_ptr<cpu::detail::DevCpuImpl> m_spDevCpuImpl;
		    };

		    namespace trait
		    {
		        //! The CPU device name get trait specialization.
		        template<>
		        struct GetName<DevCpu>
		        {
		            ALPAKA_FN_HOST static auto getName(DevCpu const& /* dev */) -> std::string
		            {
		                return cpu::detail::getCpuName();
		            }
		        };

		        //! The CPU device available memory get trait specialization.
		        template<>
		        struct GetMemBytes<DevCpu>
		        {
		            ALPAKA_FN_HOST static auto getMemBytes(DevCpu const& /* dev */) -> std::size_t
		            {
		                return cpu::detail::getTotalGlobalMemSizeBytes();
		            }
		        };

		        //! The CPU device free memory get trait specialization.
		        template<>
		        struct GetFreeMemBytes<DevCpu>
		        {
		            ALPAKA_FN_HOST static auto getFreeMemBytes(DevCpu const& /* dev */) -> std::size_t
		            {
		                return cpu::detail::getFreeGlobalMemSizeBytes();
		            }
		        };

		        //! The CPU device warp size get trait specialization.
		        template<>
		        struct GetWarpSizes<DevCpu>
		        {
		            ALPAKA_FN_HOST static auto getWarpSizes(DevCpu const& /* dev */) -> std::vector<std::size_t>
		            {
		                return {1u};
		            }
		        };

		        //! The CPU device reset trait specialization.
		        template<>
		        struct Reset<DevCpu>
		        {
		            ALPAKA_FN_HOST static auto reset(DevCpu const& /* dev */) -> void
		            {
		                ALPAKA_DEBUG_FULL_LOG_SCOPE;
		                // The CPU does nothing on reset.
		            }
		        };

		        //! The CPU device native handle type trait specialization.
		        template<>
		        struct NativeHandle<DevCpu>
		        {
		            [[nodiscard]] static auto getNativeHandle(DevCpu const& dev)
		            {
		                return dev.getNativeHandle();
		            }
		        };
		    } // namespace trait

		    template<typename TElem, typename TDim, typename TIdx>
		    class BufCpu;

		    namespace trait
		    {
		        //! The CPU device memory buffer type trait specialization.
		        template<typename TElem, typename TDim, typename TIdx>
		        struct BufType<DevCpu, TElem, TDim, TIdx>
		        {
		            using type = BufCpu<TElem, TDim, TIdx>;
		        };

		        //! The CPU device platform type trait specialization.
		        template<>
		        struct PltfType<DevCpu>
		        {
		            using type = PltfCpu;
		        };
		    } // namespace trait
		    using QueueCpuNonBlocking = QueueGenericThreadsNonBlocking<DevCpu>;
		    using QueueCpuBlocking = QueueGenericThreadsBlocking<DevCpu>;

		    namespace trait
		    {
		        template<>
		        struct QueueType<DevCpu, Blocking>
		        {
		            using type = QueueCpuBlocking;
		        };

		        template<>
		        struct QueueType<DevCpu, NonBlocking>
		        {
		            using type = QueueCpuNonBlocking;
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/dev/DevCpu.hpp ==
		// ============================================================================


	// #include <limits>    // amalgamate: file already included
	// #include <typeinfo>    // amalgamate: file already included

	#ifdef ALPAKA_ACC_CPU_B_OMP2_T_SEQ_ENABLED

	#    if _OPENMP < 200203
	#        error If ALPAKA_ACC_CPU_B_OMP2_T_SEQ_ENABLED is set, the compiler has to support OpenMP 2.0 or higher!
	#    endif

	namespace alpaka
	{
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuOmp2Blocks;

	    //! The CPU OpenMP 2.0 block accelerator.
	    //!
	    //! This accelerator allows parallel kernel execution on a CPU device.
	    //! It uses OpenMP 2.0 to implement the grid block parallelism.
	    //! The block idx is restricted to 1x1x1.
	    template<
	        typename TDim,
	        typename TIdx>
	    class AccCpuOmp2Blocks final :
	        public WorkDivMembers<TDim, TIdx>,
	        public gb::IdxGbRef<TDim, TIdx>,
	        public bt::IdxBtZero<TDim, TIdx>,
	        public AtomicHierarchy<
	            AtomicCpu,   // grid atomics
	            AtomicOmpBuiltIn,    // block atomics
	            AtomicNoOp           // thread atomics
	        >,
	        public math::MathStdLib,
	        public BlockSharedMemDynMember<>,
	        public BlockSharedMemStMember<>,
	        public BlockSyncNoOp,
	        public IntrinsicCpu,
	        public MemFenceOmp2Blocks,
	        public rand::RandStdLib,
	        public warp::WarpSingleThread,
	        public concepts::Implements<ConceptAcc, AccCpuOmp2Blocks<TDim, TIdx>>
	    {
	        static_assert(
	            sizeof(TIdx) >= sizeof(int),
	            "Index type is not supported, consider using int or a larger type.");

	    public:
	        // Partial specialization with the correct TDim and TIdx is not allowed.
	        template<typename TDim2, typename TIdx2, typename TKernelFnObj, typename... TArgs>
	        friend class ::alpaka::TaskKernelCpuOmp2Blocks;

	        AccCpuOmp2Blocks(AccCpuOmp2Blocks const&) = delete;
	        AccCpuOmp2Blocks(AccCpuOmp2Blocks&&) = delete;
	        auto operator=(AccCpuOmp2Blocks const&) -> AccCpuOmp2Blocks& = delete;
	        auto operator=(AccCpuOmp2Blocks&&) -> AccCpuOmp2Blocks& = delete;

	    private:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST AccCpuOmp2Blocks(TWorkDiv const& workDiv, std::size_t const& blockSharedMemDynSizeBytes)
	            : WorkDivMembers<TDim, TIdx>(workDiv)
	            , gb::IdxGbRef<TDim, TIdx>(m_gridBlockIdx)
	            , bt::IdxBtZero<TDim, TIdx>()
	            , AtomicHierarchy<
	                  AtomicCpu, // atomics between grids
	                  AtomicOmpBuiltIn, // atomics between blocks
	                  AtomicNoOp // atomics between threads
	                  >()
	            , math::MathStdLib()
	            , BlockSharedMemDynMember<>(blockSharedMemDynSizeBytes)
	            , BlockSharedMemStMember<>(staticMemBegin(), staticMemCapacity())
	            , BlockSyncNoOp()
	            , MemFenceOmp2Blocks()
	            , rand::RandStdLib()
	            , m_gridBlockIdx(Vec<TDim, TIdx>::zeros())
	        {
	        }

	    private:
	        // getIdx
	        Vec<TDim, TIdx> mutable m_gridBlockIdx; //!< The index of the currently executed block.
	    };

	    namespace trait
	    {
	        //! The CPU OpenMP 2.0 block accelerator accelerator type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct AccType<AccCpuOmp2Blocks<TDim, TIdx>>
	        {
	            using type = AccCpuOmp2Blocks<TDim, TIdx>;
	        };
	        //! The CPU OpenMP 2.0 block accelerator device properties get trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccDevProps<AccCpuOmp2Blocks<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccDevProps(DevCpu const& /* dev */) -> alpaka::AccDevProps<TDim, TIdx>
	            {
	                return {// m_multiProcessorCount
	                        static_cast<TIdx>(1),
	                        // m_gridBlockExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_gridBlockCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_blockThreadExtentMax
	                        Vec<TDim, TIdx>::ones(),
	                        // m_blockThreadCountMax
	                        static_cast<TIdx>(1),
	                        // m_threadElemExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_threadElemCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_sharedMemSizeBytes
	                        static_cast<size_t>(AccCpuOmp2Blocks<TDim, TIdx>::staticAllocBytes())};
	            }
	        };
	        //! The CPU OpenMP 2.0 block accelerator name trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccName<AccCpuOmp2Blocks<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccName() -> std::string
	            {
	                return "AccCpuOmp2Blocks<" + std::to_string(TDim::value) + "," + core::demangled<TIdx> + ">";
	            }
	        };

	        //! The CPU OpenMP 2.0 block accelerator device type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DevType<AccCpuOmp2Blocks<TDim, TIdx>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU OpenMP 2.0 block accelerator dimension getter trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DimType<AccCpuOmp2Blocks<TDim, TIdx>>
	        {
	            using type = TDim;
	        };

	        //! The CPU OpenMP 2.0 block accelerator execution task type trait specialization.
	        template<typename TDim, typename TIdx, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
	        struct CreateTaskKernel<AccCpuOmp2Blocks<TDim, TIdx>, TWorkDiv, TKernelFnObj, TArgs...>
	        {
	            ALPAKA_FN_HOST static auto createTaskKernel(
	                TWorkDiv const& workDiv,
	                TKernelFnObj const& kernelFnObj,
	                TArgs&&... args)
	            {
	                return TaskKernelCpuOmp2Blocks<TDim, TIdx, TKernelFnObj, TArgs...>(
	                    workDiv,
	                    kernelFnObj,
	                    std::forward<TArgs>(args)...);
	            }
	        };

	        //! The CPU OpenMP 2.0 block execution task platform type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct PltfType<AccCpuOmp2Blocks<TDim, TIdx>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU OpenMP 2.0 block accelerator idx type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct IdxType<AccCpuOmp2Blocks<TDim, TIdx>>
	        {
	            using type = TIdx;
	        };

	        template<typename TDim, typename TIdx>
	        struct AccToTag<alpaka::AccCpuOmp2Blocks<TDim, TIdx>>
	        {
	            using type = alpaka::TagCpuOmp2Blocks;
	        };

	        template<typename TDim, typename TIdx>
	        struct TagToAcc<alpaka::TagCpuOmp2Blocks, TDim, TIdx>
	        {
	            using type = alpaka::AccCpuOmp2Blocks<TDim, TIdx>;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/acc/AccCpuOmp2Blocks.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/acc/AccCpuOmp2Threads.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, René Widera, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Base classes.
	// #include "alpaka/atomic/AtomicCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/atomic/AtomicHierarchy.hpp"    // amalgamate: file already expanded
	// #include "alpaka/atomic/AtomicOmpBuiltIn.hpp"    // amalgamate: file already expanded
	// #include "alpaka/block/shared/dyn/BlockSharedMemDynMember.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/block/shared/st/BlockSharedMemStMemberMasterSync.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Erik Zenker, Matthias Werner, René Widera
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/block/shared/st/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/block/shared/st/detail/BlockSharedMemStMemberImpl.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/core/AlignedAlloc.hpp ==
			// ==
			/* Copyright 2022 René Widera, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

			#include <new>

			namespace alpaka::core
			{
			    ALPAKA_FN_INLINE ALPAKA_FN_HOST auto alignedAlloc(size_t alignment, size_t size) -> void*
			    {
			        return ::operator new(size, std::align_val_t{alignment});
			    }

			    ALPAKA_FN_INLINE ALPAKA_FN_HOST void alignedFree(size_t alignment, void* ptr)
			    {
			        ::operator delete(ptr, std::align_val_t{alignment});
			    }
			} // namespace alpaka::core
			// ==
			// == ./include/alpaka/core/AlignedAlloc.hpp ==
			// ============================================================================

		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Vectorize.hpp"    // amalgamate: file already expanded

		// #include <functional>    // amalgamate: file already included
		// #include <memory>    // amalgamate: file already included
		// #include <utility>    // amalgamate: file already included
		// #include <vector>    // amalgamate: file already included


		namespace alpaka
		{
		    template<std::size_t TDataAlignBytes = core::vectorization::defaultAlignment>
		    class BlockSharedMemStMemberMasterSync
		        : public detail::BlockSharedMemStMemberImpl<TDataAlignBytes>
		        , public concepts::Implements<ConceptBlockSharedSt, BlockSharedMemStMemberMasterSync<TDataAlignBytes>>
		    {
		    public:
		        BlockSharedMemStMemberMasterSync(
		            uint8_t* mem,
		            std::size_t capacity,
		            std::function<void()> fnSync,
		            std::function<bool()> fnIsMasterThread)
		            : detail::BlockSharedMemStMemberImpl<TDataAlignBytes>(mem, capacity)
		            , m_syncFn(std::move(fnSync))
		            , m_isMasterThreadFn(std::move(fnIsMasterThread))
		        {
		        }

		        std::function<void()> m_syncFn;
		        std::function<bool()> m_isMasterThreadFn;
		    };

		    namespace trait
		    {
		#if BOOST_COMP_GNUC
		#    pragma GCC diagnostic push
		#    pragma GCC diagnostic ignored                                                                                    \
		        "-Wcast-align" // "cast from 'unsigned char*' to 'unsigned int*' increases required alignment of target type"
		#endif
		        template<typename T, std::size_t TDataAlignBytes, std::size_t TuniqueId>
		        struct DeclareSharedVar<T, TuniqueId, BlockSharedMemStMemberMasterSync<TDataAlignBytes>>
		        {
		            ALPAKA_FN_HOST static auto declareVar(
		                BlockSharedMemStMemberMasterSync<TDataAlignBytes> const& blockSharedMemSt) -> T&
		            {
		                auto* data = blockSharedMemSt.template getVarPtr<T>(TuniqueId);

		                if(!data)
		                {
		                    // Assure that all threads have executed the return of the last allocBlockSharedArr function (if
		                    // there was one before).
		                    blockSharedMemSt.m_syncFn();
		                    if(blockSharedMemSt.m_isMasterThreadFn())
		                    {
		                        blockSharedMemSt.template alloc<T>(TuniqueId);
		                    }

		                    blockSharedMemSt.m_syncFn();
		                    // lookup for the data chunk allocated by the master thread
		                    data = blockSharedMemSt.template getLatestVarPtr<T>();
		                }
		                ALPAKA_ASSERT(data != nullptr);
		                return *data;
		            }
		        };
		#if BOOST_COMP_GNUC
		#    pragma GCC diagnostic pop
		#endif
		        template<std::size_t TDataAlignBytes>
		        struct FreeSharedVars<BlockSharedMemStMemberMasterSync<TDataAlignBytes>>
		        {
		            ALPAKA_FN_HOST static auto freeVars(BlockSharedMemStMemberMasterSync<TDataAlignBytes> const&) -> void
		            {
		                // shared memory block data will be reused
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/block/shared/st/BlockSharedMemStMemberMasterSync.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/block/sync/BlockSyncBarrierOmp.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/block/sync/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

		#ifdef _OPENMP

		namespace alpaka
		{
		    //! The OpenMP barrier block synchronization.
		    class BlockSyncBarrierOmp : public concepts::Implements<ConceptBlockSync, BlockSyncBarrierOmp>
		    {
		    public:
		        std::uint8_t mutable m_generation = 0u;
		        int mutable m_result[2];
		    };

		    namespace trait
		    {
		        template<>
		        struct SyncBlockThreads<BlockSyncBarrierOmp>
		        {
		            ALPAKA_FN_HOST static auto syncBlockThreads(BlockSyncBarrierOmp const& /* blockSync */) -> void
		            {
		// NOTE: This waits for all threads in all blocks.
		// If multiple blocks are executed in parallel this is not optimal.
		#    pragma omp barrier
		            }
		        };

		        namespace detail
		        {
		            template<typename TOp>
		            struct AtomicOp;
		            template<>
		            struct AtomicOp<BlockCount>
		            {
		                void operator()(int& result, bool value)
		                {
		#    pragma omp atomic
		                    result += static_cast<int>(value);
		                }
		            };
		            template<>
		            struct AtomicOp<BlockAnd>
		            {
		                void operator()(int& result, bool value)
		                {
		#    pragma omp atomic
		                    result &= static_cast<int>(value);
		                }
		            };
		            template<>
		            struct AtomicOp<BlockOr>
		            {
		                void operator()(int& result, bool value)
		                {
		#    pragma omp atomic
		                    result |= static_cast<int>(value);
		                }
		            };
		        } // namespace detail

		        template<typename TOp>
		        struct SyncBlockThreadsPredicate<TOp, BlockSyncBarrierOmp>
		        {
		            ALPAKA_NO_HOST_ACC_WARNING
		            ALPAKA_FN_ACC static auto syncBlockThreadsPredicate(BlockSyncBarrierOmp const& blockSync, int predicate)
		                -> int
		            {
		// The first thread initializes the value.
		// There is an implicit barrier at the end of omp single.
		// NOTE: This code is executed only once for all OpenMP threads.
		// If multiple blocks with multiple threads are executed in parallel
		// this reduction is executed only for one block!
		#    pragma omp single
		                {
		                    ++blockSync.m_generation;
		                    blockSync.m_result[blockSync.m_generation % 2u] = TOp::InitialValue;
		                }

		                auto const generationMod2(blockSync.m_generation % 2u);
		                int& result(blockSync.m_result[generationMod2]);
		                bool const predicateBool(predicate != 0);

		                detail::AtomicOp<TOp>()(result, predicateBool);

		// Wait for all threads to write their predicate into the vector.
		// NOTE: This waits for all threads in all blocks.
		// If multiple blocks are executed in parallel this is not optimal.
		#    pragma omp barrier

		                return blockSync.m_result[generationMod2];
		            }
		        };
		    } // namespace trait
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/block/sync/BlockSyncBarrierOmp.hpp ==
		// ============================================================================

	// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/idx/bt/IdxBtOmp.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/idx/MapIdx.hpp ==
			// ==
			/* Copyright 2023 Axel Hübl, Benjamin Worpitz, Erik Zenker, Jan Stephan, Jeffrey Kelling, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			namespace alpaka
			{
			    namespace detail
			    {
			        //! Maps a linear index to a N dimensional index.
			        template<std::size_t TidxDimOut, std::size_t TidxDimIn, typename TSfinae = void>
			        struct MapIdx;
			        //! Maps a N dimensional index to the same N dimensional index.
			        template<std::size_t TidxDim>
			        struct MapIdx<TidxDim, TidxDim>
			        {
			            // \tparam TElem Type of the index values.
			            // \param idx Idx to be mapped.
			            // \param extent Spatial size to map the index to.
			            // \return A N dimensional vector.
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TElem>
			            ALPAKA_FN_HOST_ACC static auto mapIdx(
			                Vec<DimInt<TidxDim>, TElem> const& idx,
			                [[maybe_unused]] Vec<DimInt<TidxDim>, TElem> const& extent) -> Vec<DimInt<TidxDim>, TElem>
			            {
			                return idx;
			            }
			        };
			        //! Maps a 1 dimensional index to a N dimensional index.
			        template<std::size_t TidxDimOut>
			        struct MapIdx<TidxDimOut, 1u, std::enable_if_t<(TidxDimOut > 1u)>>
			        {
			            // \tparam TElem Type of the index values.
			            // \param idx Idx to be mapped.
			            // \param extent Spatial size to map the index to
			            // \return A N dimensional vector.
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TElem>
			            ALPAKA_FN_HOST_ACC static auto mapIdx(
			                Vec<DimInt<1u>, TElem> const& idx,
			                Vec<DimInt<TidxDimOut>, TElem> const& extent) -> Vec<DimInt<TidxDimOut>, TElem>
			            {
			                auto idxNd = Vec<DimInt<TidxDimOut>, TElem>::all(0u);

			                constexpr std::size_t lastIdx(TidxDimOut - 1u);

			                // fast-dim
			                idxNd[lastIdx] = static_cast<TElem>(idx[0u] % extent[lastIdx]);

			                // in-between
			                TElem hyperPlanesBefore = extent[lastIdx];
			                for(std::size_t r(1u); r < lastIdx; ++r)
			                {
			                    std::size_t const d = lastIdx - r;
			                    idxNd[d] = static_cast<TElem>(idx[0u] / hyperPlanesBefore % extent[d]);
			                    hyperPlanesBefore *= extent[d];
			                }

			                // slow-dim
			                idxNd[0u] = static_cast<TElem>(idx[0u] / hyperPlanesBefore);

			                return idxNd;
			            }
			        };
			        //! Maps a 1 dimensional index to a N dimensional index.
			        template<std::size_t TidxDimIn>
			        struct MapIdx<1u, TidxDimIn, std::enable_if_t<(TidxDimIn > 1u)>>
			        {
			            // \tparam TElem Type of the index values.
			            // \param idx Idx to be mapped.
			            // \param extent Spatial size to map the index to.
			            // \return A 1 dimensional vector.
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TElem>
			            ALPAKA_FN_HOST_ACC static auto mapIdx(
			                Vec<DimInt<TidxDimIn>, TElem> const& idx,
			                Vec<DimInt<TidxDimIn>, TElem> const& extent) -> Vec<DimInt<1u>, TElem>
			            {
			                TElem idx1d(idx[0u]);
			                for(std::size_t d(1u); d < TidxDimIn; ++d)
			                {
			                    idx1d = static_cast<TElem>(idx1d * extent[d] + idx[d]);
			                }
			                return {idx1d};
			            }
			        };

			        template<std::size_t TidxDimOut>
			        struct MapIdx<TidxDimOut, 0u>
			        {
			            template<typename TElem, std::size_t TidxDimExtents>
			            ALPAKA_FN_HOST_ACC static auto mapIdx(
			                Vec<DimInt<0u>, TElem> const&,
			                Vec<DimInt<TidxDimExtents>, TElem> const&) -> Vec<DimInt<TidxDimOut>, TElem>
			            {
			                return Vec<DimInt<TidxDimOut>, TElem>::zeros();
			            }
			        };

			        template<std::size_t TidxDimIn>
			        struct MapIdx<0u, TidxDimIn>
			        {
			            template<typename TElem, std::size_t TidxDimExtents>
			            ALPAKA_FN_HOST_ACC static auto mapIdx(
			                Vec<DimInt<TidxDimIn>, TElem> const&,
			                Vec<DimInt<TidxDimExtents>, TElem> const&) -> Vec<DimInt<0u>, TElem>
			            {
			                return {};
			            }
			        };
			    } // namespace detail

			    //! Maps a N dimensional index to a N dimensional position.
			    //!
			    //! \tparam TidxDimOut Dimension of the index vector to map to.
			    //! \tparam TidxDimIn Dimension of the index vector to map from.
			    //! \tparam TElem Type of the elements of the index vector to map from.
			    ALPAKA_NO_HOST_ACC_WARNING template<
			        std::size_t TidxDimOut,
			        std::size_t TidxDimIn,
			        std::size_t TidxDimExtents,
			        typename TElem>
			    ALPAKA_FN_HOST_ACC auto mapIdx(
			        Vec<DimInt<TidxDimIn>, TElem> const& idx,
			        Vec<DimInt<TidxDimExtents>, TElem> const& extent) -> Vec<DimInt<TidxDimOut>, TElem>
			    {
			        return detail::MapIdx<TidxDimOut, TidxDimIn>::mapIdx(idx, extent);
			    }

			    namespace detail
			    {
			        //! Maps a linear index to a N dimensional index assuming a buffer wihtout padding.
			        template<std::size_t TidxDimOut, std::size_t TidxDimIn, typename TSfinae = void>
			        struct MapIdxPitchBytes;
			        //! Maps a N dimensional index to the same N dimensional index assuming a buffer wihtout padding.
			        template<std::size_t TidxDim>
			        struct MapIdxPitchBytes<TidxDim, TidxDim>
			        {
			            // \tparam TElem Type of the index values.
			            // \param idx Idx to be mapped.
			            // \param pitch Spatial pitch (in elems) to map the index to
			            // \return N dimensional vector.
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TElem>
			            ALPAKA_FN_HOST_ACC static auto mapIdxPitchBytes(
			                Vec<DimInt<TidxDim>, TElem> const& idx,
			                [[maybe_unused]] Vec<DimInt<TidxDim>, TElem> const& pitch) -> Vec<DimInt<TidxDim>, TElem>
			            {
			                return idx;
			            }
			        };
			        //! Maps a 1 dimensional index to a N dimensional index assuming a buffer wihtout padding.
			        template<std::size_t TidxDimOut>
			        struct MapIdxPitchBytes<TidxDimOut, 1u, std::enable_if_t<(TidxDimOut > 1u)>>
			        {
			            // \tparam TElem Type of the index values.
			            // \param idx Idx to be mapped.
			            // \param pitch Spatial pitch (in elems) to map the index to
			            // \return N dimensional vector.
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TElem>
			            ALPAKA_FN_HOST_ACC static auto mapIdxPitchBytes(
			                Vec<DimInt<1u>, TElem> const& idx,
			                Vec<DimInt<TidxDimOut>, TElem> const& pitch) -> Vec<DimInt<TidxDimOut>, TElem>
			            {
			                auto idxNd = Vec<DimInt<TidxDimOut>, TElem>::all(0u);

			                constexpr std::size_t lastIdx = TidxDimOut - 1u;

			                TElem tmp = idx[0u];
			                for(std::size_t d(0u); d < lastIdx; ++d)
			                {
			                    idxNd[d] = static_cast<TElem>(tmp / pitch[d + 1]);
			                    tmp %= pitch[d + 1];
			                }
			                idxNd[lastIdx] = tmp;

			                return idxNd;
			            }
			        };
			        //! Maps a N dimensional index to a 1 dimensional index assuming a buffer without padding.
			        template<std::size_t TidxDimIn>
			        struct MapIdxPitchBytes<1u, TidxDimIn, std::enable_if_t<(TidxDimIn > 1u)>>
			        {
			            // \tparam TElem Type of the index values.
			            // \param idx Idx to be mapped.
			            // \param pitch Spatial pitch (in elems) to map the index to
			            // \return A 1 dimensional vector.
			            ALPAKA_NO_HOST_ACC_WARNING
			            template<typename TElem>
			            ALPAKA_FN_HOST_ACC static auto mapIdxPitchBytes(
			                Vec<DimInt<TidxDimIn>, TElem> const& idx,
			                Vec<DimInt<TidxDimIn>, TElem> const& pitch) -> Vec<DimInt<1u>, TElem>
			            {
			                constexpr auto lastDim = TidxDimIn - 1;
			                TElem idx1d = idx[lastDim];
			                for(std::size_t d(0u); d < lastDim; ++d)
			                {
			                    idx1d = static_cast<TElem>(idx1d + pitch[d + 1] * idx[d]);
			                }
			                return {idx1d};
			            }
			        };

			        template<std::size_t TidxDimOut>
			        struct MapIdxPitchBytes<TidxDimOut, 0u>
			        {
			            template<typename TElem, std::size_t TidxDimExtents>
			            ALPAKA_FN_HOST_ACC static auto mapIdxPitchBytes(
			                Vec<DimInt<0u>, TElem> const&,
			                Vec<DimInt<TidxDimExtents>, TElem> const&) -> Vec<DimInt<TidxDimOut>, TElem>
			            {
			                return Vec<DimInt<TidxDimOut>, TElem>::zeros();
			            }
			        };

			        template<std::size_t TidxDimIn>
			        struct MapIdxPitchBytes<0u, TidxDimIn>
			        {
			            template<typename TElem, std::size_t TidxDimExtents>
			            ALPAKA_FN_HOST_ACC static auto mapIdxPitchBytes(
			                Vec<DimInt<TidxDimIn>, TElem> const&,
			                Vec<DimInt<TidxDimExtents>, TElem> const&) -> Vec<DimInt<0u>, TElem>
			            {
			                return {};
			            }
			        };
			    } // namespace detail

			    //! Maps a N dimensional index to a N dimensional position based on
			    //! pitch in a buffer without padding or a byte buffer.
			    //!
			    //! \tparam TidxDimOut Dimension of the index vector to map to.
			    //! \tparam TidxDimIn Dimension of the index vector to map from.
			    //! \tparam TElem Type of the elements of the index vector to map from.
			    ALPAKA_NO_HOST_ACC_WARNING
			    template<std::size_t TidxDimOut, std::size_t TidxDimIn, std::size_t TidxDimPitch, typename TElem>
			    ALPAKA_FN_HOST_ACC auto mapIdxPitchBytes(
			        Vec<DimInt<TidxDimIn>, TElem> const& idx,
			        Vec<DimInt<TidxDimPitch>, TElem> const& pitch) -> Vec<DimInt<TidxDimOut>, TElem>
			    {
			        return detail::MapIdxPitchBytes<TidxDimOut, TidxDimIn>::mapIdxPitchBytes(idx, pitch);
			    }
			} // namespace alpaka
			// ==
			// == ./include/alpaka/idx/MapIdx.hpp ==
			// ============================================================================

		// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded
		// #include "alpaka/workdiv/Traits.hpp"    // amalgamate: file already expanded

		#ifdef _OPENMP

		// #    include <omp.h>    // amalgamate: file already included

		namespace alpaka
		{
		    namespace bt
		    {
		        //! The OpenMP accelerator index provider.
		        template<typename TDim, typename TIdx>
		        class IdxBtOmp : public concepts::Implements<ConceptIdxBt, IdxBtOmp<TDim, TIdx>>
		        {
		        };
		    } // namespace bt

		    namespace trait
		    {
		        //! The OpenMP accelerator index dimension get trait specialization.
		        template<typename TDim, typename TIdx>
		        struct DimType<bt::IdxBtOmp<TDim, TIdx>>
		        {
		            using type = TDim;
		        };

		        //! The OpenMP accelerator block thread index get trait specialization.
		        template<typename TDim, typename TIdx>
		        struct GetIdx<bt::IdxBtOmp<TDim, TIdx>, origin::Block, unit::Threads>
		        {
		            //! \return The index of the current thread in the block.
		            template<typename TWorkDiv>
		            static auto getIdx(bt::IdxBtOmp<TDim, TIdx> const& /* idx */, TWorkDiv const& workDiv) -> Vec<TDim, TIdx>
		            {
		                // We assume that the thread id is positive.
		                ALPAKA_ASSERT_OFFLOAD(::omp_get_thread_num() >= 0);
		                // \TODO: Would it be faster to precompute the index and cache it inside an array?
		                return mapIdx<TDim::value>(
		                    Vec<DimInt<1u>, TIdx>(static_cast<TIdx>(::omp_get_thread_num())),
		                    getWorkDiv<Block, Threads>(workDiv));
		            }
		        };

		        template<typename TIdx>
		        struct GetIdx<bt::IdxBtOmp<DimInt<1u>, TIdx>, origin::Block, unit::Threads>
		        {
		            //! \return The index of the current thread in the block.
		            template<typename TWorkDiv>
		            static auto getIdx(bt::IdxBtOmp<DimInt<1u>, TIdx> const& /* idx */, TWorkDiv const&)
		                -> Vec<DimInt<1u>, TIdx>
		            {
		                return Vec<DimInt<1u>, TIdx>(static_cast<TIdx>(::omp_get_thread_num()));
		            }
		        };

		        //! The OpenMP accelerator block thread index idx type trait specialization.
		        template<typename TDim, typename TIdx>
		        struct IdxType<bt::IdxBtOmp<TDim, TIdx>>
		        {
		            using type = TIdx;
		        };
		    } // namespace trait
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/idx/bt/IdxBtOmp.hpp ==
		// ============================================================================

	// #include "alpaka/idx/gb/IdxGbRef.hpp"    // amalgamate: file already expanded
	// #include "alpaka/intrinsic/IntrinsicCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/math/MathStdLib.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/mem/fence/MemFenceOmp2Threads.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/fence/Traits.hpp"    // amalgamate: file already expanded

		#ifdef ALPAKA_ACC_CPU_B_SEQ_T_OMP2_ENABLED

		#    if _OPENMP < 200203
		#        error If ALPAKA_ACC_CPU_B_SEQ_T_OMP2_ENABLED is set, the compiler has to support OpenMP 2.0 or higher!
		#    endif

		namespace alpaka
		{
		    //! The CPU OpenMP 2.0 block memory fence.
		    class MemFenceOmp2Threads : public concepts::Implements<ConceptMemFence, MemFenceOmp2Threads>
		    {
		    };

		    namespace trait
		    {
		        template<typename TMemScope>
		        struct MemFence<MemFenceOmp2Threads, TMemScope>
		        {
		            static auto mem_fence(MemFenceOmp2Threads const&, TMemScope const&)
		            {
		                /*
		                 * Intuitively, this pragma creates a fence on the block level.
		                 *
		                 * Creating a block fence is enough for the whole device because the blocks are executed serially. By
		                 * definition of fences, preceding blocks don't have a guarantee to see the results of this block's
		                 * STORE operations (only that they will be ordered correctly); the following blocks see the results
		                 * once they start. Consider the following code:
		                 *
		                 * int x = 1;
		                 * int y = 2;
		                 *
		                 * void foo()
		                 * {
		                 *     x = 10;
		                 *     alpaka::mem_fence(acc, memory_scope::device);
		                 *     y = 20;
		                 * }
		                 *
		                 * void bar()
		                 * {
		                 *     auto b = y;
		                 *     alpaka::mem_fence(acc, memory_scope::device);
		                 *     auto a = x;
		                 * }
		                 *
		                 * The following are all valid outcomes:
		                 *   a == 1 && b == 2
		                 *   a == 10 && b == 2
		                 *   a == 10 && b == 20
		                 */
		#    pragma omp flush
		#    ifdef _MSC_VER
		                ; // MSVC needs an empty statement here or it diagnoses a syntax error
		#    endif
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		#endif
		// ==
		// == ./include/alpaka/mem/fence/MemFenceOmp2Threads.hpp ==
		// ============================================================================

	// #include "alpaka/rand/RandStdLib.hpp"    // amalgamate: file already expanded
	// #include "alpaka/warp/WarpSingleThread.hpp"    // amalgamate: file already expanded
	// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

	// Specialized traits.
	// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
	// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/core/ClipCast.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/meta/Integral.hpp"    // amalgamate: file already expanded

		// #include <algorithm>    // amalgamate: file already included
		// #include <limits>    // amalgamate: file already included

		namespace alpaka::core
		{
		    //! \return The input casted and clipped to T.
		    template<typename T, typename V>
		    auto clipCast(V const& val) -> T
		    {
		        static_assert(
		            std::is_integral_v<T> && std::is_integral_v<V>,
		            "clipCast can not be called with non-integral types!");

		        auto constexpr max = static_cast<V>(std::numeric_limits<alpaka::meta::LowerMax<T, V>>::max());
		        auto constexpr min = static_cast<V>(std::numeric_limits<alpaka::meta::HigherMin<T, V>>::min());

		        return static_cast<T>(std::max(min, std::min(max, val)));
		    }
		} // namespace alpaka::core
		// ==
		// == ./include/alpaka/core/ClipCast.hpp ==
		// ============================================================================

	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded

	// #include <limits>    // amalgamate: file already included
	// #include <typeinfo>    // amalgamate: file already included

	#ifdef ALPAKA_ACC_CPU_B_SEQ_T_OMP2_ENABLED

	#    if _OPENMP < 200203
	#        error If ALPAKA_ACC_CPU_B_SEQ_T_OMP2_ENABLED is set, the compiler has to support OpenMP 2.0 or higher!
	#    endif

	// #    include <omp.h>    // amalgamate: file already included

	namespace alpaka
	{
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuOmp2Threads;

	    //! The CPU OpenMP 2.0 thread accelerator.
	    //!
	    //! This accelerator allows parallel kernel execution on a CPU device.
	    //! It uses OpenMP 2.0 to implement the block thread parallelism.
	    template<
	        typename TDim,
	        typename TIdx>
	    class AccCpuOmp2Threads final :
	        public WorkDivMembers<TDim, TIdx>,
	        public gb::IdxGbRef<TDim, TIdx>,
	        public bt::IdxBtOmp<TDim, TIdx>,
	        public AtomicHierarchy<
	            AtomicCpu,   // grid atomics
	            AtomicOmpBuiltIn,    // block atomics
	            AtomicOmpBuiltIn     // thread atomics
	        >,
	        public math::MathStdLib,
	        public BlockSharedMemDynMember<>,
	        public BlockSharedMemStMemberMasterSync<>,
	        public BlockSyncBarrierOmp,
	        public IntrinsicCpu,
	        public MemFenceOmp2Threads,
	        public rand::RandStdLib,
	        public warp::WarpSingleThread,
	        public concepts::Implements<ConceptAcc, AccCpuOmp2Threads<TDim, TIdx>>
	    {
	        static_assert(
	            sizeof(TIdx) >= sizeof(int),
	            "Index type is not supported, consider using int or a larger type.");

	    public:
	        // Partial specialization with the correct TDim and TIdx is not allowed.
	        template<typename TDim2, typename TIdx2, typename TKernelFnObj, typename... TArgs>
	        friend class ::alpaka::TaskKernelCpuOmp2Threads;

	        AccCpuOmp2Threads(AccCpuOmp2Threads const&) = delete;
	        AccCpuOmp2Threads(AccCpuOmp2Threads&&) = delete;
	        auto operator=(AccCpuOmp2Threads const&) -> AccCpuOmp2Threads& = delete;
	        auto operator=(AccCpuOmp2Threads&&) -> AccCpuOmp2Threads& = delete;

	    private:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST AccCpuOmp2Threads(TWorkDiv const& workDiv, std::size_t const& blockSharedMemDynSizeBytes)
	            : WorkDivMembers<TDim, TIdx>(workDiv)
	            , gb::IdxGbRef<TDim, TIdx>(m_gridBlockIdx)
	            , bt::IdxBtOmp<TDim, TIdx>()
	            , AtomicHierarchy<
	                  AtomicCpu, // atomics between grids
	                  AtomicOmpBuiltIn, // atomics between blocks
	                  AtomicOmpBuiltIn // atomics between threads
	                  >()
	            , math::MathStdLib()
	            , BlockSharedMemDynMember<>(blockSharedMemDynSizeBytes)
	            , BlockSharedMemStMemberMasterSync<>(
	                  staticMemBegin(),
	                  staticMemCapacity(),
	                  [this]() { syncBlockThreads(*this); },
	                  []() noexcept { return (::omp_get_thread_num() == 0); })
	            , BlockSyncBarrierOmp()
	            , MemFenceOmp2Threads()
	            , rand::RandStdLib()
	            , m_gridBlockIdx(Vec<TDim, TIdx>::zeros())
	        {
	        }

	    private:
	        // getIdx
	        Vec<TDim, TIdx> mutable m_gridBlockIdx; //!< The index of the currently executed block.
	    };

	    namespace trait
	    {
	        //! The CPU OpenMP 2.0 thread accelerator accelerator type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct AccType<AccCpuOmp2Threads<TDim, TIdx>>
	        {
	            using type = AccCpuOmp2Threads<TDim, TIdx>;
	        };
	        //! The CPU OpenMP 2.0 thread accelerator device properties get trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccDevProps<AccCpuOmp2Threads<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccDevProps(DevCpu const& dev) -> alpaka::AccDevProps<TDim, TIdx>
	            {
	#    ifdef ALPAKA_CI
	                auto const blockThreadCountMax = alpaka::core::clipCast<TIdx>(std::min(4, ::omp_get_max_threads()));
	#    else
	                auto const blockThreadCountMax = alpaka::core::clipCast<TIdx>(::omp_get_max_threads());
	#    endif
	                return {// m_multiProcessorCount
	                        static_cast<TIdx>(1),
	                        // m_gridBlockExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_gridBlockCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_blockThreadExtentMax
	                        Vec<TDim, TIdx>::all(blockThreadCountMax),
	                        // m_blockThreadCountMax
	                        blockThreadCountMax,
	                        // m_threadElemExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_threadElemCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_sharedMemSizeBytes
	                        getMemBytes(dev)};
	            }
	        };
	        //! The CPU OpenMP 2.0 thread accelerator name trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccName<AccCpuOmp2Threads<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccName() -> std::string
	            {
	                return "AccCpuOmp2Threads<" + std::to_string(TDim::value) + "," + core::demangled<TIdx> + ">";
	            }
	        };

	        //! The CPU OpenMP 2.0 thread accelerator device type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DevType<AccCpuOmp2Threads<TDim, TIdx>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU OpenMP 2.0 thread accelerator dimension getter trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DimType<AccCpuOmp2Threads<TDim, TIdx>>
	        {
	            using type = TDim;
	        };

	        //! The CPU OpenMP 2.0 thread accelerator execution task type trait specialization.
	        template<typename TDim, typename TIdx, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
	        struct CreateTaskKernel<AccCpuOmp2Threads<TDim, TIdx>, TWorkDiv, TKernelFnObj, TArgs...>
	        {
	            ALPAKA_FN_HOST static auto createTaskKernel(
	                TWorkDiv const& workDiv,
	                TKernelFnObj const& kernelFnObj,
	                TArgs&&... args)
	            {
	                return TaskKernelCpuOmp2Threads<TDim, TIdx, TKernelFnObj, TArgs...>(
	                    workDiv,
	                    kernelFnObj,
	                    std::forward<TArgs>(args)...);
	            }
	        };

	        //! The CPU OpenMP 2.0 thread execution task platform type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct PltfType<AccCpuOmp2Threads<TDim, TIdx>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU OpenMP 2.0 thread accelerator idx type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct IdxType<AccCpuOmp2Threads<TDim, TIdx>>
	        {
	            using type = TIdx;
	        };

	        template<typename TDim, typename TIdx>
	        struct AccToTag<alpaka::AccCpuOmp2Threads<TDim, TIdx>>
	        {
	            using type = alpaka::TagCpuOmp2Threads;
	        };

	        template<typename TDim, typename TIdx>
	        struct TagToAcc<alpaka::TagCpuOmp2Threads, TDim, TIdx>
	        {
	            using type = alpaka::AccCpuOmp2Threads<TDim, TIdx>;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/acc/AccCpuOmp2Threads.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/acc/AccCpuSerial.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, René Widera, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Base classes.
	// #include "alpaka/atomic/AtomicCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/atomic/AtomicHierarchy.hpp"    // amalgamate: file already expanded
	// #include "alpaka/atomic/AtomicNoOp.hpp"    // amalgamate: file already expanded
	// #include "alpaka/block/shared/dyn/BlockSharedMemDynMember.hpp"    // amalgamate: file already expanded
	// #include "alpaka/block/shared/st/BlockSharedMemStMember.hpp"    // amalgamate: file already expanded
	// #include "alpaka/block/sync/BlockSyncNoOp.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/bt/IdxBtZero.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/gb/IdxGbRef.hpp"    // amalgamate: file already expanded
	// #include "alpaka/intrinsic/IntrinsicCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/math/MathStdLib.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/mem/fence/MemFenceCpuSerial.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan, Andrea Bocci
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/fence/Traits.hpp"    // amalgamate: file already expanded

		// #include <atomic>    // amalgamate: file already included

		namespace alpaka
		{
		    //! The serial CPU memory fence.
		    class MemFenceCpuSerial : public concepts::Implements<ConceptMemFence, MemFenceCpuSerial>
		    {
		    };

		    namespace trait
		    {
		        template<>
		        struct MemFence<MemFenceCpuSerial, memory_scope::Block>
		        {
		            static auto mem_fence(MemFenceCpuSerial const&, memory_scope::Block const&)
		            {
		                /* Nothing to be done on the block level for the serial case. */
		            }
		        };

		        template<>
		        struct MemFence<MemFenceCpuSerial, memory_scope::Grid>
		        {
		            static auto mem_fence(MemFenceCpuSerial const&, memory_scope::Grid const&)
		            {
		                /* Nothing to be done on the grid level for the serial case. */
		            }
		        };

		        template<typename TMemScope>
		        struct MemFence<MemFenceCpuSerial, TMemScope>
		        {
		            static auto mem_fence(MemFenceCpuSerial const&, TMemScope const&)
		            {
		                /* Enable device fences because we may want to synchronize with other (serial) kernels. */

		                static std::atomic<int> dummy{42};

		                /* ISO C++ fences are only clearly defined if there are atomic operations surrounding them. So we use
		                 * these dummy operations to ensure this.*/
		                auto x = dummy.load(std::memory_order_relaxed);
		                std::atomic_thread_fence(std::memory_order_acq_rel);
		                dummy.store(x, std::memory_order_relaxed);
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/mem/fence/MemFenceCpuSerial.hpp ==
		// ============================================================================

	// #include "alpaka/rand/RandStdLib.hpp"    // amalgamate: file already expanded
	// #include "alpaka/warp/WarpSingleThread.hpp"    // amalgamate: file already expanded
	// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

	// Specialized traits.
	// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
	// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded

	// #include <memory>    // amalgamate: file already included
	// #include <typeinfo>    // amalgamate: file already included

	#ifdef ALPAKA_ACC_CPU_B_SEQ_T_SEQ_ENABLED

	namespace alpaka
	{
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuSerial;

	    //! The CPU serial accelerator.
	    //!
	    //! This accelerator allows serial kernel execution on a CPU device.
	    //! The block idx is restricted to 1x1x1 and all blocks are executed serially so there is no parallelism at all.
	    template<
	        typename TDim,
	        typename TIdx>
	    class AccCpuSerial final :
	        public WorkDivMembers<TDim, TIdx>,
	        public gb::IdxGbRef<TDim, TIdx>,
	        public bt::IdxBtZero<TDim, TIdx>,
	        public AtomicHierarchy<
	            AtomicCpu, // grid atomics
	            AtomicNoOp,        // block atomics
	            AtomicNoOp         // thread atomics
	        >,
	        public math::MathStdLib,
	        public BlockSharedMemDynMember<>,
	        public BlockSharedMemStMember<>,
	        public BlockSyncNoOp,
	        public IntrinsicCpu,
	        public MemFenceCpuSerial,
	        public rand::RandStdLib,
	        public warp::WarpSingleThread,
	        public concepts::Implements<ConceptAcc, AccCpuSerial<TDim, TIdx>>
	    {
	        static_assert(
	            sizeof(TIdx) >= sizeof(int),
	            "Index type is not supported, consider using int or a larger type.");

	    public:
	        // Partial specialization with the correct TDim and TIdx is not allowed.
	        template<typename TDim2, typename TIdx2, typename TKernelFnObj, typename... TArgs>
	        friend class ::alpaka::TaskKernelCpuSerial;

	        AccCpuSerial(AccCpuSerial const&) = delete;
	        AccCpuSerial(AccCpuSerial&&) = delete;
	        auto operator=(AccCpuSerial const&) -> AccCpuSerial& = delete;
	        auto operator=(AccCpuSerial&&) -> AccCpuSerial& = delete;

	    private:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST AccCpuSerial(TWorkDiv const& workDiv, size_t const& blockSharedMemDynSizeBytes)
	            : WorkDivMembers<TDim, TIdx>(workDiv)
	            , gb::IdxGbRef<TDim, TIdx>(m_gridBlockIdx)
	            , bt::IdxBtZero<TDim, TIdx>()
	            , AtomicHierarchy<
	                  AtomicCpu, // atomics between grids
	                  AtomicNoOp, // atomics between blocks
	                  AtomicNoOp // atomics between threads
	                  >()
	            , math::MathStdLib()
	            , BlockSharedMemDynMember<>(blockSharedMemDynSizeBytes)
	            , BlockSharedMemStMember<>(staticMemBegin(), staticMemCapacity())
	            , BlockSyncNoOp()
	            , MemFenceCpuSerial()
	            , rand::RandStdLib()
	            , m_gridBlockIdx(Vec<TDim, TIdx>::zeros())
	        {
	        }

	    private:
	        // getIdx
	        Vec<TDim, TIdx> mutable m_gridBlockIdx; //!< The index of the currently executed block.
	    };

	    namespace trait
	    {
	        //! The CPU serial accelerator accelerator type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct AccType<AccCpuSerial<TDim, TIdx>>
	        {
	            using type = AccCpuSerial<TDim, TIdx>;
	        };
	        //! The CPU serial accelerator device properties get trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccDevProps<AccCpuSerial<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccDevProps(DevCpu const& /* dev */) -> AccDevProps<TDim, TIdx>
	            {
	                return {// m_multiProcessorCount
	                        static_cast<TIdx>(1),
	                        // m_gridBlockExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_gridBlockCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_blockThreadExtentMax
	                        Vec<TDim, TIdx>::ones(),
	                        // m_blockThreadCountMax
	                        static_cast<TIdx>(1),
	                        // m_threadElemExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_threadElemCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_sharedMemSizeBytes
	                        static_cast<size_t>(AccCpuSerial<TDim, TIdx>::staticAllocBytes())};
	            }
	        };
	        //! The CPU serial accelerator name trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccName<AccCpuSerial<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccName() -> std::string
	            {
	                return "AccCpuSerial<" + std::to_string(TDim::value) + "," + core::demangled<TIdx> + ">";
	            }
	        };

	        //! The CPU serial accelerator device type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DevType<AccCpuSerial<TDim, TIdx>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU serial accelerator dimension getter trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DimType<AccCpuSerial<TDim, TIdx>>
	        {
	            using type = TDim;
	        };

	        //! The CPU serial accelerator execution task type trait specialization.
	        template<typename TDim, typename TIdx, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
	        struct CreateTaskKernel<AccCpuSerial<TDim, TIdx>, TWorkDiv, TKernelFnObj, TArgs...>
	        {
	            ALPAKA_FN_HOST static auto createTaskKernel(
	                TWorkDiv const& workDiv,
	                TKernelFnObj const& kernelFnObj,
	                TArgs&&... args)
	            {
	                return TaskKernelCpuSerial<TDim, TIdx, TKernelFnObj, TArgs...>(
	                    workDiv,
	                    kernelFnObj,
	                    std::forward<TArgs>(args)...);
	            }
	        };

	        //! The CPU serial execution task platform type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct PltfType<AccCpuSerial<TDim, TIdx>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU serial accelerator idx type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct IdxType<AccCpuSerial<TDim, TIdx>>
	        {
	            using type = TIdx;
	        };

	        template<typename TDim, typename TIdx>
	        struct AccToTag<alpaka::AccCpuSerial<TDim, TIdx>>
	        {
	            using type = alpaka::TagCpuSerial;
	        };

	        template<typename TDim, typename TIdx>
	        struct TagToAcc<alpaka::TagCpuSerial, TDim, TIdx>
	        {
	            using type = alpaka::AccCpuSerial<TDim, TIdx>;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/acc/AccCpuSerial.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/acc/AccCpuSyclIntel.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
		// ============================================================================
		// == ./include/alpaka/acc/AccGenericSycl.hpp ==
		// ==
		/* Copyright 2023 Jan Stephan, Antonio Di Pilato, Andrea Bocci
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// Base classes.
			// ============================================================================
			// == ./include/alpaka/atomic/AtomicGenericSycl.hpp ==
			// ==
			/* Copyright 2023 Jan Stephan, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/atomic/Op.hpp"    // amalgamate: file already expanded
			// #include "alpaka/atomic/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/meta/DependentFalseType.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include <type_traits>    // amalgamate: file already included

				namespace alpaka::meta
				{
				    //! A false_type being dependent on a ignored template parameter.
				    //! This allows to use static_assert in uninstantiated template specializations without triggering.
				    template<typename T>
				    struct DependentFalseType : std::false_type
				    {
				    };
				} // namespace alpaka::meta
				// ==
				// == ./include/alpaka/meta/DependentFalseType.hpp ==
				// ============================================================================


			// #include <cstdint>    // amalgamate: file already included
			// #include <type_traits>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			#    include <CL/sycl.hpp>

			namespace alpaka
			{
			    //! The SYCL accelerator atomic ops.
			    //
			    //  Atomics can used in the hierarchy level grids, blocks and threads.
			    //  Atomics are not guaranteed to be safe between devices
			    class AtomicGenericSycl
			    {
			    };

			    namespace detail
			    {
			        template<typename THierarchy>
			        struct SyclMemoryScope
			        {
			        };

			        template<>
			        struct SyclMemoryScope<hierarchy::Grids>
			        {
			            static constexpr auto value = sycl::memory_scope::device;
			        };

			        template<>
			        struct SyclMemoryScope<hierarchy::Blocks>
			        {
			            static constexpr auto value = sycl::memory_scope::device;
			        };

			        template<>
			        struct SyclMemoryScope<hierarchy::Threads>
			        {
			            static constexpr auto value = sycl::memory_scope::work_group;
			        };

			        template<typename T>
			        inline auto get_global_ptr(T* const addr)
			        {
			            return sycl::address_space_cast<sycl::access::address_space::global_space, sycl::access::decorated::no>(
			                addr);
			        }

			        template<typename T>
			        inline auto get_local_ptr(T* const addr)
			        {
			            return sycl::address_space_cast<sycl::access::address_space::local_space, sycl::access::decorated::no>(
			                addr);
			        }

			        template<typename T, typename THierarchy>
			        using global_ref = sycl::atomic_ref<
			            T,
			            sycl::memory_order::relaxed,
			            SyclMemoryScope<THierarchy>::value,
			            sycl::access::address_space::global_space>;

			        template<typename T, typename THierarchy>
			        using local_ref = sycl::atomic_ref<
			            T,
			            sycl::memory_order::relaxed,
			            SyclMemoryScope<THierarchy>::value,
			            sycl::access::address_space::local_space>;

			        template<typename THierarchy, typename T, typename TOp>
			        inline auto callAtomicOp(T* const addr, TOp&& op)
			        {
			            if(auto ptr = get_global_ptr(addr); ptr != nullptr)
			            {
			                auto ref = global_ref<T, THierarchy>{*addr};
			                return op(ref);
			            }
			            else
			            {
			                auto ref = local_ref<T, THierarchy>{*addr};
			                return op(ref);
			            }
			        }

			        template<typename TRef, typename T, typename TEval>
			        inline auto casWithCondition(T* const addr, TEval&& eval)
			        {
			            auto ref = TRef{*addr};

			            auto old_val = ref.load();
			            auto assumed = T{};

			            do
			            {
			                assumed = old_val;
			                auto const new_val = eval(old_val);
			                old_val = ref.compare_exchange_strong(assumed, new_val);
			            } while(assumed != old_val);


			            return old_val;
			        }
			    } // namespace detail
			} // namespace alpaka

			namespace alpaka::trait
			{
			    // Add.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicAdd, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_integral_v<T> || std::is_floating_point_v<T>, "SYCL atomics do not support this type");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            return alpaka::detail::callAtomicOp<THierarchy>(
			                addr,
			                [&value](auto& ref) { return ref.fetch_add(value); });
			        }
			    };

			    // Sub.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicSub, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_integral_v<T> || std::is_floating_point_v<T>, "SYCL atomics do not support this type");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            return alpaka::detail::callAtomicOp<THierarchy>(
			                addr,
			                [&value](auto& ref) { return ref.fetch_sub(value); });
			        }
			    };

			    // Min.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicMin, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_integral_v<T> || std::is_floating_point_v<T>, "SYCL atomics do not support this type");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            return alpaka::detail::callAtomicOp<THierarchy>(
			                addr,
			                [&value](auto& ref) { return ref.fetch_min(value); });
			        }
			    };

			    // Max.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicMax, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_integral_v<T> || std::is_floating_point_v<T>, "SYCL atomics do not support this type");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            return alpaka::detail::callAtomicOp<THierarchy>(
			                addr,
			                [&value](auto& ref) { return ref.fetch_max(value); });
			        }
			    };

			    // Exch.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicExch, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_integral_v<T> || std::is_floating_point_v<T>, "SYCL atomics do not support this type");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            return alpaka::detail::callAtomicOp<THierarchy>(addr, [&value](auto& ref) { return ref.exchange(value); });
			        }
			    };

			    // Inc.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicInc, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_unsigned_v<T>, "atomicInc only supported for unsigned types");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            auto inc = [&value](auto old_val) { return (old_val >= value) ? static_cast<T>(0) : (old_val + 1u); };
			            if(auto ptr = alpaka::detail::get_global_ptr(addr); ptr != nullptr)
			                return alpaka::detail::casWithCondition<alpaka::detail::global_ref<T, THierarchy>>(addr, inc);
			            else
			                return alpaka::detail::casWithCondition<alpaka::detail::local_ref<T, THierarchy>>(addr, inc);
			        }
			    };

			    // Dec.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicDec, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_unsigned_v<T>, "atomicDec only supported for unsigned types");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            auto dec
			                = [&value](auto& old_val) { return ((old_val == 0) || (old_val > value)) ? value : (old_val - 1u); };
			            if(auto ptr = alpaka::detail::get_global_ptr(addr); ptr != nullptr)
			                return alpaka::detail::casWithCondition<alpaka::detail::global_ref<T, THierarchy>>(addr, dec);
			            else
			                return alpaka::detail::casWithCondition<alpaka::detail::local_ref<T, THierarchy>>(addr, dec);
			        }
			    };

			    // And.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicAnd, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_integral_v<T>, "Bitwise operations only supported for integral types.");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            return alpaka::detail::callAtomicOp<THierarchy>(
			                addr,
			                [&value](auto& ref) { return ref.fetch_and(value); });
			        }
			    };

			    // Or.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicOr, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_integral_v<T>, "Bitwise operations only supported for integral types.");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            return alpaka::detail::callAtomicOp<THierarchy>(addr, [&value](auto& ref) { return ref.fetch_or(value); });
			        }
			    };

			    // Xor.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicXor, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_integral_v<T>, "Bitwise operations only supported for integral types.");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& value) -> T
			        {
			            return alpaka::detail::callAtomicOp<THierarchy>(
			                addr,
			                [&value](auto& ref) { return ref.fetch_xor(value); });
			        }
			    };

			    // Cas.
			    //! The SYCL accelerator atomic operation.
			    template<typename T, typename THierarchy>
			    struct AtomicOp<AtomicCas, AtomicGenericSycl, T, THierarchy>
			    {
			        static_assert(std::is_integral_v<T> || std::is_floating_point_v<T>, "SYCL atomics do not support this type");

			        static auto atomicOp(AtomicGenericSycl const&, T* const addr, T const& compare, T const& value) -> T
			        {
			            auto cas = [&compare, &value](auto& ref)
			            {
			                // SYCL stores the value in *addr to the "compare" parameter if the values are not equal. Since
			                // alpaka's interface does not expect this we need to copy "compare" to this function and forget it
			                // afterwards.
			                auto tmp = compare;

			                // We always want to return the old value at the end.
			                const auto old = ref.load();

			                // This returns a bool telling us if the exchange happened or not. Useless in this case.
			                ref.compare_exchange_strong(tmp, value);

			                return old;
			            };

			            if(auto ptr = alpaka::detail::get_global_ptr(addr); ptr != nullptr)
			            {
			                auto ref = alpaka::detail::global_ref<T, THierarchy>{*addr};
			                return cas(ref);
			            }
			            else
			            {
			                auto ref = alpaka::detail::local_ref<T, THierarchy>{*addr};
			                return cas(ref);
			            }
			        }
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/atomic/AtomicGenericSycl.hpp ==
			// ============================================================================

		// #include "alpaka/atomic/AtomicHierarchy.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/block/shared/dyn/BlockSharedMemDynGenericSycl.hpp ==
			// ==
			/* Copyright 2023 Jan Stephan, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/block/shared/dyn/Traits.hpp"    // amalgamate: file already expanded

			// #include <cstddef>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED
			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka
			{
			    //! The SYCL block shared memory allocator.
			    class BlockSharedMemDynGenericSycl
			        : public concepts::Implements<ConceptBlockSharedDyn, BlockSharedMemDynGenericSycl>
			    {
			    public:
			        using BlockSharedMemDynBase = BlockSharedMemDynGenericSycl;

			        BlockSharedMemDynGenericSycl(sycl::local_accessor<std::byte> accessor) : m_accessor{accessor}
			        {
			        }

			        sycl::local_accessor<std::byte> m_accessor;
			    };
			} // namespace alpaka

			namespace alpaka::trait
			{
			    template<typename T>
			    struct GetDynSharedMem<T, BlockSharedMemDynGenericSycl>
			    {
			        static auto getMem(BlockSharedMemDynGenericSycl const& shared) -> T*
			        {
			            auto void_ptr = sycl::multi_ptr<void, sycl::access::address_space::local_space>{shared.m_accessor};
			            auto t_ptr = static_cast<sycl::multi_ptr<T, sycl::access::address_space::local_space>>(void_ptr);
			            return t_ptr.get();
			        }
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/block/shared/dyn/BlockSharedMemDynGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/block/shared/st/BlockSharedMemStGenericSycl.hpp ==
			// ==
			/* Copyright 2023 Jan Stephan, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/block/shared/st/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/block/shared/st/detail/BlockSharedMemStMemberImpl.hpp"    // amalgamate: file already expanded

			// #include <cstdint>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka
			{
			    //! The generic SYCL shared memory allocator.
			    class BlockSharedMemStGenericSycl
			        : public alpaka::detail::BlockSharedMemStMemberImpl<>
			        , public concepts::Implements<ConceptBlockSharedSt, BlockSharedMemStGenericSycl>
			    {
			    public:
			        BlockSharedMemStGenericSycl(sycl::local_accessor<std::byte> accessor)
			            : BlockSharedMemStMemberImpl(
			                reinterpret_cast<std::uint8_t*>(accessor.get_pointer().get()),
			                accessor.size())
			            , m_accessor{accessor}
			        {
			        }

			    private:
			        sycl::local_accessor<std::byte> m_accessor;
			    };
			} // namespace alpaka

			namespace alpaka::trait
			{
			    template<typename T, std::size_t TUniqueId>
			    struct DeclareSharedVar<T, TUniqueId, BlockSharedMemStGenericSycl>
			    {
			        static auto declareVar(BlockSharedMemStGenericSycl const& smem) -> T&
			        {
			            auto* data = smem.template getVarPtr<T>(TUniqueId);

			            if(!data)
			            {
			                smem.template alloc<T>(TUniqueId);
			                data = smem.template getLatestVarPtr<T>();
			            }
			            ALPAKA_ASSERT(data != nullptr);
			            return *data;
			        }
			    };

			    template<>
			    struct FreeSharedVars<BlockSharedMemStGenericSycl>
			    {
			        static auto freeVars(BlockSharedMemStGenericSycl const&) -> void
			        {
			            // shared memory block data will be reused
			        }
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/block/shared/st/BlockSharedMemStGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/block/sync/BlockSyncGenericSycl.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/block/sync/Traits.hpp"    // amalgamate: file already expanded

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka
			{
			    //! The SYCL block synchronization.
			    template<typename TDim>
			    class BlockSyncGenericSycl : public concepts::Implements<ConceptBlockSync, BlockSyncGenericSycl<TDim>>
			    {
			    public:
			        using BlockSyncBase = BlockSyncGenericSycl<TDim>;

			        BlockSyncGenericSycl(sycl::nd_item<TDim::value> work_item) : my_item{work_item}
			        {
			        }

			        sycl::nd_item<TDim::value> my_item;
			    };
			} // namespace alpaka

			namespace alpaka::trait
			{
			    template<typename TDim>
			    struct SyncBlockThreads<BlockSyncGenericSycl<TDim>>
			    {
			        static auto syncBlockThreads(BlockSyncGenericSycl<TDim> const& blockSync) -> void
			        {
			            blockSync.my_item.barrier();
			        }
			    };

			    template<typename TDim>
			    struct SyncBlockThreadsPredicate<BlockCount, BlockSyncGenericSycl<TDim>>
			    {
			        static auto syncBlockThreadsPredicate(BlockSyncGenericSycl<TDim> const& blockSync, int predicate) -> int
			        {
			            auto const group = blockSync.my_item.get_group();
			            blockSync.my_item.barrier();

			            auto const counter = (predicate != 0) ? 1 : 0;
			            return sycl::reduce_over_group(group, counter, sycl::plus<>{});
			        }
			    };

			    template<typename TDim>
			    struct SyncBlockThreadsPredicate<BlockAnd, BlockSyncGenericSycl<TDim>>
			    {
			        static auto syncBlockThreadsPredicate(BlockSyncGenericSycl<TDim> const& blockSync, int predicate) -> int
			        {
			            auto const group = blockSync.my_item.get_group();
			            blockSync.my_item.barrier();

			            return static_cast<int>(sycl::all_of_group(group, static_cast<bool>(predicate)));
			        }
			    };

			    template<typename TDim>
			    struct SyncBlockThreadsPredicate<BlockOr, BlockSyncGenericSycl<TDim>>
			    {
			        static auto syncBlockThreadsPredicate(BlockSyncGenericSycl<TDim> const& blockSync, int predicate) -> int
			        {
			            auto const group = blockSync.my_item.get_group();
			            blockSync.my_item.barrier();

			            return static_cast<int>(sycl::any_of_group(group, static_cast<bool>(predicate)));
			        }
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/block/sync/BlockSyncGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/idx/bt/IdxBtGenericSycl.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/core/Sycl.hpp ==
				// ==
				/* Copyright 2022 Jan Stephan
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/elem/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/meta/IntegerSequence.hpp"    // amalgamate: file already expanded
				// #include "alpaka/offset/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

				// #include <array>    // amalgamate: file already included
				// #include <cstddef>    // amalgamate: file already included
				// #include <iostream>    // amalgamate: file already included
				// #include <stdexcept>    // amalgamate: file already included
				// #include <string>    // amalgamate: file already included
				// #include <type_traits>    // amalgamate: file already included
				// #include <utility>    // amalgamate: file already included

				#ifdef ALPAKA_ACC_SYCL_ENABLED

				// #    include <CL/sycl.hpp>    // amalgamate: file already included

				// SYCL vector types trait specializations.
				namespace alpaka
				{
				    namespace detail
				    {
				        // Remove std::is_same boilerplate
				        template<typename T, typename... Ts>
				        struct is_any : std::bool_constant<(std::is_same_v<T, Ts> || ...)>
				        {
				        };
				    } // namespace detail

				    //! In contrast to CUDA SYCL doesn't know 1D vectors. It does
				    //! support OpenCL's data types which have additional requirements
				    //! on top of those in the C++ standard. Note that SYCL's equivalent
				    //! to CUDA's dim3 type is a different class type and thus not used
				    //! here.
				    template<typename T>
				    struct IsSyclBuiltInType
				        : detail::is_any<
				              T,
				              // built-in scalar types - these are the standard C++ built-in types, std::size_t, std::byte and
				              // sycl::half
				              sycl::half,

				              // 2 component vector types
				              sycl::char2,
				              sycl::schar2,
				              sycl::uchar2,
				              sycl::short2,
				              sycl::ushort2,
				              sycl::int2,
				              sycl::uint2,
				              sycl::long2,
				              sycl::ulong2,
				              sycl::longlong2,
				              sycl::ulonglong2,
				              sycl::float2,
				              sycl::double2,
				              sycl::half2,

				              // 3 component vector types
				              sycl::char3,
				              sycl::schar3,
				              sycl::uchar3,
				              sycl::short3,
				              sycl::ushort3,
				              sycl::int3,
				              sycl::uint3,
				              sycl::long3,
				              sycl::ulong3,
				              sycl::longlong3,
				              sycl::ulonglong3,
				              sycl::float3,
				              sycl::double3,
				              sycl::half3,

				              // 4 component vector types
				              sycl::char4,
				              sycl::schar4,
				              sycl::uchar4,
				              sycl::short4,
				              sycl::ushort4,
				              sycl::int4,
				              sycl::uint4,
				              sycl::long4,
				              sycl::ulong4,
				              sycl::longlong4,
				              sycl::ulonglong4,
				              sycl::float4,
				              sycl::double4,
				              sycl::half4,

				              // 8 component vector types
				              sycl::char8,
				              sycl::schar8,
				              sycl::uchar8,
				              sycl::short8,
				              sycl::ushort8,
				              sycl::int8,
				              sycl::uint8,
				              sycl::long8,
				              sycl::ulong8,
				              sycl::longlong8,
				              sycl::ulonglong8,
				              sycl::float8,
				              sycl::double8,
				              sycl::half8,

				              // 16 component vector types
				              sycl::char16,
				              sycl::schar16,
				              sycl::uchar16,
				              sycl::short16,
				              sycl::ushort16,
				              sycl::int16,
				              sycl::uint16,
				              sycl::long16,
				              sycl::ulong16,
				              sycl::longlong16,
				              sycl::ulonglong16,
				              sycl::float16,
				              sycl::double16,
				              sycl::half16>
				    {
				    };
				} // namespace alpaka

				namespace alpaka::trait
				{
				    //! SYCL's types get trait specialization.
				    template<typename T>
				    struct DimType<T, std::enable_if_t<IsSyclBuiltInType<T>::value>>
				    {
				        using type = std::conditional_t<std::is_scalar_v<T>, DimInt<std::size_t{1}>, DimInt<T::size()>>;
				    };

				    //! The SYCL vectors' elem type trait specialization.
				    template<typename T>
				    struct ElemType<T, std::enable_if_t<IsSyclBuiltInType<T>::value>>
				    {
				        using type = std::conditional_t<std::is_scalar_v<T>, T, typename T::element_type>;
				    };
				} // namespace alpaka::trait

				namespace alpaka::trait
				{
				    //! The SYCL vectors' extent get trait specialization.
				    template<typename TExtent>
				    struct GetExtent<DimInt<Dim<TExtent>::value>, TExtent, std::enable_if_t<IsSyclBuiltInType<TExtent>::value>>
				    {
				        static auto getExtent(TExtent const& extent)
				        {
				            if constexpr(std::is_scalar_v<TExtent>)
				                return extent;
				            else
				            {
				                // Creates a SYCL vector with one element from a multidimensional vector. The element is a reference
				                // to the requested dimension's vector element. Then return the element's value.
				                return extent.template swizzle<DimInt<Dim<TExtent>::value>::value>();
				            }
				        }
				    };

				    //! The SYCL vectors' extent set trait specialization.
				    template<typename TExtent, typename TExtentVal>
				    struct SetExtent<
				        DimInt<Dim<TExtent>::value>,
				        TExtent,
				        TExtentVal,
				        std::enable_if_t<IsSyclBuiltInType<TExtent>::value>>
				    {
				        static auto setExtent(TExtent const& extent, TExtentVal const& extentVal)
				        {
				            if constexpr(std::is_scalar_v<TExtent>)
				                extent = extentVal;
				            else
				            {
				                // Creates a SYCL vector with one element from a multidimensional vector. The element is a reference
				                // to the requested dimension's vector element. Then set the element's value.
				                extent.template swizzle<DimInt<Dim<TExtent>::value>::value>() = extentVal;
				            }
				        }
				    };

				    //! The SYCL vectors' offset get trait specialization.
				    template<typename TOffsets>
				    struct GetOffset<DimInt<Dim<TOffsets>::value>, TOffsets, std::enable_if_t<IsSyclBuiltInType<TOffsets>::value>>
				    {
				        static auto getOffset(TOffsets const& offsets)
				        {
				            if constexpr(std::is_scalar_v<TOffsets>)
				                return offsets;
				            else
				            {
				                // Creates a SYCL vector with one element from a multidimensional vector. The element is a reference
				                // to the requested dimension's vector element. Then return the element's value.
				                return offsets.template swizzle<DimInt<Dim<TOffsets>::value>::value>();
				            }
				        }
				    };

				    //! The SYCL vectors' offset set trait specialization.
				    template<typename TOffsets, typename TOffset>
				    struct SetOffset<
				        DimInt<Dim<TOffsets>::value>,
				        TOffsets,
				        TOffset,
				        std::enable_if_t<IsSyclBuiltInType<TOffsets>::value>>
				    {
				        static auto setOffset(TOffsets const& offsets, TOffset const& offset)
				        {
				            if constexpr(std::is_scalar_v<TOffsets>)
				                offsets = offset;
				            else
				            {
				                // Creates a SYCL vector with one element from a multidimensional vector. The element is a reference
				                // to the requested dimension's vector element. Then set the element's value.
				                offsets.template swizzle<DimInt<Dim<TOffsets>::value>::value>() = offset;
				            }
				        }
				    };

				    //! The SYCL vectors' idx type trait specialization.
				    template<typename TIdx>
				    struct IdxType<TIdx, std::enable_if_t<IsSyclBuiltInType<TIdx>::value>>
				    {
				        using type = std::size_t;
				    };
				} // namespace alpaka::trait

				#endif
				// ==
				// == ./include/alpaka/core/Sycl.hpp ==
				// ============================================================================

			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka::bt
			{
			    //! The SYCL accelerator ND index provider.
			    template<typename TDim, typename TIdx>
			    class IdxBtGenericSycl : public concepts::Implements<ConceptIdxBt, IdxBtGenericSycl<TDim, TIdx>>
			    {
			    public:
			        using IdxBtBase = IdxBtGenericSycl;

			        explicit IdxBtGenericSycl(sycl::nd_item<TDim::value> work_item) : my_item{work_item}
			        {
			        }

			        sycl::nd_item<TDim::value> my_item;
			    };
			} // namespace alpaka::bt

			namespace alpaka::trait
			{
			    //! The SYCL accelerator index dimension get trait specialization.
			    template<typename TDim, typename TIdx>
			    struct DimType<bt::IdxBtGenericSycl<TDim, TIdx>>
			    {
			        using type = TDim;
			    };

			    //! The SYCL accelerator block thread index get trait specialization.
			    template<typename TDim, typename TIdx>
			    struct GetIdx<bt::IdxBtGenericSycl<TDim, TIdx>, origin::Block, unit::Threads>
			    {
			        //! \return The index of the current thread in the block.
			        template<typename TWorkDiv>
			        static auto getIdx(bt::IdxBtGenericSycl<TDim, TIdx> const& idx, TWorkDiv const&) -> Vec<TDim, TIdx>
			        {
			            if constexpr(TDim::value == 1)
			                return Vec<TDim, TIdx>{static_cast<TIdx>(idx.my_item.get_local_id(0))};
			            else if constexpr(TDim::value == 2)
			            {
			                return Vec<TDim, TIdx>{
			                    static_cast<TIdx>(idx.my_item.get_local_id(1)),
			                    static_cast<TIdx>(idx.my_item.get_local_id(0))};
			            }
			            else
			            {
			                return Vec<TDim, TIdx>{
			                    static_cast<TIdx>(idx.my_item.get_local_id(2)),
			                    static_cast<TIdx>(idx.my_item.get_local_id(1)),
			                    static_cast<TIdx>(idx.my_item.get_local_id(0))};
			            }
			        }
			    };

			    //! The SYCL accelerator block thread index idx type trait specialization.
			    template<typename TDim, typename TIdx>
			    struct IdxType<bt::IdxBtGenericSycl<TDim, TIdx>>
			    {
			        using type = TIdx;
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/idx/bt/IdxBtGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/idx/gb/IdxGbGenericSycl.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka::gb
			{
			    //! The SYCL accelerator ND index provider.
			    template<typename TDim, typename TIdx>
			    class IdxGbGenericSycl : public concepts::Implements<ConceptIdxGb, IdxGbGenericSycl<TDim, TIdx>>
			    {
			    public:
			        using IdxGbBase = IdxGbGenericSycl;

			        explicit IdxGbGenericSycl(sycl::nd_item<TDim::value> work_item) : my_item{work_item}
			        {
			        }

			        sycl::nd_item<TDim::value> my_item;
			    };
			} // namespace alpaka::gb

			namespace alpaka::trait
			{
			    //! The SYCL accelerator index dimension get trait specialization.
			    template<typename TDim, typename TIdx>
			    struct DimType<gb::IdxGbGenericSycl<TDim, TIdx>>
			    {
			        using type = TDim;
			    };

			    //! The SYCL accelerator grid block index get trait specialization.
			    template<typename TDim, typename TIdx>
			    struct GetIdx<gb::IdxGbGenericSycl<TDim, TIdx>, origin::Grid, unit::Blocks>
			    {
			        //! \return The index of the current block in the grid.
			        template<typename TWorkDiv>
			        static auto getIdx(gb::IdxGbGenericSycl<TDim, TIdx> const& idx, TWorkDiv const&)
			        {
			            if constexpr(TDim::value == 1)
			                return Vec<TDim, TIdx>(static_cast<TIdx>(idx.my_item.get_group(0)));
			            else if constexpr(TDim::value == 2)
			            {
			                return Vec<TDim, TIdx>(
			                    static_cast<TIdx>(idx.my_item.get_group(1)),
			                    static_cast<TIdx>(idx.my_item.get_group(0)));
			            }
			            else
			            {
			                return Vec<TDim, TIdx>(
			                    static_cast<TIdx>(idx.my_item.get_group(2)),
			                    static_cast<TIdx>(idx.my_item.get_group(1)),
			                    static_cast<TIdx>(idx.my_item.get_group(0)));
			            }
			        }
			    };

			    //! The SYCL accelerator grid block index idx type trait specialization.
			    template<typename TDim, typename TIdx>
			    struct IdxType<gb::IdxGbGenericSycl<TDim, TIdx>>
			    {
			        using type = TIdx;
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/idx/gb/IdxGbGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/intrinsic/IntrinsicGenericSycl.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/intrinsic/IntrinsicFallback.hpp"    // amalgamate: file already expanded
			// #include "alpaka/intrinsic/Traits.hpp"    // amalgamate: file already expanded

			// #include <cstdint>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka
			{
			    //! The SYCL intrinsic.
			    class IntrinsicGenericSycl : public concepts::Implements<ConceptIntrinsic, IntrinsicGenericSycl>
			    {
			    };
			} // namespace alpaka

			namespace alpaka::trait
			{
			    template<>
			    struct Popcount<IntrinsicGenericSycl>
			    {
			        static auto popcount(IntrinsicGenericSycl const&, std::uint32_t value) -> std::int32_t
			        {
			            return static_cast<std::int32_t>(sycl::popcount(value));
			        }

			        static auto popcount(IntrinsicGenericSycl const&, std::uint64_t value) -> std::int32_t
			        {
			            return static_cast<std::int32_t>(sycl::popcount(value));
			        }
			    };

			    template<>
			    struct Ffs<IntrinsicGenericSycl>
			    {
			        static auto ffs(IntrinsicGenericSycl const&, std::int32_t value) -> std::int32_t
			        {
			            // There is no FFS operation in SYCL but we can emulate it using popcount.
			            return (value == 0) ? 0 : sycl::popcount(value ^ ~(-value));
			        }

			        static auto ffs(IntrinsicGenericSycl const&, std::int64_t value) -> std::int32_t
			        {
			            // There is no FFS operation in SYCL but we can emulate it using popcount.
			            return (value == 0l) ? 0 : static_cast<std::int32_t>(sycl::popcount(value ^ ~(-value)));
			        }
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/intrinsic/IntrinsicGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/math/MathGenericSycl.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/math/Complex.hpp ==
				// ==
				/* Copyright 2022 Sergei Bastrakov
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/math/FloatEqualExact.hpp ==
					// ==
					/* Copyright 2021 Jiri Vyskocil
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

					// #include <type_traits>    // amalgamate: file already included

					namespace alpaka
					{
					    namespace math
					    {
					        /** Compare two floating point numbers for exact equivalence. Use only when necessary, and be aware of the
					         * implications. Most codes should not use this function and instead implement a correct epsilon-based
					         * comparison. If you are unfamiliar with the topic, check out
					         * https://www.geeksforgeeks.org/problem-in-comparing-floating-point-numbers-and-how-to-compare-them-correctly/
					         * or Goldberg 1991: "What every computer scientist should know about floating-point arithmetic",
					         * https://dl.acm.org/doi/10.1145/103162.103163
					         *
					         * This function calls the == operator for floating point types, but disables the warning issued by the
					         * compiler when compiling with the float equality warning checks enabled. This warning is valid an valuable in
					         * most codes and should be generally enabled, but there are specific instances where a piece of code might
					         * need to do an exact comparison (e.g. @a CudaVectorArrayWrapperTest.cpp). The verbose name for the function
					         * is intentional as it should raise a red flag if used while not absolutely needed. Users are advised to add a
					         * justification whenever they use this function.
					         *
					         * @tparam T both operands have to be the same type and conform to std::is_floating_point
					         * @param a first operand
					         * @param b second operand
					         * @return a == b
					         */
					        template<typename T>
					        ALPAKA_FN_INLINE ALPAKA_FN_HOST_ACC auto floatEqualExactNoWarning(T a, T b) -> bool
					        {
					            static_assert(std::is_floating_point_v<T>, "floatEqualExactNoWarning is for floating point values only!");

					            // So far only GCC and Clang check for float comparison and both accept the GCC pragmas.
					#ifdef __GNUC__
					#    pragma GCC diagnostic push
					#    pragma GCC diagnostic ignored "-Wfloat-equal"
					#endif
					            return a == b;
					#ifdef __GNUC__
					#    pragma GCC diagnostic pop
					#endif
					        }
					    } // namespace math
					} // namespace alpaka
					// ==
					// == ./include/alpaka/math/FloatEqualExact.hpp ==
					// ============================================================================


				// #include <cmath>    // amalgamate: file already included
				// #include <complex>    // amalgamate: file already included
				// #include <iostream>    // amalgamate: file already included
				// #include <type_traits>    // amalgamate: file already included

				namespace alpaka
				{
				    //! Implementation of a complex number useable on host and device.
				    //!
				    //! It follows the layout of std::complex and so array-oriented access.
				    //! The class template implements all methods and operators as std::complex<T>.
				    //! Additionally, it provides an implicit conversion to and from std::complex<T>.
				    //! All methods besides operators << and >> are host-device.
				    //! It does not provide non-member functions of std::complex besides the operators.
				    //! Those are provided the same way as alpaka math functions for real numbers.
				    //!
				    //! Note that unlike most of alpaka, this is a concrete type template, and not merely a concept.
				    //!
				    //! Naming and order of the methods match https://en.cppreference.com/w/cpp/numeric/complex in C++17.
				    //! Implementation chose to not extend it e.g. by adding constexpr to some places that would get it in C++20.
				    //! The motivation is that with internal conversion to std::complex<T> for CPU backends, it would define the common
				    //! interface for genetic code anyways.
				    //! So it is more clear to have alpaka's interface exactly matching when possible, and not "improving".
				    //!
				    //! @tparam T type of the real and imaginary part: float, double, or long double.
				    template<typename T>
				    class Complex
				    {
				    public:
				        // Make sure the input type is floating-point
				        static_assert(std::is_floating_point_v<T>);

				        //! Type of the real and imaginary parts
				        using value_type = T;

				        //! Constructor from the given real and imaginary parts
				        constexpr ALPAKA_FN_HOST_ACC Complex(T const& real = T{}, T const& imag = T{}) : m_real(real), m_imag(imag)
				        {
				        }

				        //! Copy constructor
				        constexpr Complex(Complex const& other) = default;

				        //! Constructor from Complex of another type
				        template<typename U>
				        constexpr ALPAKA_FN_HOST_ACC Complex(Complex<U> const& other)
				            : m_real(static_cast<T>(other.real()))
				            , m_imag(static_cast<T>(other.imag()))
				        {
				        }

				        //! Constructor from std::complex
				        constexpr ALPAKA_FN_HOST_ACC Complex(std::complex<T> const& other) : m_real(other.real()), m_imag(other.imag())
				        {
				        }

				        //! Conversion to std::complex
				        constexpr ALPAKA_FN_HOST_ACC operator std::complex<T>() const
				        {
				            return std::complex<T>{m_real, m_imag};
				        }

				        //! Assignment
				        Complex& operator=(Complex const&) = default;

				        //! Get the real part
				        constexpr ALPAKA_FN_HOST_ACC T real() const
				        {
				            return m_real;
				        }

				        //! Set the real part
				        constexpr ALPAKA_FN_HOST_ACC void real(T value)
				        {
				            m_real = value;
				        }

				        //! Get the imaginary part
				        constexpr ALPAKA_FN_HOST_ACC T imag() const
				        {
				            return m_imag;
				        }

				        //! Set the imaginary part
				        constexpr ALPAKA_FN_HOST_ACC void imag(T value)
				        {
				            m_imag = value;
				        }

				        //! Addition assignment with a real number
				        ALPAKA_FN_HOST_ACC Complex& operator+=(T const& other)
				        {
				            m_real += other;
				            return *this;
				        }

				        //! Addition assignment with a complex number
				        template<typename U>
				        ALPAKA_FN_HOST_ACC Complex& operator+=(Complex<U> const& other)
				        {
				            m_real += static_cast<T>(other.real());
				            m_imag += static_cast<T>(other.imag());
				            return *this;
				        }

				        //! Subtraction assignment with a real number
				        ALPAKA_FN_HOST_ACC Complex& operator-=(T const& other)
				        {
				            m_real -= other;
				            return *this;
				        }

				        //! Subtraction assignment with a complex number
				        template<typename U>
				        ALPAKA_FN_HOST_ACC Complex& operator-=(Complex<U> const& other)
				        {
				            m_real -= static_cast<T>(other.real());
				            m_imag -= static_cast<T>(other.imag());
				            return *this;
				        }

				        //! Multiplication assignment with a real number
				        ALPAKA_FN_HOST_ACC Complex& operator*=(T const& other)
				        {
				            m_real *= other;
				            m_imag *= other;
				            return *this;
				        }

				        //! Multiplication assignment with a complex number
				        template<typename U>
				        ALPAKA_FN_HOST_ACC Complex& operator*=(Complex<U> const& other)
				        {
				            auto const newReal = m_real * static_cast<T>(other.real()) - m_imag * static_cast<T>(other.imag());
				            auto const newImag = m_imag * static_cast<T>(other.real()) + m_real * static_cast<T>(other.imag());
				            m_real = newReal;
				            m_imag = newImag;
				            return *this;
				        }

				        //! Division assignment with a real number
				        ALPAKA_FN_HOST_ACC Complex& operator/=(T const& other)
				        {
				            m_real /= other;
				            m_imag /= other;
				            return *this;
				        }

				        //! Division assignment with a complex number
				        template<typename U>
				        ALPAKA_FN_HOST_ACC Complex& operator/=(Complex<U> const& other)
				        {
				            return *this *= Complex{
				                       static_cast<T>(other.real() / (other.real() * other.real() + other.imag() * other.imag())),
				                       static_cast<T>(-other.imag() / (other.real() * other.real() + other.imag() * other.imag()))};
				        }

				    private:
				        //! Real and imaginary parts, storage enables array-oriented access
				        T m_real, m_imag;
				    };

				    //! Host-device arithmetic operations matching std::complex<T>.
				    //!
				    //! They take and return alpaka::Complex.
				    //!
				    //! @{
				    //!

				    //! Unary plus (added for compatibility with std::complex)
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator+(Complex<T> const& val)
				    {
				        return val;
				    }

				    //! Unary minus
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator-(Complex<T> const& val)
				    {
				        return Complex<T>{-val.real(), -val.imag()};
				    }

				    //! Addition of two complex numbers
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator+(Complex<T> const& lhs, Complex<T> const& rhs)
				    {
				        return Complex<T>{lhs.real() + rhs.real(), lhs.imag() + rhs.imag()};
				    }

				    //! Addition of a complex and a real number
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator+(Complex<T> const& lhs, T const& rhs)
				    {
				        return Complex<T>{lhs.real() + rhs, lhs.imag()};
				    }

				    //! Addition of a real and a complex number
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator+(T const& lhs, Complex<T> const& rhs)
				    {
				        return Complex<T>{lhs + rhs.real(), rhs.imag()};
				    }

				    //! Subtraction of two complex numbers
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator-(Complex<T> const& lhs, Complex<T> const& rhs)
				    {
				        return Complex<T>{lhs.real() - rhs.real(), lhs.imag() - rhs.imag()};
				    }

				    //! Subtraction of a complex and a real number
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator-(Complex<T> const& lhs, T const& rhs)
				    {
				        return Complex<T>{lhs.real() - rhs, lhs.imag()};
				    }

				    //! Subtraction of a real and a complex number
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator-(T const& lhs, Complex<T> const& rhs)
				    {
				        return Complex<T>{lhs - rhs.real(), -rhs.imag()};
				    }

				    //! Muptiplication of two complex numbers
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator*(Complex<T> const& lhs, Complex<T> const& rhs)
				    {
				        return Complex<T>{
				            lhs.real() * rhs.real() - lhs.imag() * rhs.imag(),
				            lhs.imag() * rhs.real() + lhs.real() * rhs.imag()};
				    }

				    //! Muptiplication of a complex and a real number
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator*(Complex<T> const& lhs, T const& rhs)
				    {
				        return Complex<T>{lhs.real() * rhs, lhs.imag() * rhs};
				    }

				    //! Muptiplication of a real and a complex number
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator*(T const& lhs, Complex<T> const& rhs)
				    {
				        return Complex<T>{lhs * rhs.real(), lhs * rhs.imag()};
				    }

				    //! Division of two complex numbers
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator/(Complex<T> const& lhs, Complex<T> const& rhs)
				    {
				        return Complex<T>{
				            (lhs.real() * rhs.real() + lhs.imag() * rhs.imag()) / (rhs.real() * rhs.real() + rhs.imag() * rhs.imag()),
				            (lhs.imag() * rhs.real() - lhs.real() * rhs.imag()) / (rhs.real() * rhs.real() + rhs.imag() * rhs.imag())};
				    }

				    //! Division of complex and a real number
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator/(Complex<T> const& lhs, T const& rhs)
				    {
				        return Complex<T>{lhs.real() / rhs, lhs.imag() / rhs};
				    }

				    //! Division of a real and a complex number
				    template<typename T>
				    ALPAKA_FN_HOST_ACC Complex<T> operator/(T const& lhs, Complex<T> const& rhs)
				    {
				        return Complex<T>{
				            lhs * rhs.real() / (rhs.real() * rhs.real() + rhs.imag() * rhs.imag()),
				            -lhs * rhs.imag() / (rhs.real() * rhs.real() + rhs.imag() * rhs.imag())};
				    }

				    //! Equality of two complex numbers
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC bool operator==(Complex<T> const& lhs, Complex<T> const& rhs)
				    {
				        return math::floatEqualExactNoWarning(lhs.real(), rhs.real())
				               && math::floatEqualExactNoWarning(lhs.imag(), rhs.imag());
				    }

				    //! Equality of a complex and a real number
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC bool operator==(Complex<T> const& lhs, T const& rhs)
				    {
				        return math::floatEqualExactNoWarning(lhs.real(), rhs)
				               && math::floatEqualExactNoWarning(lhs.imag(), static_cast<T>(0));
				    }

				    //! Equality of a real and a complex number
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC bool operator==(T const& lhs, Complex<T> const& rhs)
				    {
				        return math::floatEqualExactNoWarning(lhs, rhs.real())
				               && math::floatEqualExactNoWarning(static_cast<T>(0), rhs.imag());
				    }

				    //! Inequality of two complex numbers.
				    //!
				    //! @note this and other versions of operator != should be removed since C++20, as so does std::complex
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC bool operator!=(Complex<T> const& lhs, Complex<T> const& rhs)
				    {
				        return !(lhs == rhs);
				    }

				    //! Inequality of a complex and a real number
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC bool operator!=(Complex<T> const& lhs, T const& rhs)
				    {
				        return !math::floatEqualExactNoWarning(lhs.real(), rhs)
				               || !math::floatEqualExactNoWarning(lhs.imag(), static_cast<T>(0));
				    }

				    //! Inequality of a real and a complex number
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC bool operator!=(T const& lhs, Complex<T> const& rhs)
				    {
				        return !math::floatEqualExactNoWarning(lhs, rhs.real())
				               || !math::floatEqualExactNoWarning(static_cast<T>(0), rhs.imag());
				    }

				    //! @}

				    //! Host-only output of a complex number
				    template<typename T, typename TChar, typename TTraits>
				    std::basic_ostream<TChar, TTraits>& operator<<(std::basic_ostream<TChar, TTraits>& os, Complex<T> const& x)
				    {
				        os << x.operator std::complex<T>();
				        return os;
				    }

				    //! Host-only input of a complex number
				    template<typename T, typename TChar, typename TTraits>
				    std::basic_istream<TChar, TTraits>& operator>>(std::basic_istream<TChar, TTraits>& is, Complex<T> const& x)
				    {
				        std::complex<T> z;
				        is >> z;
				        x = z;
				        return is;
				    }

				    //! Host-only math functions matching std::complex<T>.
				    //!
				    //! Due to issue #1688, these functions are technically marked host-device and suppress related warnings.
				    //! However, they must be called for host only.
				    //!
				    //! They take and return alpaka::Complex (or a real number when appropriate).
				    //! Internally cast, fall back to std::complex implementation and cast back.
				    //! These functions can be used directly on the host side.
				    //! They are also picked up by ADL in math traits for CPU backends.
				    //!
				    //! On the device side, alpaka math traits must be used instead.
				    //! Note that the set of the traits is currently a bit smaller.
				    //!
				    //! @{
				    //!

				    //! Absolute value
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC T abs(Complex<T> const& x)
				    {
				        return std::abs(std::complex<T>(x));
				    }

				    //! Arc cosine
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> acos(Complex<T> const& x)
				    {
				        return std::acos(std::complex<T>(x));
				    }

				    //! Arc hyperbolic cosine
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> acosh(Complex<T> const& x)
				    {
				        return std::acosh(std::complex<T>(x));
				    }

				    //! Argument
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC T arg(Complex<T> const& x)
				    {
				        return std::arg(std::complex<T>(x));
				    }

				    //! Arc sine
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> asin(Complex<T> const& x)
				    {
				        return std::asin(std::complex<T>(x));
				    }

				    //! Arc hyperbolic sine
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> asinh(Complex<T> const& x)
				    {
				        return std::asinh(std::complex<T>(x));
				    }

				    //! Arc tangent
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> atan(Complex<T> const& x)
				    {
				        return std::atan(std::complex<T>(x));
				    }

				    //! Arc hyperbolic tangent
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> atanh(Complex<T> const& x)
				    {
				        return std::atanh(std::complex<T>(x));
				    }

				    //! Complex conjugate
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> conj(Complex<T> const& x)
				    {
				        return std::conj(std::complex<T>(x));
				    }

				    //! Cosine
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> cos(Complex<T> const& x)
				    {
				        return std::cos(std::complex<T>(x));
				    }

				    //! Hyperbolic cosine
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> cosh(Complex<T> const& x)
				    {
				        return std::cosh(std::complex<T>(x));
				    }

				    //! Exponential
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> exp(Complex<T> const& x)
				    {
				        return std::exp(std::complex<T>(x));
				    }

				    //! Natural logarithm
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> log(Complex<T> const& x)
				    {
				        return std::log(std::complex<T>(x));
				    }

				    //! Base 10 logarithm
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> log10(Complex<T> const& x)
				    {
				        return std::log10(std::complex<T>(x));
				    }

				    //! Squared magnitude
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC T norm(Complex<T> const& x)
				    {
				        return std::norm(std::complex<T>(x));
				    }

				    //! Get a complex number with given magnitude and phase angle
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> polar(T const& r, T const& theta = T())
				    {
				        return std::polar(r, theta);
				    }

				    //! Complex power of a complex number
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T, typename U>
				    constexpr ALPAKA_FN_HOST_ACC auto pow(Complex<T> const& x, Complex<U> const& y)
				    {
				        // Use same type promotion as std::pow
				        auto const result = std::pow(std::complex<T>(x), std::complex<U>(y));
				        using ValueType = typename decltype(result)::value_type;
				        return Complex<ValueType>(result);
				    }

				    //! Real power of a complex number
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T, typename U>
				    constexpr ALPAKA_FN_HOST_ACC auto pow(Complex<T> const& x, U const& y)
				    {
				        return pow(x, Complex<U>(y));
				    }

				    //! Complex power of a real number
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T, typename U>
				    constexpr ALPAKA_FN_HOST_ACC auto pow(T const& x, Complex<U> const& y)
				    {
				        return pow(Complex<T>(x), y);
				    }

				    //! Projection onto the Riemann sphere
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> proj(Complex<T> const& x)
				    {
				        return std::proj(std::complex<T>(x));
				    }

				    //! Sine
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> sin(Complex<T> const& x)
				    {
				        return std::sin(std::complex<T>(x));
				    }

				    //! Hyperbolic sine
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> sinh(Complex<T> const& x)
				    {
				        return std::sinh(std::complex<T>(x));
				    }

				    //! Square root
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> sqrt(Complex<T> const& x)
				    {
				        return std::sqrt(std::complex<T>(x));
				    }

				    //! Tangent
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> tan(Complex<T> const& x)
				    {
				        return std::tan(std::complex<T>(x));
				    }

				    //! Hyperbolic tangent
				    ALPAKA_NO_HOST_ACC_WARNING
				    template<typename T>
				    constexpr ALPAKA_FN_HOST_ACC Complex<T> tanh(Complex<T> const& x)
				    {
				        return std::tanh(std::complex<T>(x));
				    }

				    //! @}

				} // namespace alpaka
				// ==
				// == ./include/alpaka/math/Complex.hpp ==
				// ============================================================================

			// #include "alpaka/math/Traits.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			//! The mathematical operation specifics.
			namespace alpaka::math
			{
			    //! The SYCL abs.
			    class AbsGenericSycl : public concepts::Implements<alpaka::math::ConceptMathAbs, AbsGenericSycl>
			    {
			    };

			    //! The SYCL acos.
			    class AcosGenericSycl : public concepts::Implements<alpaka::math::ConceptMathAcos, AcosGenericSycl>
			    {
			    };

			    //! The SYCL acosh.
			    class AcoshGenericSycl : public concepts::Implements<alpaka::math::ConceptMathAcosh, AcoshGenericSycl>
			    {
			    };

			    //! The SYCL arg.
			    class ArgGenericSycl : public concepts::Implements<alpaka::math::ConceptMathArg, ArgGenericSycl>
			    {
			    };

			    //! The SYCL asin.
			    class AsinGenericSycl : public concepts::Implements<alpaka::math::ConceptMathAsin, AsinGenericSycl>
			    {
			    };

			    //! The SYCL asinh.
			    class AsinhGenericSycl : public concepts::Implements<alpaka::math::ConceptMathAsinh, AsinhGenericSycl>
			    {
			    };

			    //! The SYCL atan.
			    class AtanGenericSycl : public concepts::Implements<alpaka::math::ConceptMathAtan, AtanGenericSycl>
			    {
			    };

			    //! The SYCL atanh.
			    class AtanhGenericSycl : public concepts::Implements<alpaka::math::ConceptMathAtanh, AtanhGenericSycl>
			    {
			    };

			    //! The SYCL atan2.
			    class Atan2GenericSycl : public concepts::Implements<alpaka::math::ConceptMathAtan2, Atan2GenericSycl>
			    {
			    };

			    //! The SYCL cbrt.
			    class CbrtGenericSycl : public concepts::Implements<alpaka::math::ConceptMathCbrt, CbrtGenericSycl>
			    {
			    };

			    //! The SYCL ceil.
			    class CeilGenericSycl : public concepts::Implements<alpaka::math::ConceptMathCeil, CeilGenericSycl>
			    {
			    };

			    //! The SYCL conj.
			    class ConjGenericSycl : public concepts::Implements<alpaka::math::ConceptMathConj, ConjGenericSycl>
			    {
			    };

			    //! The SYCL cos.
			    class CosGenericSycl : public concepts::Implements<alpaka::math::ConceptMathCos, CosGenericSycl>
			    {
			    };

			    //! The SYCL cosh.
			    class CoshGenericSycl : public concepts::Implements<alpaka::math::ConceptMathCosh, CoshGenericSycl>
			    {
			    };

			    //! The SYCL erf.
			    class ErfGenericSycl : public concepts::Implements<alpaka::math::ConceptMathErf, ErfGenericSycl>
			    {
			    };

			    //! The SYCL exp.
			    class ExpGenericSycl : public concepts::Implements<alpaka::math::ConceptMathExp, ExpGenericSycl>
			    {
			    };

			    //! The SYCL floor.
			    class FloorGenericSycl : public concepts::Implements<alpaka::math::ConceptMathFloor, FloorGenericSycl>
			    {
			    };

			    //! The SYCL fmod.
			    class FmodGenericSycl : public concepts::Implements<alpaka::math::ConceptMathFmod, FmodGenericSycl>
			    {
			    };

			    //! The SYCL isfinite.
			    class IsfiniteGenericSycl : public concepts::Implements<alpaka::math::ConceptMathIsfinite, IsfiniteGenericSycl>
			    {
			    };

			    //! The SYCL isfinite.
			    class IsinfGenericSycl : public concepts::Implements<alpaka::math::ConceptMathIsinf, IsinfGenericSycl>
			    {
			    };

			    //! The SYCL isnan.
			    class IsnanGenericSycl : public concepts::Implements<alpaka::math::ConceptMathIsnan, IsnanGenericSycl>
			    {
			    };

			    //! The SYCL log.
			    class LogGenericSycl : public concepts::Implements<alpaka::math::ConceptMathLog, LogGenericSycl>
			    {
			    };

			    //! The SYCL max.
			    class MaxGenericSycl : public concepts::Implements<alpaka::math::ConceptMathMax, MaxGenericSycl>
			    {
			    };

			    //! The SYCL min.
			    class MinGenericSycl : public concepts::Implements<alpaka::math::ConceptMathMin, MinGenericSycl>
			    {
			    };

			    //! The SYCL pow.
			    class PowGenericSycl : public concepts::Implements<alpaka::math::ConceptMathPow, PowGenericSycl>
			    {
			    };

			    //! The SYCL remainder.
			    class RemainderGenericSycl : public concepts::Implements<alpaka::math::ConceptMathRemainder, RemainderGenericSycl>
			    {
			    };

			    //! The SYCL round.
			    class RoundGenericSycl : public concepts::Implements<alpaka::math::ConceptMathRound, RoundGenericSycl>
			    {
			    };

			    //! The SYCL rsqrt.
			    class RsqrtGenericSycl : public concepts::Implements<alpaka::math::ConceptMathRsqrt, RsqrtGenericSycl>
			    {
			    };

			    //! The SYCL sin.
			    class SinGenericSycl : public concepts::Implements<alpaka::math::ConceptMathSin, SinGenericSycl>
			    {
			    };

			    //! The SYCL sinh.
			    class SinhGenericSycl : public concepts::Implements<alpaka::math::ConceptMathSinh, SinhGenericSycl>
			    {
			    };

			    //! The SYCL sincos.
			    class SinCosGenericSycl : public concepts::Implements<alpaka::math::ConceptMathSinCos, SinCosGenericSycl>
			    {
			    };

			    //! The SYCL sqrt.
			    class SqrtGenericSycl : public concepts::Implements<alpaka::math::ConceptMathSqrt, SqrtGenericSycl>
			    {
			    };

			    //! The SYCL tan.
			    class TanGenericSycl : public concepts::Implements<alpaka::math::ConceptMathTan, TanGenericSycl>
			    {
			    };

			    //! The SYCL tanh.
			    class TanhGenericSycl : public concepts::Implements<alpaka::math::ConceptMathTanh, TanhGenericSycl>
			    {
			    };

			    //! The SYCL trunc.
			    class TruncGenericSycl : public concepts::Implements<alpaka::math::ConceptMathTrunc, TruncGenericSycl>
			    {
			    };

			    //! The SYCL math trait specializations.
			    class MathGenericSycl
			        : public AbsGenericSycl
			        , public AcosGenericSycl
			        , public AcoshGenericSycl
			        , public ArgGenericSycl
			        , public AsinGenericSycl
			        , public AsinhGenericSycl
			        , public AtanGenericSycl
			        , public AtanhGenericSycl
			        , public Atan2GenericSycl
			        , public CbrtGenericSycl
			        , public CeilGenericSycl
			        , public ConjGenericSycl
			        , public CosGenericSycl
			        , public CoshGenericSycl
			        , public ErfGenericSycl
			        , public ExpGenericSycl
			        , public FloorGenericSycl
			        , public FmodGenericSycl
			        , public IsfiniteGenericSycl
			        , public IsinfGenericSycl
			        , public IsnanGenericSycl
			        , public LogGenericSycl
			        , public MaxGenericSycl
			        , public MinGenericSycl
			        , public PowGenericSycl
			        , public RemainderGenericSycl
			        , public RoundGenericSycl
			        , public RsqrtGenericSycl
			        , public SinGenericSycl
			        , public SinhGenericSycl
			        , public SinCosGenericSycl
			        , public SqrtGenericSycl
			        , public TanGenericSycl
			        , public TanhGenericSycl
			        , public TruncGenericSycl
			    {
			    };
			} // namespace alpaka::math

			namespace alpaka::math::trait
			{
			    //! The SYCL abs trait specialization.
			    template<typename TArg>
			    struct Abs<math::AbsGenericSycl, TArg, std::enable_if_t<std::is_arithmetic_v<TArg>>>
			    {
			        auto operator()(math::AbsGenericSycl const&, TArg const& arg)
			        {
			            if constexpr(std::is_integral_v<TArg>)
			                return sycl::abs(arg);
			            else if constexpr(std::is_floating_point_v<TArg>)
			                return sycl::fabs(arg);
			            else
			                static_assert(!sizeof(TArg), "Unsupported data type");
			        }
			    };

			    //! The SYCL acos trait specialization.
			    template<typename TArg>
			    struct Acos<math::AcosGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::AcosGenericSycl const&, TArg const& arg)
			        {
			            return sycl::acos(arg);
			        }
			    };

			    //! The SYCL acosh trait specialization.
			    template<typename TArg>
			    struct Acosh<math::AcoshGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::AcoshGenericSycl const&, TArg const& arg)
			        {
			            return sycl::acosh(arg);
			        }
			    };

			    //! The SYCL arg trait specialization.
			    template<typename TArgument>
			    struct Arg<math::ArgGenericSycl, TArgument, std::enable_if_t<std::is_arithmetic_v<TArgument>>>
			    {
			        auto operator()(math::ArgGenericSycl const&, TArgument const& argument)
			        {
			            if constexpr(std::is_integral_v<TArgument>)
			                return sycl::atan2(0.0, static_cast<double>(argument));
			            else if constexpr(std::is_floating_point_v<TArgument>)
			                return sycl::atan2(TArgument{0.0}, argument);
			            else
			                static_assert(!sizeof(TArgument), "Unsupported data type");
			        }
			    };

			    //! The SYCL asin trait specialization.
			    template<typename TArg>
			    struct Asin<math::AsinGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::AsinGenericSycl const&, TArg const& arg)
			        {
			            return sycl::asin(arg);
			        }
			    };

			    //! The SYCL asinh trait specialization.
			    template<typename TArg>
			    struct Asinh<math::AsinhGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::AsinhGenericSycl const&, TArg const& arg)
			        {
			            return sycl::asinh(arg);
			        }
			    };

			    //! The SYCL atan trait specialization.
			    template<typename TArg>
			    struct Atan<math::AtanGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::AtanGenericSycl const&, TArg const& arg)
			        {
			            return sycl::atan(arg);
			        }
			    };

			    //! The SYCL atanh trait specialization.
			    template<typename TArg>
			    struct Atanh<math::AtanhGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::AtanhGenericSycl const&, TArg const& arg)
			        {
			            return sycl::atanh(arg);
			        }
			    };

			    //! The SYCL atan2 trait specialization.
			    template<typename Ty, typename Tx>
			    struct Atan2<
			        math::Atan2GenericSycl,
			        Ty,
			        Tx,
			        std::enable_if_t<std::is_floating_point_v<Ty> && std::is_floating_point_v<Tx>>>
			    {
			        auto operator()(math::Atan2GenericSycl const&, Ty const& y, Tx const& x)
			        {
			            return sycl::atan2(y, x);
			        }
			    };

			    //! The SYCL cbrt trait specialization.
			    template<typename TArg>
			    struct Cbrt<math::CbrtGenericSycl, TArg, std::enable_if_t<std::is_arithmetic_v<TArg>>>
			    {
			        auto operator()(math::CbrtGenericSycl const&, TArg const& arg)
			        {
			            if constexpr(std::is_integral_v<TArg>)
			                return sycl::cbrt(static_cast<double>(arg)); // Mirror CUDA back-end and use double for ints
			            else if constexpr(std::is_floating_point_v<TArg>)
			                return sycl::cbrt(arg);
			            else
			                static_assert(!sizeof(TArg), "Unsupported data type");
			        }
			    };

			    //! The SYCL ceil trait specialization.
			    template<typename TArg>
			    struct Ceil<math::CeilGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::CeilGenericSycl const&, TArg const& arg)
			        {
			            return sycl::ceil(arg);
			        }
			    };

			    //! The SYCL conj trait specialization.
			    template<typename TArg>
			    struct Conj<math::ConjGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::ConjGenericSycl const&, TArg const& arg)
			        {
			            return Complex<TArg>{arg, TArg{0.0}};
			        }
			    };

			    //! The SYCL cos trait specialization.
			    template<typename TArg>
			    struct Cos<math::CosGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::CosGenericSycl const&, TArg const& arg)
			        {
			            return sycl::cos(arg);
			        }
			    };

			    //! The SYCL cos trait specialization.
			    template<typename TArg>
			    struct Cosh<math::CoshGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::CoshGenericSycl const&, TArg const& arg)
			        {
			            return sycl::cosh(arg);
			        }
			    };

			    //! The SYCL erf trait specialization.
			    template<typename TArg>
			    struct Erf<math::ErfGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::ErfGenericSycl const&, TArg const& arg)
			        {
			            return sycl::erf(arg);
			        }
			    };

			    //! The SYCL exp trait specialization.
			    template<typename TArg>
			    struct Exp<math::ExpGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::ExpGenericSycl const&, TArg const& arg)
			        {
			            return sycl::exp(arg);
			        }
			    };

			    //! The SYCL floor trait specialization.
			    template<typename TArg>
			    struct Floor<math::FloorGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::FloorGenericSycl const&, TArg const& arg)
			        {
			            return sycl::floor(arg);
			        }
			    };

			    //! The SYCL fmod trait specialization.
			    template<typename Tx, typename Ty>
			    struct Fmod<
			        math::FmodGenericSycl,
			        Tx,
			        Ty,
			        std::enable_if_t<std::is_floating_point_v<Tx> && std::is_floating_point_v<Ty>>>
			    {
			        auto operator()(math::FmodGenericSycl const&, Tx const& x, Ty const& y)
			        {
			            return sycl::fmod(x, y);
			        }
			    };

			    //! The SYCL isfinite trait specialization.
			    template<typename TArg>
			    struct Isfinite<math::IsfiniteGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::IsfiniteGenericSycl const&, TArg const& arg)
			        {
			            return sycl::isfinite(arg);
			        }
			    };

			    //! The SYCL isinf trait specialization.
			    template<typename TArg>
			    struct Isinf<math::IsinfGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::IsinfGenericSycl const&, TArg const& arg)
			        {
			            return sycl::isinf(arg);
			        }
			    };

			    //! The SYCL isnan trait specialization.
			    template<typename TArg>
			    struct Isnan<math::IsnanGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::IsnanGenericSycl const&, TArg const& arg)
			        {
			            return sycl::isnan(arg);
			        }
			    };

			    //! The SYCL log trait specialization.
			    template<typename TArg>
			    struct Log<math::LogGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::LogGenericSycl const&, TArg const& arg)
			        {
			            return sycl::log(arg);
			        }
			    };

			    //! The SYCL max trait specialization.
			    template<typename Tx, typename Ty>
			    struct Max<math::MaxGenericSycl, Tx, Ty, std::enable_if_t<std::is_arithmetic_v<Tx> && std::is_arithmetic_v<Ty>>>
			    {
			        auto operator()(math::MaxGenericSycl const&, Tx const& x, Ty const& y)
			        {
			            if constexpr(std::is_integral_v<Tx> && std::is_integral_v<Ty>)
			                return sycl::max(x, y);
			            else if constexpr(std::is_floating_point_v<Tx> && std::is_floating_point_v<Ty>)
			                return sycl::fmax(x, y);
			            else if constexpr(
			                (std::is_floating_point_v<Tx> && std::is_integral_v<Ty>)
			                || (std::is_integral_v<Tx> && std::is_floating_point_v<Ty>) )
			                return sycl::fmax(static_cast<double>(x), static_cast<double>(y)); // mirror CUDA back-end
			            else
			                static_assert(!sizeof(Tx), "Unsupported data type");
			        }
			    };

			    //! The SYCL min trait specialization.
			    template<typename Tx, typename Ty>
			    struct Min<math::MinGenericSycl, Tx, Ty, std::enable_if_t<std::is_arithmetic_v<Tx> && std::is_arithmetic_v<Ty>>>
			    {
			        auto operator()(math::MinGenericSycl const&, Tx const& x, Ty const& y)
			        {
			            if constexpr(std::is_integral_v<Tx> && std::is_integral_v<Ty>)
			                return sycl::min(x, y);
			            else if constexpr(std::is_floating_point_v<Tx> || std::is_floating_point_v<Ty>)
			                return sycl::fmin(x, y);
			            else if constexpr(
			                (std::is_floating_point_v<Tx> && std::is_integral_v<Ty>)
			                || (std::is_integral_v<Tx> && std::is_floating_point_v<Ty>) )
			                return sycl::fmin(static_cast<double>(x), static_cast<double>(y)); // mirror CUDA back-end
			            else
			                static_assert(!sizeof(Tx), "Unsupported data type");
			        }
			    };

			    //! The SYCL pow trait specialization.
			    template<typename TBase, typename TExp>
			    struct Pow<
			        math::PowGenericSycl,
			        TBase,
			        TExp,
			        std::enable_if_t<std::is_floating_point_v<TBase> && std::is_floating_point_v<TExp>>>
			    {
			        auto operator()(math::PowGenericSycl const&, TBase const& base, TExp const& exp)
			        {
			            return sycl::pow(base, exp);
			        }
			    };

			    //! The SYCL remainder trait specialization.
			    template<typename Tx, typename Ty>
			    struct Remainder<
			        math::RemainderGenericSycl,
			        Tx,
			        Ty,
			        std::enable_if_t<std::is_floating_point_v<Tx> && std::is_floating_point_v<Ty>>>
			    {
			        auto operator()(math::RemainderGenericSycl const&, Tx const& x, Ty const& y)
			        {
			            return sycl::remainder(x, y);
			        }
			    };

			    //! The SYCL round trait specialization.
			    template<typename TArg>
			    struct Round<math::RoundGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::RoundGenericSycl const&, TArg const& arg)
			        {
			            return sycl::round(arg);
			        }
			    };

			    //! The SYCL lround trait specialization.
			    template<typename TArg>
			    struct Lround<math::RoundGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::RoundGenericSycl const&, TArg const& arg)
			        {
			            return static_cast<long>(sycl::round(arg));
			        }
			    };

			    //! The SYCL llround trait specialization.
			    template<typename TArg>
			    struct Llround<math::RoundGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::RoundGenericSycl const&, TArg const& arg)
			        {
			            return static_cast<long long>(sycl::round(arg));
			        }
			    };

			    //! The SYCL rsqrt trait specialization.
			    template<typename TArg>
			    struct Rsqrt<math::RsqrtGenericSycl, TArg, std::enable_if_t<std::is_arithmetic_v<TArg>>>
			    {
			        auto operator()(math::RsqrtGenericSycl const&, TArg const& arg)
			        {
			            if(std::is_floating_point_v<TArg>)
			                return sycl::rsqrt(arg);
			            else if(std::is_integral_v<TArg>)
			                return sycl::rsqrt(static_cast<double>(arg)); // mirror CUDA back-end and use double for ints
			            else
			                static_assert(!sizeof(TArg), "Unsupported data type");
			        }
			    };

			    //! The SYCL sin trait specialization.
			    template<typename TArg>
			    struct Sin<math::SinGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::SinGenericSycl const&, TArg const& arg)
			        {
			            return sycl::sin(arg);
			        }
			    };

			    //! The SYCL sinh trait specialization.
			    template<typename TArg>
			    struct Sinh<math::SinhGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::SinhGenericSycl const&, TArg const& arg)
			        {
			            return sycl::sinh(arg);
			        }
			    };

			    //! The SYCL sincos trait specialization.
			    template<typename TArg>
			    struct SinCos<math::SinCosGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::SinCosGenericSycl const&, TArg const& arg, TArg& result_sin, TArg& result_cos) -> void
			        {
			            result_sin = sycl::sincos(arg, &result_cos);
			        }
			    };

			    //! The SYCL sqrt trait specialization.
			    template<typename TArg>
			    struct Sqrt<math::SqrtGenericSycl, TArg, std::enable_if_t<std::is_arithmetic_v<TArg>>>
			    {
			        auto operator()(math::SqrtGenericSycl const&, TArg const& arg)
			        {
			            if constexpr(std::is_floating_point_v<TArg>)
			                return sycl::sqrt(arg);
			            else if constexpr(std::is_integral_v<TArg>)
			                return sycl::sqrt(static_cast<double>(arg)); // mirror CUDA back-end and use double for ints
			        }
			    };

			    //! The SYCL tan trait specialization.
			    template<typename TArg>
			    struct Tan<math::TanGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::TanGenericSycl const&, TArg const& arg)
			        {
			            return sycl::tan(arg);
			        }
			    };

			    //! The SYCL tanh trait specialization.
			    template<typename TArg>
			    struct Tanh<math::TanhGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::TanhGenericSycl const&, TArg const& arg)
			        {
			            return sycl::tanh(arg);
			        }
			    };

			    //! The SYCL trunc trait specialization.
			    template<typename TArg>
			    struct Trunc<math::TruncGenericSycl, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			    {
			        auto operator()(math::TruncGenericSycl const&, TArg const& arg)
			        {
			            return sycl::trunc(arg);
			        }
			    };
			} // namespace alpaka::math::trait

			#endif
			// ==
			// == ./include/alpaka/math/MathGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/mem/fence/MemFenceGenericSycl.hpp ==
			// ==
			/* Copyright 2023 Jan Stephan, Luca Ferragina, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/mem/fence/Traits.hpp"    // amalgamate: file already expanded

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka
			{
			    namespace detail
			    {
			        template<typename TAlpakaMemScope>
			        struct SyclFenceProps
			        {
			        };

			        template<>
			        struct SyclFenceProps<alpaka::memory_scope::Block>
			        {
			            static constexpr auto scope = sycl::memory_scope::work_group;
			            static constexpr auto space = sycl::access::address_space::local_space;
			        };

			        template<>
			        struct SyclFenceProps<alpaka::memory_scope::Device>
			        {
			            static constexpr auto scope = sycl::memory_scope::device;
			            static constexpr auto space = sycl::access::address_space::global_space;
			        };
			    } // namespace detail

			    //! The SYCL memory fence.
			    class MemFenceGenericSycl : public concepts::Implements<ConceptMemFence, MemFenceGenericSycl>
			    {
			    public:
			        MemFenceGenericSycl(
			            sycl::accessor<int, 1, sycl::access_mode::read_write, sycl::target::device> global_dummy,
			            sycl::local_accessor<int> local_dummy)
			            : m_global_dummy{global_dummy}
			            , m_local_dummy{local_dummy}
			        {
			        }

			        sycl::accessor<int, 1, sycl::access_mode::read_write, sycl::target::device> m_global_dummy;
			        sycl::local_accessor<int> m_local_dummy;
			    };
			} // namespace alpaka

			namespace alpaka::trait
			{
			    template<typename TMemScope>
			    struct MemFence<MemFenceGenericSycl, TMemScope>
			    {
			        static auto mem_fence(MemFenceGenericSycl const& fence, TMemScope const&)
			        {
			            static constexpr auto scope = detail::SyclFenceProps<TMemScope>::scope;
			            static constexpr auto space = detail::SyclFenceProps<TMemScope>::space;
			            auto dummy
			                = (scope == sycl::memory_scope::work_group)
			                      ? sycl::atomic_ref<int, sycl::memory_order::relaxed, scope, space>{fence.m_local_dummy[0]}
			                      : sycl::atomic_ref<int, sycl::memory_order::relaxed, scope, space>{fence.m_global_dummy[0]};
			            auto const dummy_val = dummy.load();
			            sycl::atomic_fence(sycl::memory_order::acq_rel, scope);
			            dummy.store(dummy_val);
			        }
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/mem/fence/MemFenceGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/warp/WarpGenericSycl.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/warp/Traits.hpp"    // amalgamate: file already expanded

			// #include <cstdint>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka::warp
			{
			    //! The SYCL warp.
			    template<typename TDim>
			    class WarpGenericSycl : public concepts::Implements<alpaka::warp::ConceptWarp, WarpGenericSycl<TDim>>
			    {
			    public:
			        WarpGenericSycl(sycl::nd_item<TDim::value> my_item) : m_item{my_item}
			        {
			        }

			        sycl::nd_item<TDim::value> m_item;
			    };
			} // namespace alpaka::warp

			namespace alpaka::warp::trait
			{
			    template<typename TDim>
			    struct GetSize<warp::WarpGenericSycl<TDim>>
			    {
			        static auto getSize(warp::WarpGenericSycl<TDim> const& warp) -> std::int32_t
			        {
			            auto const sub_group = warp.m_item.get_sub_group();
			            // SYCL sub-groups are always 1D
			            return static_cast<std::int32_t>(sub_group.get_local_linear_range());
			        }
			    };

			    template<typename TDim>
			    struct Activemask<warp::WarpGenericSycl<TDim>>
			    {
			        static auto activemask(warp::WarpGenericSycl<TDim> const& warp) -> std::uint32_t
			        {
			            // SYCL has no way of querying this. Since sub-group functions have to be executed in convergent code
			            // regions anyway we return the full mask.
			            auto const sub_group = warp.m_item.get_sub_group();
			            return sycl::ext::oneapi::group_ballot(sub_group, true);
			        }
			    };

			    template<typename TDim>
			    struct All<warp::WarpGenericSycl<TDim>>
			    {
			        static auto all(warp::WarpGenericSycl<TDim> const& warp, std::int32_t predicate) -> std::int32_t
			        {
			            auto const sub_group = warp.m_item.get_sub_group();
			            return static_cast<std::int32_t>(sycl::all_of_group(sub_group, static_cast<bool>(predicate)));
			        }
			    };

			    template<typename TDim>
			    struct Any<warp::WarpGenericSycl<TDim>>
			    {
			        static auto any(warp::WarpGenericSycl<TDim> const& warp, std::int32_t predicate) -> std::int32_t
			        {
			            auto const sub_group = warp.m_item.get_sub_group();
			            return static_cast<std::int32_t>(sycl::any_of_group(sub_group, static_cast<bool>(predicate)));
			        }
			    };

			    template<typename TDim>
			    struct Ballot<warp::WarpGenericSycl<TDim>>
			    {
			        static auto ballot(warp::WarpGenericSycl<TDim> const& warp, std::int32_t predicate)
			        {
			            auto const sub_group = warp.m_item.get_sub_group();
			            return sycl::ext::oneapi::group_ballot(sub_group, static_cast<bool>(predicate));
			        }
			    };

			    template<typename TDim>
			    struct Shfl<warp::WarpGenericSycl<TDim>>
			    {
			        template<typename T>
			        static auto shfl(warp::WarpGenericSycl<TDim> const& warp, T value, std::int32_t srcLane, std::int32_t width)
			        {
			            /* If width < srcLane the sub-group needs to be split into assumed subdivisions. The first item of each
			               subdivision has the assumed index 0. The srcLane index is relative to the subdivisions.

			               Example: If we assume a sub-group size of 32 and a width of 16 we will receive two subdivisions:
			               The first starts at sub-group index 0 and the second at sub-group index 16. For srcLane = 4 the
			               first subdivision will access the value at sub-group index 4 and the second at sub-group index 20. */
			            auto const actual_group = warp.m_item.get_sub_group();
			            auto const actual_item_id = actual_group.get_local_linear_id();

			            auto const assumed_group_id = actual_item_id / width;
			            auto const assumed_item_id = actual_item_id % width;

			            auto const assumed_src_id = static_cast<std::size_t>(srcLane % width);
			            auto const actual_src_id = assumed_src_id + assumed_group_id * width;

			            auto const src = sycl::id<1>{actual_src_id};

			            return sycl::select_from_group(actual_group, value, src);
			        }
			    };
			} // namespace alpaka::warp::trait

			#endif
			// ==
			// == ./include/alpaka/warp/WarpGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/workdiv/WorkDivGenericSycl.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded
			// #include "alpaka/workdiv/Traits.hpp"    // amalgamate: file already expanded

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka
			{
			    //! The SYCL accelerator work division.
			    template<typename TDim, typename TIdx>
			    class WorkDivGenericSycl : public concepts::Implements<ConceptWorkDiv, WorkDivGenericSycl<TDim, TIdx>>
			    {
			    public:
			        using WorkDivBase = WorkDivGenericSycl;

			        WorkDivGenericSycl(Vec<TDim, TIdx> const& threadElemExtent, sycl::nd_item<TDim::value> work_item)
			            : m_threadElemExtent{threadElemExtent}
			            , my_item{work_item}
			        {
			        }

			        Vec<TDim, TIdx> const& m_threadElemExtent;
			        sycl::nd_item<TDim::value> my_item;
			    };
			} // namespace alpaka

			namespace alpaka::trait
			{
			    //! The SYCL accelerator work division dimension get trait specialization.
			    template<typename TDim, typename TIdx>
			    struct DimType<WorkDivGenericSycl<TDim, TIdx>>
			    {
			        using type = TDim;
			    };

			    //! The SYCL accelerator work division idx type trait specialization.
			    template<typename TDim, typename TIdx>
			    struct IdxType<WorkDivGenericSycl<TDim, TIdx>>
			    {
			        using type = TIdx;
			    };

			    //! The SYCL accelerator work division grid block extent trait specialization.
			    template<typename TDim, typename TIdx>
			    struct GetWorkDiv<WorkDivGenericSycl<TDim, TIdx>, origin::Grid, unit::Blocks>
			    {
			        //! \return The number of blocks in each dimension of the grid.
			        static auto getWorkDiv(WorkDivGenericSycl<TDim, TIdx> const& workDiv) -> Vec<TDim, TIdx>
			        {
			            if constexpr(TDim::value == 1)
			                return Vec<TDim, TIdx>{static_cast<TIdx>(workDiv.my_item.get_group_range(0))};
			            else if constexpr(TDim::value == 2)
			            {
			                return Vec<TDim, TIdx>{
			                    static_cast<TIdx>(workDiv.my_item.get_group_range(1)),
			                    static_cast<TIdx>(workDiv.my_item.get_group_range(0))};
			            }
			            else
			            {
			                return Vec<TDim, TIdx>{
			                    static_cast<TIdx>(workDiv.my_item.get_group_range(2)),
			                    static_cast<TIdx>(workDiv.my_item.get_group_range(1)),
			                    static_cast<TIdx>(workDiv.my_item.get_group_range(0))};
			            }
			        }
			    };

			    //! The SYCL accelerator work division block thread extent trait specialization.
			    template<typename TDim, typename TIdx>
			    struct GetWorkDiv<WorkDivGenericSycl<TDim, TIdx>, origin::Block, unit::Threads>
			    {
			        //! \return The number of threads in each dimension of a block.
			        static auto getWorkDiv(WorkDivGenericSycl<TDim, TIdx> const& workDiv) -> Vec<TDim, TIdx>
			        {
			            if constexpr(TDim::value == 1)
			                return Vec<TDim, TIdx>{static_cast<TIdx>(workDiv.my_item.get_local_range(0))};
			            else if constexpr(TDim::value == 2)
			            {
			                return Vec<TDim, TIdx>{
			                    static_cast<TIdx>(workDiv.my_item.get_local_range(1)),
			                    static_cast<TIdx>(workDiv.my_item.get_local_range(0))};
			            }
			            else
			            {
			                return Vec<TDim, TIdx>{
			                    static_cast<TIdx>(workDiv.my_item.get_local_range(2)),
			                    static_cast<TIdx>(workDiv.my_item.get_local_range(1)),
			                    static_cast<TIdx>(workDiv.my_item.get_local_range(0))};
			            }
			        }
			    };

			    //! The SYCL accelerator work division thread element extent trait specialization.
			    template<typename TDim, typename TIdx>
			    struct GetWorkDiv<WorkDivGenericSycl<TDim, TIdx>, origin::Thread, unit::Elems>
			    {
			        //! \return The number of blocks in each dimension of the grid.
			        static auto getWorkDiv(WorkDivGenericSycl<TDim, TIdx> const& workDiv) -> Vec<TDim, TIdx>
			        {
			            return workDiv.m_threadElemExtent;
			        }
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/workdiv/WorkDivGenericSycl.hpp ==
			// ============================================================================


		// Specialized traits.
		// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

		// Implementation details.
		// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/ClipCast.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded

		// #include <cstddef>    // amalgamate: file already included
		// #include <string>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included

		#ifdef ALPAKA_ACC_SYCL_ENABLED

		// #    include <CL/sycl.hpp>    // amalgamate: file already included

		namespace alpaka
		{
		    //! The SYCL accelerator.
		    //!
		    //! This accelerator allows parallel kernel execution on SYCL devices.
		    template<typename TDim, typename TIdx>
		    class AccGenericSycl
		        : public WorkDivGenericSycl<TDim, TIdx>
		        , public gb::IdxGbGenericSycl<TDim, TIdx>
		        , public bt::IdxBtGenericSycl<TDim, TIdx>
		        , public AtomicHierarchy<AtomicGenericSycl, AtomicGenericSycl, AtomicGenericSycl>
		        , public math::MathGenericSycl
		        , public BlockSharedMemDynGenericSycl
		        , public BlockSharedMemStGenericSycl
		        , public BlockSyncGenericSycl<TDim>
		        , public IntrinsicGenericSycl
		        , public MemFenceGenericSycl
		        , public warp::WarpGenericSycl<TDim>
		    {
		    public:
		        AccGenericSycl(AccGenericSycl const&) = delete;
		        AccGenericSycl(AccGenericSycl&&) = delete;
		        auto operator=(AccGenericSycl const&) -> AccGenericSycl& = delete;
		        auto operator=(AccGenericSycl&&) -> AccGenericSycl& = delete;

		#    ifdef ALPAKA_SYCL_IOSTREAM_ENABLED
		        AccGenericSycl(
		            Vec<TDim, TIdx> const& threadElemExtent,
		            sycl::nd_item<TDim::value> work_item,
		            sycl::local_accessor<std::byte> dyn_shared_acc,
		            sycl::local_accessor<std::byte> st_shared_acc,
		            sycl::accessor<int, 1, sycl::access_mode::read_write, sycl::target::device> global_fence_dummy,
		            sycl::local_accessor<int> local_fence_dummy,
		            sycl::stream output_stream)
		            : WorkDivGenericSycl<TDim, TIdx>{threadElemExtent, work_item}
		            , gb::IdxGbGenericSycl<TDim, TIdx>{work_item}
		            , bt::IdxBtGenericSycl<TDim, TIdx>{work_item}
		            , AtomicHierarchy<AtomicGenericSycl, AtomicGenericSycl, AtomicGenericSycl>{}
		            , math::MathGenericSycl{}
		            , BlockSharedMemDynGenericSycl{dyn_shared_acc}
		            , BlockSharedMemStGenericSycl{st_shared_acc}
		            , BlockSyncGenericSycl<TDim>{work_item}
		            , IntrinsicGenericSycl{}
		            , MemFenceGenericSycl{global_fence_dummy, local_fence_dummy}
		            , warp::WarpGenericSycl<TDim>{work_item}
		            , cout{output_stream}
		        {
		        }

		        sycl::stream cout;
		#    else
		        AccGenericSycl(
		            Vec<TDim, TIdx> const& threadElemExtent,
		            sycl::nd_item<TDim::value> work_item,
		            sycl::local_accessor<std::byte> dyn_shared_acc,
		            sycl::local_accessor<std::byte> st_shared_acc,
		            sycl::accessor<int, 1, sycl::access_mode::read_write, sycl::target::device> global_fence_dummy,
		            sycl::local_accessor<int> local_fence_dummy)
		            : WorkDivGenericSycl<TDim, TIdx>{threadElemExtent, work_item}
		            , gb::IdxGbGenericSycl<TDim, TIdx>{work_item}
		            , bt::IdxBtGenericSycl<TDim, TIdx>{work_item}
		            , AtomicHierarchy<AtomicGenericSycl, AtomicGenericSycl, AtomicGenericSycl>{}
		            , math::MathGenericSycl{}
		            , BlockSharedMemDynGenericSycl{dyn_shared_acc}
		            , BlockSharedMemStGenericSycl{st_shared_acc}
		            , BlockSyncGenericSycl<TDim>{work_item}
		            , IntrinsicGenericSycl{}
		            , MemFenceGenericSycl{global_fence_dummy, local_fence_dummy}
		            , warp::WarpGenericSycl<TDim>{work_item}
		        {
		        }
		#    endif
		    };
		} // namespace alpaka

		namespace alpaka::trait
		{
		    //! The SYCL accelerator type trait specialization.
		    template<template<typename, typename> typename TAcc, typename TDim, typename TIdx>
		    struct AccType<TAcc<TDim, TIdx>, std::enable_if_t<std::is_base_of_v<AccGenericSycl<TDim, TIdx>, TAcc<TDim, TIdx>>>>
		    {
		        using type = TAcc<TDim, TIdx>;
		    };

		    //! The SYCL accelerator device properties get trait specialization.
		    template<template<typename, typename> typename TAcc, typename TDim, typename TIdx>
		    struct GetAccDevProps<
		        TAcc<TDim, TIdx>,
		        std::enable_if_t<std::is_base_of_v<AccGenericSycl<TDim, TIdx>, TAcc<TDim, TIdx>>>>
		    {
		        static auto getAccDevProps(typename DevType<TAcc<TDim, TIdx>>::type const& dev) -> AccDevProps<TDim, TIdx>
		        {
		            auto const device = dev.getNativeHandle().first;
		            auto const max_threads_dim
		                = device.template get_info<sycl::info::device::max_work_item_sizes<TDim::value>>();
		            Vec<TDim, TIdx> max_threads_dim_vec{};
		            for(int i = 0; i < static_cast<int>(TDim::value); i++)
		                max_threads_dim_vec[i] = alpaka::core::clipCast<TIdx>(max_threads_dim[i]);
		            return {// m_multiProcessorCount
		                    alpaka::core::clipCast<TIdx>(device.template get_info<sycl::info::device::max_compute_units>()),
		                    // m_gridBlockExtentMax
		                    getExtentVecEnd<TDim>(Vec<DimInt<3u>, TIdx>(
		                        // WARNING: There is no SYCL way to determine these values
		                        std::numeric_limits<TIdx>::max(),
		                        std::numeric_limits<TIdx>::max(),
		                        std::numeric_limits<TIdx>::max())),
		                    // m_gridBlockCountMax
		                    std::numeric_limits<TIdx>::max(),
		                    // m_blockThreadExtentMax
		                    max_threads_dim_vec,
		                    // m_blockThreadCountMax
		                    alpaka::core::clipCast<TIdx>(device.template get_info<sycl::info::device::max_work_group_size>()),
		                    // m_threadElemExtentMax
		                    Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
		                    // m_threadElemCountMax
		                    std::numeric_limits<TIdx>::max(),
		                    // m_sharedMemSizeBytes
		                    device.template get_info<sycl::info::device::local_mem_size>()};
		        }
		    };

		    //! The SYCL accelerator dimension getter trait specialization.
		    template<template<typename, typename> typename TAcc, typename TDim, typename TIdx>
		    struct DimType<TAcc<TDim, TIdx>, std::enable_if_t<std::is_base_of_v<AccGenericSycl<TDim, TIdx>, TAcc<TDim, TIdx>>>>
		    {
		        using type = TDim;
		    };

		    //! The SYCL accelerator idx type trait specialization.
		    template<template<typename, typename> typename TAcc, typename TDim, typename TIdx>
		    struct IdxType<TAcc<TDim, TIdx>, std::enable_if_t<std::is_base_of_v<AccGenericSycl<TDim, TIdx>, TAcc<TDim, TIdx>>>>
		    {
		        using type = TIdx;
		    };
		} // namespace alpaka::trait

		#endif
		// ==
		// == ./include/alpaka/acc/AccGenericSycl.hpp ==
		// ============================================================================

	// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/dev/DevCpuSyclIntel.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
			// ============================================================================
			// == ./include/alpaka/dev/DevGenericSycl.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan, Antonio Di Pilato
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/mem/buf/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/Properties.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/queue/sycl/QueueGenericSyclBase.hpp ==
				// ==
				/* Copyright 2022 Jan Stephan, Antonio Di Pilato
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once

				// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/event/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

				// #include <algorithm>    // amalgamate: file already included
				#include <exception>
				// #include <memory>    // amalgamate: file already included
				// #include <mutex>    // amalgamate: file already included
				#include <shared_mutex>
				// #include <type_traits>    // amalgamate: file already included
				// #include <utility>    // amalgamate: file already included
				// #include <vector>    // amalgamate: file already included

				#ifdef ALPAKA_ACC_SYCL_ENABLED

				// #    include <CL/sycl.hpp>    // amalgamate: file already included

				namespace alpaka::detail
				{
				    template<typename T, typename = void>
				    inline constexpr auto is_sycl_task = false;

				    template<typename T>
				    inline constexpr auto is_sycl_task<T, std::void_t<decltype(T::is_sycl_task)>> = true;

				    template<typename T, typename = void>
				    inline constexpr auto is_sycl_kernel = false;

				    template<typename T>
				    inline constexpr auto is_sycl_kernel<T, std::void_t<decltype(T::is_sycl_kernel)>> = true;

				    class QueueGenericSyclImpl
				    {
				    public:
				        QueueGenericSyclImpl(sycl::context context, sycl::device device)
				            : m_queue{
				                std::move(context), // This is important. In SYCL a device can belong to multiple contexts.
				                std::move(device),
				                {sycl::property::queue::enable_profiling{}, sycl::property::queue::in_order{}}}
				        {
				        }

				        // This class will only exist as a pointer. We don't care about copy and move semantics.
				        QueueGenericSyclImpl(QueueGenericSyclImpl const& other) = delete;
				        auto operator=(QueueGenericSyclImpl const& rhs) -> QueueGenericSyclImpl& = delete;

				        QueueGenericSyclImpl(QueueGenericSyclImpl&& other) noexcept = delete;
				        auto operator=(QueueGenericSyclImpl&& rhs) noexcept -> QueueGenericSyclImpl& = delete;

				        ~QueueGenericSyclImpl()
				        {
				            try
				            {
				                m_queue.wait_and_throw();
				            }
				            catch(sycl::exception const& err)
				            {
				                std::cerr << "Caught SYCL exception while destructing a SYCL queue: " << err.what() << " ("
				                          << err.code() << ')' << std::endl;
				            }
				            catch(std::exception const& err)
				            {
				                std::cerr << "The following runtime error(s) occured while destructing a SYCL queue:" << err.what()
				                          << std::endl;
				            }
				        }

				        // Don't call this without locking first!
				        auto clean_dependencies() -> void
				        {
				            // Clean up completed events
				            auto const start = std::begin(m_dependencies);
				            auto const old_end = std::end(m_dependencies);
				            auto const new_end = std::remove_if(
				                start,
				                old_end,
				                [](sycl::event ev) {
				                    return ev.get_info<sycl::info::event::command_execution_status>()
				                           == sycl::info::event_command_status::complete;
				                });

				            m_dependencies.erase(new_end, old_end);
				        }

				        auto register_dependency(sycl::event event) -> void
				        {
				            std::lock_guard<std::shared_mutex> lock{m_mutex};

				            clean_dependencies();
				            m_dependencies.push_back(event);
				        }

				        auto empty() const -> bool
				        {
				            std::shared_lock<std::shared_mutex> lock{m_mutex};
				            return m_last_event.get_info<sycl::info::event::command_execution_status>()
				                   == sycl::info::event_command_status::complete;
				        }

				        auto wait() -> void
				        {
				            // SYCL queues are thread-safe.
				            m_queue.wait_and_throw();
				        }

				        auto get_last_event() const -> sycl::event
				        {
				            std::shared_lock<std::shared_mutex> lock{m_mutex};
				            return m_last_event;
				        }

				        template<bool TBlocking, typename TTask>
				        auto enqueue(TTask const& task) -> void
				        {
				            {
				                std::lock_guard<std::shared_mutex> lock{m_mutex};

				                clean_dependencies();

				                // Execute task
				                m_last_event = m_queue.submit(
				                    [this, &task](sycl::handler& cgh)
				                    {
				                        if(!m_dependencies.empty())
				                            cgh.depends_on(m_dependencies);

				                        if constexpr(is_sycl_kernel<TTask>) // Kernel
				                            task(cgh, m_fence_dummy); // Will call cgh.parallel_for internally
				                        else if constexpr(is_sycl_task<TTask>) // Copy / Fill
				                            task(cgh); // Will call cgh.{copy, fill} internally
				                        else // Host
				                            cgh.host_task(task);
				                    });

				                m_dependencies.clear();
				            }

				            if constexpr(TBlocking)
				                wait();
				        }

				        [[nodiscard]] auto getNativeHandle() const noexcept
				        {
				            return m_queue;
				        }

				        std::vector<sycl::event> m_dependencies;
				        sycl::event m_last_event;
				        sycl::buffer<int, 1> m_fence_dummy{sycl::range<1>{1}};
				        std::shared_mutex mutable m_mutex;

				    private:
				        sycl::queue m_queue;
				    };

				    template<typename TDev, bool TBlocking>
				    class QueueGenericSyclBase
				    {
				    public:
				        QueueGenericSyclBase(TDev const& dev)
				            : m_dev{dev}
				            , m_impl{std::make_shared<detail::QueueGenericSyclImpl>(
				                  dev.getNativeHandle().second,
				                  dev.getNativeHandle().first)}
				        {
				            m_dev.m_impl->register_queue(m_impl);
				        }

				        friend auto operator==(QueueGenericSyclBase const& lhs, QueueGenericSyclBase const& rhs) -> bool
				        {
				            return (lhs.m_dev == rhs.m_dev) && (lhs.m_impl == rhs.m_impl);
				        }

				        friend auto operator!=(QueueGenericSyclBase const& lhs, QueueGenericSyclBase const& rhs) -> bool
				        {
				            return !(lhs == rhs);
				        }

				        [[nodiscard]] auto getNativeHandle() const noexcept
				        {
				            return m_impl->getNativeHandle();
				        }

				        TDev m_dev;
				        std::shared_ptr<detail::QueueGenericSyclImpl> m_impl;
				    };
				} // namespace alpaka::detail

				namespace alpaka
				{
				    template<typename TDev>
				    class EventGenericSycl;
				}

				namespace alpaka::trait
				{
				    //! The SYCL blocking queue device type trait specialization.
				    template<typename TDev, bool TBlocking>
				    struct DevType<detail::QueueGenericSyclBase<TDev, TBlocking>>
				    {
				        using type = TDev;
				    };

				    //! The SYCL blocking queue device get trait specialization.
				    template<typename TDev, bool TBlocking>
				    struct GetDev<detail::QueueGenericSyclBase<TDev, TBlocking>>
				    {
				        static auto getDev(detail::QueueGenericSyclBase<TDev, TBlocking> const& queue)
				        {
				            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;
				            return queue.m_dev;
				        }
				    };

				    //! The SYCL blocking queue event type trait specialization.
				    template<typename TDev, bool TBlocking>
				    struct EventType<detail::QueueGenericSyclBase<TDev, TBlocking>>
				    {
				        using type = EventGenericSycl<TDev>;
				    };

				    //! The SYCL blocking queue enqueue trait specialization.
				    template<typename TDev, bool TBlocking, typename TTask>
				    struct Enqueue<detail::QueueGenericSyclBase<TDev, TBlocking>, TTask>
				    {
				        static auto enqueue(detail::QueueGenericSyclBase<TDev, TBlocking>& queue, TTask const& task) -> void
				        {
				            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;
				            queue.m_impl->template enqueue<TBlocking>(task);
				        }
				    };

				    //! The SYCL blocking queue test trait specialization.
				    template<typename TDev, bool TBlocking>
				    struct Empty<detail::QueueGenericSyclBase<TDev, TBlocking>>
				    {
				        static auto empty(detail::QueueGenericSyclBase<TDev, TBlocking> const& queue) -> bool
				        {
				            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;
				            return queue.m_impl->empty();
				        }
				    };

				    //! The SYCL blocking queue thread wait trait specialization.
				    //!
				    //! Blocks execution of the calling thread until the queue has finished processing all previously requested
				    //! tasks (kernels, data copies, ...)
				    template<typename TDev, bool TBlocking>
				    struct CurrentThreadWaitFor<detail::QueueGenericSyclBase<TDev, TBlocking>>
				    {
				        static auto currentThreadWaitFor(detail::QueueGenericSyclBase<TDev, TBlocking> const& queue) -> void
				        {
				            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;
				            queue.m_impl->wait();
				        }
				    };

				    //! The SYCL queue native handle trait specialization.
				    template<typename TDev, bool TBlocking>
				    struct NativeHandle<detail::QueueGenericSyclBase<TDev, TBlocking>>
				    {
				        [[nodiscard]] static auto getNativeHandle(detail::QueueGenericSyclBase<TDev, TBlocking> const& queue)
				        {
				            return queue.getNativeHandle();
				        }
				    };
				} // namespace alpaka::trait

				#endif
				// ==
				// == ./include/alpaka/queue/sycl/QueueGenericSyclBase.hpp ==
				// ============================================================================

			// #include "alpaka/traits/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

			// #include <algorithm>    // amalgamate: file already included
			// #include <cstddef>    // amalgamate: file already included
			// #include <memory>    // amalgamate: file already included
			// #include <mutex>    // amalgamate: file already included
			// #include <shared_mutex>    // amalgamate: file already included
			// #include <string>    // amalgamate: file already included
			// #include <utility>    // amalgamate: file already included
			// #include <vector>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka
			{
			    template<typename TElem, typename TDim, typename TIdx, typename TDev>
			    class BufGenericSycl;

			    namespace detail
			    {
			        class DevGenericSyclImpl
			        {
			        public:
			            DevGenericSyclImpl(sycl::device device, sycl::context context)
			                : m_device{std::move(device)}
			                , m_context{std::move(context)}
			            {
			            }

			            // Don't call this without locking first!
			            auto clean_queues() -> void
			            {
			                // Clean up dead queues
			                auto const start = std::begin(m_queues);
			                auto const old_end = std::end(m_queues);
			                auto const new_end = std::remove_if(start, old_end, [](auto q_ptr) { return q_ptr.expired(); });
			                m_queues.erase(new_end, old_end);
			            }

			            auto register_queue(std::shared_ptr<QueueGenericSyclImpl> const& queue) -> void
			            {
			                std::lock_guard<std::shared_mutex> lock{m_mutex};

			                clean_queues();
			                m_queues.emplace_back(queue);
			            }

			            auto register_dependency(sycl::event event) -> void
			            {
			                std::shared_lock<std::shared_mutex> lock{m_mutex};

			                for(auto& q_ptr : m_queues)
			                {
			                    if(auto ptr = q_ptr.lock(); ptr != nullptr)
			                        ptr->register_dependency(event);
			                }
			            }

			            auto wait()
			            {
			                std::shared_lock<std::shared_mutex> lock{m_mutex};

			                for(auto& q_ptr : m_queues)
			                {
			                    if(auto ptr = q_ptr.lock(); ptr != nullptr)
			                        ptr->wait();
			                }
			            }

			            auto get_device() const -> sycl::device
			            {
			                return m_device;
			            }

			            auto get_context() const -> sycl::context
			            {
			                return m_context;
			            }

			        private:
			            sycl::device m_device;
			            sycl::context m_context;
			            std::vector<std::weak_ptr<QueueGenericSyclImpl>> m_queues;
			            std::shared_mutex mutable m_mutex;
			        };
			    } // namespace detail

			    //! The SYCL device handle.
			    template<typename TPltf>
			    class DevGenericSycl
			        : public concepts::Implements<ConceptCurrentThreadWaitFor, DevGenericSycl<TPltf>>
			        , public concepts::Implements<ConceptDev, DevGenericSycl<TPltf>>
			    {
			    public:
			        DevGenericSycl(sycl::device device, sycl::context context)
			            : m_impl{std::make_shared<detail::DevGenericSyclImpl>(std::move(device), std::move(context))}
			        {
			        }

			        friend auto operator==(DevGenericSycl const& lhs, DevGenericSycl const& rhs) -> bool
			        {
			            return (lhs.m_impl == rhs.m_impl);
			        }

			        friend auto operator!=(DevGenericSycl const& lhs, DevGenericSycl const& rhs) -> bool
			        {
			            return !(lhs == rhs);
			        }

			        [[nodiscard]] auto getNativeHandle() const -> std::pair<sycl::device, sycl::context>
			        {
			            return std::make_pair(m_impl->get_device(), m_impl->get_context());
			        }

			        std::shared_ptr<detail::DevGenericSyclImpl> m_impl;
			    };
			} // namespace alpaka

			namespace alpaka::trait
			{
			    //! The SYCL device name get trait specialization.
			    template<typename TPltf>
			    struct GetName<DevGenericSycl<TPltf>>
			    {
			        static auto getName(DevGenericSycl<TPltf> const& dev) -> std::string
			        {
			            auto const device = dev.getNativeHandle().first;
			            return device.template get_info<sycl::info::device::name>();
			        }
			    };

			    //! The SYCL device available memory get trait specialization.
			    template<typename TPltf>
			    struct GetMemBytes<DevGenericSycl<TPltf>>
			    {
			        static auto getMemBytes(DevGenericSycl<TPltf> const& dev) -> std::size_t
			        {
			            auto const device = dev.getNativeHandle().first;
			            return device.template get_info<sycl::info::device::global_mem_size>();
			        }
			    };

			    //! The SYCL device free memory get trait specialization.
			    template<typename TPltf>
			    struct GetFreeMemBytes<DevGenericSycl<TPltf>>
			    {
			        static auto getFreeMemBytes(DevGenericSycl<TPltf> const& /* dev */) -> std::size_t
			        {
			            static_assert(!sizeof(TPltf), "Querying free device memory not supported for SYCL devices.");
			            return std::size_t{};
			        }
			    };

			    //! The SYCL device warp size get trait specialization.
			    template<typename TPltf>
			    struct GetWarpSizes<DevGenericSycl<TPltf>>
			    {
			        static auto getWarpSizes(DevGenericSycl<TPltf> const& dev) -> std::vector<std::size_t>
			        {
			            auto const device = dev.getNativeHandle().first;
			            return device.template get_info<sycl::info::device::sub_group_sizes>();
			        }
			    };

			    //! The SYCL device reset trait specialization.
			    template<typename TPltf>
			    struct Reset<DevGenericSycl<TPltf>>
			    {
			        static auto reset(DevGenericSycl<TPltf> const&) -> void
			        {
			            static_assert(!sizeof(TPltf), "Explicit device reset not supported for SYCL devices");
			        }
			    };

			    //! The SYCL device native handle trait specialization.
			    template<typename TPltf>
			    struct NativeHandle<DevGenericSycl<TPltf>>
			    {
			        [[nodiscard]] static auto getNativeHandle(DevGenericSycl<TPltf> const& dev)
			        {
			            return dev.getNativeHandle();
			        }
			    };

			    //! The SYCL device memory buffer type trait specialization.
			    template<typename TElem, typename TDim, typename TIdx, typename TPltf>
			    struct BufType<DevGenericSycl<TPltf>, TElem, TDim, TIdx>
			    {
			        using type = BufGenericSycl<TElem, TDim, TIdx, DevGenericSycl<TPltf>>;
			    };

			    //! The SYCL device platform type trait specialization.
			    template<typename TPltf>
			    struct PltfType<DevGenericSycl<TPltf>>
			    {
			        using type = TPltf;
			    };

			    //! The thread SYCL device wait specialization.
			    template<typename TPltf>
			    struct CurrentThreadWaitFor<DevGenericSycl<TPltf>>
			    {
			        static auto currentThreadWaitFor(DevGenericSycl<TPltf> const& dev) -> void
			        {
			            dev.m_impl->wait();
			        }
			    };

			    //! The SYCL blocking queue trait specialization.
			    template<typename TPltf>
			    struct QueueType<DevGenericSycl<TPltf>, Blocking>
			    {
			        using type = detail::QueueGenericSyclBase<DevGenericSycl<TPltf>, true>;
			    };

			    //! The SYCL non-blocking queue trait specialization.
			    template<typename TPltf>
			    struct QueueType<DevGenericSycl<TPltf>, NonBlocking>
			    {
			        using type = detail::QueueGenericSyclBase<DevGenericSycl<TPltf>, false>;
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/dev/DevGenericSycl.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/pltf/PltfCpuSyclIntel.hpp ==
			// ==
			/* Copyright 2023 Jan Stephan, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/dev/DevGenericSycl.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/pltf/PltfGenericSycl.hpp ==
				// ==
				/* Copyright 2023 Jan Stephan
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
				// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

				// #include <cstddef>    // amalgamate: file already included
				// #include <exception>    // amalgamate: file already included
				#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
				// #    include <iostream>    // amalgamate: file already included
				#endif
				// #include <mutex>    // amalgamate: file already included
				#include <optional>
				#include <sstream>
				// #include <stdexcept>    // amalgamate: file already included
				// #include <vector>    // amalgamate: file already included

				#ifdef ALPAKA_ACC_SYCL_ENABLED

				// #    include <CL/sycl.hpp>    // amalgamate: file already included

				namespace alpaka
				{
				    //! The SYCL device manager.
				    template<typename TSelector>
				    class PltfGenericSycl : public concepts::Implements<ConceptPltf, PltfGenericSycl<TSelector>>
				    {
				    public:
				        PltfGenericSycl() = delete;

				        [[nodiscard]] static auto syclPlatform() -> sycl::platform&
				        {
				            auto lock = std::scoped_lock<std::mutex>{mutex};

				            if(!initialized)
				                do_initialization();

				            return *platform_opt;
				        }

				        [[nodiscard]] static auto syclDevices() -> std::vector<sycl::device>&
				        {
				            auto lock = std::scoped_lock<std::mutex>{mutex};

				            if(!initialized)
				                do_initialization();

				            return devices;
				        }

				        [[nodiscard]] static auto syclContext() -> sycl::context&
				        {
				            auto lock = std::scoped_lock<std::mutex>{mutex};

				            if(!initialized)
				                do_initialization();

				            return *context_opt;
				        }

				        static auto initialize() -> void
				        {
				            auto lock = std::scoped_lock<std::mutex>{mutex};
				            do_initialization();
				        }

				        static auto reset() -> void
				        {
				            auto lock = std::scoped_lock<std::mutex>{mutex};

				            if(!initialized)
				                return;

				            context_opt.reset();
				            devices.clear();
				            platform_opt.reset();
				            initialized = false;
				        }

				    private:
				        static auto do_initialization()
				        {
				            if(initialized)
				                return;

				            platform_opt = sycl::platform{TSelector{}};
				            devices = platform_opt->get_devices();
				            context_opt = sycl::context{
				                devices,
				                [](sycl::exception_list exceptions)
				                {
				                    auto ss_err = std::stringstream{};
				                    ss_err << "Caught asynchronous SYCL exception(s):\n";
				                    for(std::exception_ptr e : exceptions)
				                    {
				                        try
				                        {
				                            std::rethrow_exception(e);
				                        }
				                        catch(sycl::exception const& err)
				                        {
				                            ss_err << err.what() << " (" << err.code() << ")\n";
				                        }
				                    }
				                    throw std::runtime_error(ss_err.str());
				                }};

				            initialized = true;
				        }

				#    if BOOST_COMP_CLANG
				#        pragma clang diagnostic push
				#        pragma clang diagnostic ignored "-Wexit-time-destructors"
				#    endif
				        inline static std::mutex mutex;
				        inline static bool initialized{false};

				        inline static std::optional<sycl::platform> platform_opt{std::nullopt};
				        inline static std::vector<sycl::device> devices;
				        inline static std::optional<sycl::context> context_opt{std::nullopt};
				#    if BOOST_COMP_CLANG
				#        pragma clang diagnostic pop
				#    endif
				    };
				} // namespace alpaka

				namespace alpaka::trait
				{
				    //! The SYCL platform device count get trait specialization.
				    template<typename TSelector>
				    struct GetDevCount<alpaka::PltfGenericSycl<TSelector>>
				    {
				        static auto getDevCount() -> std::size_t
				        {
				            ALPAKA_DEBUG_FULL_LOG_SCOPE;

				            return alpaka::PltfGenericSycl<TSelector>::syclDevices().size();
				        }
				    };

				    //! The SYCL platform device get trait specialization.
				    template<typename TSelector>
				    struct GetDevByIdx<alpaka::PltfGenericSycl<TSelector>>
				    {
				        static auto getDevByIdx(std::size_t const& devIdx)
				        {
				            ALPAKA_DEBUG_FULL_LOG_SCOPE;

				            using SyclPltf = alpaka::PltfGenericSycl<TSelector>;

				            auto const dev_num = getDevCount<SyclPltf>();

				            if(devIdx >= dev_num)
				            {
				                auto ss_err = std::stringstream{};
				                ss_err << "Unable to return device handle for device " << devIdx << ". There are only " << dev_num
				                       << " SYCL devices!";
				                throw std::runtime_error(ss_err.str());
				            }

				            auto sycl_dev = SyclPltf::syclDevices().at(devIdx);

				            // Log this device.
				#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
				            printDeviceProperties(sycl_dev);
				#    elif ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
				            std::cout << __func__ << sycl_dev.get_info<info::device::name>() << '\n';
				#    endif
				            return typename DevType<SyclPltf>::type{sycl_dev, SyclPltf::syclContext()};
				        }

				    private:
				#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
				        //! Prints all the device properties to std::cout.
				        static auto printDeviceProperties(sycl::device const& device) -> void
				        {
				            ALPAKA_DEBUG_FULL_LOG_SCOPE;

				            constexpr auto KiB = std::size_t{1024};
				            constexpr auto MiB = KiB * KiB;

				            std::cout << "Device type: ";
				            switch(device.get_info<sycl::info::device::device_type>())
				            {
				            case sycl::info::device_type::cpu:
				                std::cout << "CPU";
				                break;

				            case sycl::info::device_type::gpu:
				                std::cout << "GPU";
				                break;

				            case sycl::info::device_type::accelerator:
				                std::cout << "Accelerator";
				                break;

				            case sycl::info::device_type::custom:
				                std::cout << "Custom";
				                break;

				            case sycl::info::device_type::automatic:
				                std::cout << "Automatic";
				                break;

				            case sycl::info::device_type::host:
				                std::cout << "Host";
				                break;

				            // The SYCL spec forbids the return of device_type::all
				            // Including this here to prevent warnings because of
				            // missing cases
				            case sycl::info::device_type::all:
				                std::cout << "All";
				                break;
				            }
				            std::cout << '\n';

				            std::cout << "Name: " << device.get_info<sycl::info::device::name>() << '\n';

				            std::cout << "Vendor: " << device.get_info<sycl::info::device::vendor>() << '\n';

				            std::cout << "Vendor ID: " << device.get_info<sycl::info::device::vendor_id>() << '\n';

				            std::cout << "Driver version: " << device.get_info<sycl::info::device::driver_version>() << '\n';

				            std::cout << "SYCL version: " << device.get_info<sycl::info::device::version>() << '\n';

				            std::cout << "Backend version: " << device.get_info<sycl::info::device::backend_version>() << '\n';

				            std::cout << "Aspects: " << '\n';
				            auto const aspects = device.get_info<sycl::info::device::aspects>();
				            for(auto const& asp : aspects)
				            {
				                switch(asp)
				                {
				                // Ignore the hardware types - we already have queried this info above
				                case sycl::aspect::cpu:
				                case sycl::aspect::gpu:
				                case sycl::aspect::accelerator:
				                case sycl::aspect::custom:
				                    break;

				                case sycl::aspect::emulated:
				                    std::cout << "\t* emulated\n";
				                    break;

				                case sycl::aspect::host_debugabble:
				                    std::cout << "\t* debugabble using standard debuggers\n";
				                    break;

				                case sycl::aspect::fp16:
				                    std::cout << "\t* supports sycl::half precision\n";
				                    break;

				                case sycl::aspect::fp64:
				                    std::cout << "\t* supports double precision\n";
				                    break;

				                case sycl::aspect::atomic64:
				                    std::cout << "\t* supports 64-bit atomics\n";
				                    break;

				                case sycl::aspect::image:
				                    std::cout << "\t* supports images\n";
				                    break;

				                case sycl::aspect::online_compiler:
				                    std::cout << "\t* supports online compilation of device code\n";
				                    break;

				                case sycl::aspect::online_linker:
				                    std::cout << "\t* supports online linking of device code\n";
				                    break;

				                case sycl::aspect::queue_profiling:
				                    std::cout << "\t* supports queue profiling\n";
				                    break;

				                case sycl::aspect::usm_device_allocations:
				                    std::cout << "\t* supports explicit USM allocations\n";
				                    break;

				                case sycl::aspect::usm_host_allocations:
				                    std::cout << "\t* can access USM memory allocated by sycl::usm::alloc::host\n";
				                    break;

				                case sycl::aspect::usm_atomic_host_allocations:
				                    std::cout << "\t* can access USM memory allocated by sycl::usm::alloc::host atomically\n";
				                    break;

				                case sycl::aspect::usm_shared_allocations:
				                    std::cout << "\t* can access USM memory allocated by sycl::usm::alloc::shared\n";
				                    break;

				                case sycl::aspect::usm_atomic_shared_allocations:
				                    std::cout << "\t* can access USM memory allocated by sycl::usm::alloc::shared atomically\n";
				                    break;

				                case sycl::aspect::usm_system_allocations:
				                    std::cout << "\t* can access memory allocated by the system allocator\n";
				                    break;
				                }
				            }

				            std::cout << "Available compute units: " << device.get_info<sycl::info::device::max_compute_units>()
				                      << '\n';

				            std::cout << "Maximum work item dimensions: ";
				            auto dims = device.get_info<sycl::info::device::max_work_item_dimensions>();
				            std::cout << dims << std::endl;

				            std::cout << "Maximum number of work items:\n";
				            auto const wi_1D = device.get_info<sycl::info::device::max_work_item_sizes<1>>();
				            auto const wi_2D = device.get_info<sycl::info::device::max_work_item_sizes<2>>();
				            auto const wi_3D = device.get_info<sycl::info::device::max_work_item_sizes<3>>();
				            std::cout << "\t* 1D: (" << wi_1D.get(0) << ")\n";
				            std::cout << "\t* 2D: (" << wi_2D.get(0) << ", " << wi_2D.get(1) << ")\n";
				            std::cout << "\t* 3D: (" << wi_3D.get(0) << ", " << wi_3D.get(1) << ", " << wi_3D.get(2) << ")\n";

				            std::cout << "Maximum number of work items per work-group: "
				                      << device.get_info<sycl::info::device::max_work_group_size>() << '\n';

				            std::cout << "Maximum number of sub-groups per work-group: "
				                      << device.get_info<sycl::info::device::max_num_sub_groups>() << '\n';

				            std::cout << "Supported sub-group sizes: ";
				            auto const sg_sizes = device.get_info<sycl::info::device::sub_group_sizes>();
				            for(auto const& sz : sg_sizes)
				                std::cout << sz << ", ";
				            std::cout << '\n';

				            std::cout << "Preferred native vector width (char): "
				                      << device.get_info<sycl::info::device::preferred_vector_width_char>() << '\n';

				            std::cout << "Native ISA vector width (char): "
				                      << device.get_info<sycl::info::device::native_vector_width_char>() << '\n';

				            std::cout << "Preferred native vector width (short): "
				                      << device.get_info<sycl::info::device::preferred_vector_width_short>() << '\n';

				            std::cout << "Native ISA vector width (short): "
				                      << device.get_info<sycl::info::device::native_vector_width_short>() << '\n';

				            std::cout << "Preferred native vector width (int): "
				                      << device.get_info<sycl::info::device::preferred_vector_width_int>() << '\n';

				            std::cout << "Native ISA vector width (int): "
				                      << device.get_info<sycl::info::device::native_vector_width_int>() << '\n';

				            std::cout << "Preferred native vector width (long): "
				                      << device.get_info<sycl::info::device::preferred_vector_width_long>() << '\n';

				            std::cout << "Native ISA vector width (long): "
				                      << device.get_info<sycl::info::device::native_vector_width_long>() << '\n';

				            std::cout << "Preferred native vector width (float): "
				                      << device.get_info<sycl::info::device::preferred_vector_width_float>() << '\n';

				            std::cout << "Native ISA vector width (float): "
				                      << device.get_info<sycl::info::device::native_vector_width_float>() << '\n';

				            if(device.has_aspect(sycl::aspect::fp64))
				            {
				                std::cout << "Preferred native vector width (double): "
				                          << device.get_info<sycl::info::device::preferred_vector_width_double>() << '\n';

				                std::cout << "Native ISA vector width (double): "
				                          << device.get_info<sycl::info::device::native_vector_width_double>() << '\n';
				            }

				            if(device.has_aspect(sycl::aspect::fp16))
				            {
				                std::cout << "Preferred native vector width (half): "
				                          << device.get_info<sycl::info::device::preferred_vector_width_half>() << '\n';

				                std::cout << "Native ISA vector width (half): "
				                          << device.get_info<sycl::info::device::native_vector_width_half>() << '\n';
				            }

				            std::cout << "Maximum clock frequency: " << device.get_info<sycl::info::device::max_clock_frequency>()
				                      << " MHz\n";

				            std::cout << "Address space size: " << device.get_info<sycl::info::device::address_bits>() << "-bit\n";

				            std::cout << "Maximum size of memory object allocation: "
				                      << device.get_info<sycl::info::device::max_mem_alloc_size>() << " bytes\n";

				            if(device.has_aspect(sycl::aspect::image))
				            {
				                std::cout << "Maximum number of simultaneous image object reads per kernel: "
				                          << device.get_info<sycl::info::device::max_read_image_args>() << '\n';

				                std::cout << "Maximum number of simultaneous image writes per kernel: "
				                          << device.get_info<sycl::info::device::max_write_image_args>() << '\n';

				                std::cout << "Maximum 1D/2D image width: " << device.get_info<sycl::info::device::image2d_max_width>()
				                          << " px\n";

				                std::cout << "Maximum 2D image height: " << device.get_info<sycl::info::device::image2d_max_height>()
				                          << " px\n";

				                std::cout << "Maximum 3D image width: " << device.get_info<sycl::info::device::image3d_max_width>()
				                          << " px\n";

				                std::cout << "Maximum 3D image height: " << device.get_info<sycl::info::device::image3d_max_height>()
				                          << " px\n";

				                std::cout << "Maximum 3D image depth: " << device.get_info<sycl::info::device::image3d_max_depth>()
				                          << " px\n";

				                std::cout << "Maximum number of samplers per kernel: "
				                          << device.get_info<sycl::info::device::max_samplers>() << '\n';
				            }

				            std::cout << "Maximum kernel argument size: " << device.get_info<sycl::info::device::max_parameter_size>()
				                      << " bytes\n";

				            std::cout << "Memory base address alignment: "
				                      << device.get_info<sycl::info::device::mem_base_addr_align>() << " bit\n";

				            auto print_fp_config = [](std::string const& fp, std::vector<sycl::info::fp_config> const& conf)
				            {
				                std::cout << fp << " precision floating-point capabilities:\n";

				                auto find_and_print = [&](sycl::info::fp_config val)
				                {
				                    auto it = std::find(begin(conf), end(conf), val);
				                    std::cout << (it == std::end(conf) ? "No" : "Yes") << '\n';
				                };

				                std::cout << "\t* denorm support: ";
				                find_and_print(sycl::info::fp_config::denorm);

				                std::cout << "\t* INF & quiet NaN support: ";
				                find_and_print(sycl::info::fp_config::inf_nan);

				                std::cout << "\t* round to nearest even support: ";
				                find_and_print(sycl::info::fp_config::round_to_nearest);

				                std::cout << "\t* round to zero support: ";
				                find_and_print(sycl::info::fp_config::round_to_zero);

				                std::cout << "\t* round to infinity support: ";
				                find_and_print(sycl::info::fp_config::round_to_inf);

				                std::cout << "\t* IEEE754-2008 FMA support: ";
				                find_and_print(sycl::info::fp_config::fma);

				                std::cout << "\t* correctly rounded divide/sqrt support: ";
				                find_and_print(sycl::info::fp_config::correctly_rounded_divide_sqrt);

				                std::cout << "\t* software-implemented floating point operations: ";
				                find_and_print(sycl::info::fp_config::soft_float);
				            };

				            if(device.has_aspect(sycl::aspect::fp16))
				            {
				                auto const fp16_conf = device.get_info<sycl::info::device::half_fp_config>();
				                print_fp_config("Half", fp16_conf);
				            }

				            auto const fp32_conf = device.get_info<sycl::info::device::single_fp_config>();
				            print_fp_config("Single", fp32_conf);

				            if(device.has_aspect(sycl::aspect::fp64))
				            {
				                auto const fp64_conf = device.get_info<sycl::info::device::double_fp_config>();
				                print_fp_config("Double", fp64_conf);
				            }

				            std::cout << "Global memory cache type: ";
				            auto has_global_mem_cache = false;
				            switch(device.get_info<sycl::info::device::global_mem_cache_type>())
				            {
				            case sycl::info::global_mem_cache_type::none:
				                std::cout << "none";
				                break;

				            case sycl::info::global_mem_cache_type::read_only:
				                std::cout << "read-only";
				                has_global_mem_cache = true;
				                break;

				            case sycl::info::global_mem_cache_type::read_write:
				                std::cout << "read-write";
				                has_global_mem_cache = true;
				                break;
				            }
				            std::cout << '\n';

				            if(has_global_mem_cache)
				            {
				                std::cout << "Global memory cache line size: "
				                          << device.get_info<sycl::info::device::global_mem_cache_line_size>() << " bytes\n";

				                std::cout << "Global memory cache size: "
				                          << device.get_info<sycl::info::device::global_mem_cache_size>() / KiB << " KiB\n"
				            }

				            std::cout << "Global memory size: " << device.get_info<sycl::info::device::global_mem_size>() / MiB
				                      << " MiB" << std::endl;

				            std::cout << "Local memory type: ";
				            auto has_local_memory = false;
				            switch(device.get_info<sycl::info::device::local_mem_type>())
				            {
				            case sycl::info::local_mem_type::none:
				                std::cout << "none";
				                break;

				            case sycl::info::local_mem_type::local:
				                std::cout << "local";
				                has_local_memory = true;
				                break;

				            case sycl::info::local_mem_type::global:
				                std::cout << "global";
				                has_local_memory = true;
				                break;
				            }
				            std::cout << '\n';

				            if(has_local_memory)
				                std::cout << "Local memory size: " << device.get_info<sycl::info::device::local_mem_size>() / KiB
				                          << " KiB\n";

				            std::cout << "Error correction support: "
				                      << (device.get_info<sycl::info::device::error_correction_support>() ? "Yes" : "No") << '\n';

				            auto print_memory_orders = [](std::vector<sycl::memory_order> const& mem_orders)
				            {
				                for(auto const& cap : mem_orders)
				                {
				                    switch(cap)
				                    {
				                    case sycl::memory_order::relaxed:
				                        std::cout << "relaxed";
				                        break;

				                    case sycl::memory_order::acquire:
				                        std::cout << "acquire";
				                        break;

				                    case sycl::memory_order::release:
				                        std::cout << "release";
				                        break;

				                    case sycl::memory_order::acq_rel:
				                        std::cout << "acq_rel";
				                        break;

				                    case sycl::memory_order::seq_cst:
				                        std::cout << "seq_cst";
				                        break;
				                    }
				                    std::cout << ", ";
				                }
				                std::cout << '\n';
				            };

				            std::cout << "Supported memory orderings for atomic operations: ";
				            auto const mem_orders = device.get_info<sycl::info::device::atomic_memory_order_capabilities>();
				            print_memory_orders(mem_orders);

				            std::cout << "Supported memory orderings for sycl::atomic_fence: ";
				            auto const fence_orders = device.get_info<sycl::info::device::atomic_fence_order_capabilities>();
				            print_memory_orders(fence_orders);

				            auto print_memory_scopes = [](std::vector<sycl::memory_scope> const& mem_scopes)
				            {
				                for(auto const& cap : mem_scopes)
				                {
				                    switch(cap)
				                    {
				                    case sycl::memory_scope::work_item:
				                        std::cout << "work-item";
				                        break;

				                    case sycl::memory_scope::sub_group:
				                        std::cout << "sub-group";
				                        break;

				                    case sycl::memory_scope::work_group:
				                        std::cout << "work-group";
				                        break;

				                    case sycl::memory_scope::device:
				                        std::cout << "device";
				                        break;

				                    case sycl::memory_scope::system:
				                        std::cout << "system";
				                        break;
				                    }
				                    std::cout << ", ";
				                }
				                std::cout << '\n';
				            };

				            std::cout << "Supported memory scopes for atomic operations: ";
				            auto const mem_scopes = device.get_info<sycl::info::device::atomic_memory_scope_capabilities>();
				            print_memory_scopes(mem_scopes);

				            std::cout << "Supported memory scopes for sycl::atomic_fence: ";
				            auto const fence_scopes = device.get_info<sycl::info::device::atomic_fence_scope_capabilities>();
				            print_memory_scopes(fence_scopes);

				            std::cout << "Device timer resolution: "
				                      << device.get_info<sycl::info::device::profiling_timer_resolution>() << " ns\n";

				            std::cout << "Built-in kernels: ";
				            auto const builtins = device.get_info<sycl::info::device::built_in_kernel_ids>();
				            for(auto const& b : builtins)
				                std::cout << b.get_name() << ", ";
				            std::cout << '\n';

				            std::cout << "Maximum number of subdevices: ";
				            auto const max_subs = device.get_info<sycl::info::device::partition_max_sub_devices>();
				            std::cout << max_subs << '\n';

				            if(max_subs > 1)
				            {
				                std::cout << "Supported partition properties: ";
				                auto const part_props = device.get_info<sycl::info::device::partition_properties>();
				                auto has_affinity_domains = false;
				                for(auto const& prop : part_props)
				                {
				                    switch(prop)
				                    {
				                    case sycl::info::partition_property::no_partition:
				                        std::cout << "no partition";
				                        break;

				                    case sycl::info::partition_property::partition_equally:
				                        std::cout << "equally";
				                        break;

				                    case sycl::info::partition_property::partition_by_counts:
				                        std::cout << "by counts";
				                        break;

				                    case sycl::info::partition_property::partition_by_affinity_domain:
				                        std::cout << "by affinity domain";
				                        has_affinity_domains = true;
				                        break;
				                    }
				                    std::cout << ", ";
				                }
				                std::cout << '\n';

				                if(has_affinity_domains)
				                {
				                    std::cout << "Supported partition affinity domains: ";
				                    auto const aff_doms = device.get_info<sycl::info::device::partition_affinity_domains>();
				                    for(auto const& dom : aff_doms)
				                    {
				                        switch(dom)
				                        {
				                        case sycl::info::partition_affinity_domain::not_applicable:
				                            std::cout << "not applicable";
				                            break;

				                        case sycl::info::partition_affinity_domain::numa:
				                            std::cout << "NUMA";
				                            break;

				                        case sycl::info::partition_affinity_domain::L4_cache:
				                            std::cout << "L4 cache";
				                            break;

				                        case sycl::info::partition_affinity_domain::L3_cache:
				                            std::cout << "L3 cache";
				                            break;

				                        case sycl::info::partition_affinity_domain::L2_cache:
				                            std::cout << "L2 cache";
				                            break;

				                        case sycl::info::partition_affinity_domain::L1_cache:
				                            std::cout << "L1 cache";
				                            break;

				                        case sycl::info::partition_affinity_domain::next_partitionable:
				                            std::cout << "next partitionable";
				                            break;
				                        }
				                        std::cout << ", ";
				                    }
				                    std::cout << '\n';
				                }

				                std::cout << "Current partition property: ";
				                switch(device.get_info<sycl::info::device::partition_type_property>())
				                {
				                case sycl::info::partition_property::no_partition:
				                    std::cout << "no partition";
				                    break;

				                case sycl::info::partition_property::partition_equally:
				                    std::cout << "partitioned equally";
				                    break;

				                case sycl::info::partition_property::partition_by_counts:
				                    std::cout << "partitioned by counts";
				                    break;

				                case sycl::info::partition_property::partition_by_affinity_domain:
				                    std::cout << "partitioned by affinity domain";
				                    break;
				                }
				                std::cout << '\n';

				                std::cout << "Current partition affinity domain: ";
				                switch(device.get_info<sycl::info::device::partition_type_affinity_domain>())
				                {
				                case sycl::info::partition_affinity_domain::not_applicable:
				                    std::cout << "not applicable";
				                    break;

				                case sycl::info::partition_affinity_domain::numa:
				                    std::cout << "NUMA";
				                    break;

				                case sycl::info::partition_affinity_domain::L4_cache:
				                    std::cout << "L4 cache";
				                    break;

				                case sycl::info::partition_affinity_domain::L3_cache:
				                    std::cout << "L3 cache";
				                    break;

				                case sycl::info::partition_affinity_domain::L2_cache:
				                    std::cout << "L2 cache";
				                    break;

				                case sycl::info::partition_affinity_domain::L1_cache:
				                    std::cout << "L1 cache";
				                    break;

				                case sycl::info::partition_affinity_domain::next_partitionable:
				                    std::cout << "next partitionable";
				                    break;
				                }
				                std::cout << '\n';
				            }

				            std::cout.flush();
				        }
				#    endif
				    };
				} // namespace alpaka::trait

				#endif
				// ==
				// == ./include/alpaka/pltf/PltfGenericSycl.hpp ==
				// ============================================================================


			// #include <string>    // amalgamate: file already included

			#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_CPU)

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka
			{
			    namespace detail
			    {
			        // Prevent clang from annoying us with warnings about emitting too many vtables. These are discarded by the
			        // linker anyway.
			#    if BOOST_COMP_CLANG
			#        pragma clang diagnostic push
			#        pragma clang diagnostic ignored "-Wweak-vtables"
			#    endif
			        struct IntelCpuSelector final
			        {
			            auto operator()(sycl::device const& dev) const -> int
			            {
			                auto const& vendor = dev.get_info<sycl::info::device::vendor>();
			                auto const is_intel_cpu = (vendor.find("Intel(R) Corporation") != std::string::npos) && dev.is_cpu();

			                return is_intel_cpu ? 1 : -1;
			            }
			        };
			#    if BOOST_COMP_CLANG
			#        pragma clang diagnostic pop
			#    endif
			    } // namespace detail

			    //! The SYCL device manager.
			    using PltfCpuSyclIntel = PltfGenericSycl<detail::IntelCpuSelector>;
			} // namespace alpaka

			namespace alpaka::trait
			{
			    //! The SYCL device manager device type trait specialization.
			    template<>
			    struct DevType<PltfCpuSyclIntel>
			    {
			        using type = DevGenericSycl<PltfCpuSyclIntel>; // = DevCpuSyclIntel
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/pltf/PltfCpuSyclIntel.hpp ==
			// ============================================================================


		#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_CPU)

		namespace alpaka
		{
		    using DevCpuSyclIntel = DevGenericSycl<PltfCpuSyclIntel>;
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/dev/DevCpuSyclIntel.hpp ==
		// ============================================================================

	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/kernel/TaskKernelCpuSyclIntel.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
			// ============================================================================
			// == ./include/alpaka/kernel/TaskKernelGenericSycl.hpp ==
			// ==
			/* Copyright 2023 Jan Stephan, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/core/STLTuple/STLTuple.hpp ==
				// ==
				/***************************************************************************
				 *
				 *  Copyright (C) 2018 Codeplay Software Limited
				 *  Licensed under the Apache License, Version 2.0 (the "License");
				 *  you may not use this file except in compliance with the License.
				 *  You may obtain a copy of the License at
				 *
				 *      http://www.apache.org/licenses/LICENSE-2.0
				 *
				 *  For your convenience, a copy of the License has been included in this
				 *  repository.
				 *
				 *  Unless required by applicable law or agreed to in writing, software
				 *  distributed under the License is distributed on an "AS IS" BASIS,
				 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
				 *  See the License for the specific language governing permissions and
				 *  limitations under the License.
				 *
				 * STLTuple.h
				 *
				 * \brief:
				 *  Minimal implementation of std::tuple that is standard layout.
				 *
				  Authors:
				 *
				 *    Mehdi Goli    Codeplay Software Ltd.
				 *    Ralph Potter  Codeplay Software Ltd.
				 *    Luke Iwanski  Codeplay Software Ltd.
				 *
				 **************************************************************************/

				// clang-format off
				// #pragma once
				// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

				// suppress warnings as this is third-party code
				#if BOOST_COMP_CLANG
				#   pragma clang diagnostic push
				#   pragma clang diagnostic ignored "-Wdocumentation"
				#   pragma clang diagnostic ignored "-Wdocumentation-unknown-command"
				#endif
				#if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
				#    pragma warning(push)
				#    pragma warning(disable : 4003) // not enough arguments for function-like macro invocation
				#endif

				namespace utility {
				namespace tuple {
				/// \struct StaticIf
				/// \brief The StaticIf struct is used to statically choose the type based on
				/// the condition.
				template <bool, typename T = void>
				struct StaticIf;
				/// \brief specialisation of the \ref StaticIf when the condition is true
				template <typename T>
				struct StaticIf<true, T> {
				  typedef T type;
				};

				/// \struct Tuple
				/// \brief is a fixed-size collection of heterogeneous values
				/// \ztparam Ts...	-	the types of the elements that the tuple stores.
				/// Empty list is supported.
				template <class... Ts>
				struct Tuple {};

				/// \brief specialisation of the \ref Tuple class when the tuple has at least
				/// one element.
				/// \tparam T : the type of the first element in the tuple.
				/// \tparam Ts... the rest of the elements in the tuple. Ts... can be empty.
				template <class T, class... Ts>
				struct Tuple<T, Ts...> {
				  Tuple(T t, Ts... ts) : head(t), tail(ts...) {}
				  T head;
				  Tuple<Ts...> tail;
				};

				template <typename, typename>
				struct concat_tuple {};

				template <typename... Ts, typename... Us>
				struct concat_tuple<Tuple<Ts...>, Tuple<Us...>> {
				  using type = Tuple<Ts..., Us...>;
				};

				template <typename T>
				struct remove_last_type;

				template <typename T>
				struct remove_last_type<Tuple<T>> {
				  using type = Tuple<>;
				};

				template <typename T, typename... Args>
				struct remove_last_type<Tuple<T, Args...>> {
				  using type = typename concat_tuple<
				      Tuple<T>, typename remove_last_type<Tuple<Args...>>::type>::type;
				};

				template <typename T>
				struct remove_first_type {};

				template <typename T, typename... Ts>
				struct remove_first_type<Tuple<T, Ts...>> {
				  typedef Tuple<Ts...> type;
				};

				///\ struct ElemTypeHolder
				/// \brief ElemTypeHolder class is used to specify the types of the
				/// elements inside the tuple
				/// \tparam size_t the number of elements inside the tuple
				/// \tparam class the tuple class
				template <size_t, class>
				struct ElemTypeHolder;

				/// \brief specialisation of the \ref ElemTypeHolder class when the number of
				/// elements inside the tuple is 1
				template <class T, class... Ts>
				struct ElemTypeHolder<0, Tuple<T, Ts...>> {
				  typedef T type;
				};

				/// \brief specialisation of the \ref ElemTypeHolder class when the number of
				/// elements inside the tuple is bigger than 1. It recursively calls itself to
				/// detect the type of each element in the tuple
				/// \tparam T : the type of the first element in the tuple.
				/// \tparam Ts... the rest of the elements in the tuple. Ts... can be empty.
				/// \tparam K is the Kth element in the tuple
				template <size_t k, class T, class... Ts>
				struct ElemTypeHolder<k, Tuple<T, Ts...>> {
				  typedef typename ElemTypeHolder<k - 1, Tuple<Ts...>>::type type;
				};

				/// get
				/// \brief Extracts the first element from the tuple.
				/// K=0 represents the first element of the tuple. The tuple cannot be empty.
				/// \tparam Ts... are the type of the elements in the tuple.
				/// \param t is the tuple whose contents to extract
				/// \return  typename ElemTypeHolder<0, Tuple<Ts...> >::type &>::type

				#define TERMINATE_CONDS_TUPLE_GET(CVQual)                                      \
				  template <size_t k, class... Ts>                                             \
				  typename StaticIf<k == 0, CVQual                                             \
				                    typename ElemTypeHolder<0, Tuple<Ts...>>::type&>::type     \
				  get(CVQual Tuple<Ts...>& t) {                                                \
				    static_assert(sizeof...(Ts) != 0,                                          \
				                  "The requseted value is bigger than the size of the tuple"); \
				    return t.head;                                                             \
				  }

				TERMINATE_CONDS_TUPLE_GET(const)
				TERMINATE_CONDS_TUPLE_GET()
				#undef TERMINATE_CONDS_TUPLE_GET
				/// get
				/// \brief Extracts the Kth element from the tuple.
				///\tparam K is an integer value in [0,sizeof...(Types)).
				/// \tparam T is the (sizeof...(Types) -(K+1)) element in the tuple
				/// \tparam Ts... are the type of the elements  in the tuple.
				/// \param t is the tuple whose contents to extract
				/// \return  typename ElemTypeHolder<K, Tuple<Ts...> >::type &>::type
				#define RECURSIVE_TUPLE_GET(CVQual)                                           \
				  template <size_t k, class T, class... Ts>                                   \
				  typename StaticIf<k != 0, CVQual                                            \
				                    typename ElemTypeHolder<k, Tuple<T, Ts...>>::type&>::type \
				  get(CVQual Tuple<T, Ts...>& t) {                                            \
				    return utility::tuple::get<k - 1>(t.tail);                                \
				  }
				RECURSIVE_TUPLE_GET(const)
				RECURSIVE_TUPLE_GET()
				#undef RECURSIVE_TUPLE_GET

				/// make_tuple
				/// \brief Creates a tuple object, deducing the target type from the types of
				/// arguments.
				/// \tparam Args the type of the arguments to construct the tuple from
				/// \param args zero or more arguments to construct the tuple from
				/// \return Tuple<Args...>
				template <typename... Args>
				Tuple<Args...> make_tuple(Args... args) {
				  return Tuple<Args...>(args...);
				}

				/// size
				/// \brief Provides access to the number of elements in a tuple as a
				/// compile-time constant expression.
				/// \tparam Args the type of the arguments to construct the tuple from
				/// \return size_t
				template <typename... Args>
				static constexpr size_t size(Tuple<Args...>&) {
				  return sizeof...(Args);
				}

				/// \struct IndexList
				/// \brief Creates a list of index from the elements in the tuple
				/// \tparam Is... a list of index from [0 to sizeof...(tuple elements))
				template <size_t... Is>
				struct IndexList {};

				/// \struct RangeBuilder
				/// \brief Collects internal details for generating index ranges [MIN, MAX)
				/// Declare primary template for index range builder
				/// \tparam MIN is the starting index in the tuple
				/// \tparam N represents sizeof..(elemens)- sizeof...(Is)
				/// \tparam Is... are the list of generated index so far
				template <size_t MIN, size_t N, size_t... Is>
				struct RangeBuilder;

				/// \brief base Step: Specialisation of the \ref RangeBuilder when the
				/// MIN==MAX. In this case the Is... is [0 to sizeof...(tuple elements))
				/// \tparam MIN is the starting index of the tuple
				/// \tparam Is is [0 to sizeof...(tuple elements))
				template <size_t MIN, size_t... Is>
				struct RangeBuilder<MIN, MIN, Is...> {
				  typedef IndexList<Is...> type;
				};

				/// Induction step: Specialisation of the RangeBuilder class when N!=MIN
				/// in this case we are recursively subtracting N by one and adding one
				/// index to Is... list until MIN==N
				/// \tparam MIN is the starting index in the tuple
				/// \tparam N represents sizeof..(elemens)- sizeof...(Is)
				/// \tparam Is... are the list of generated index so far
				template <size_t MIN, size_t N, size_t... Is>
				struct RangeBuilder : public RangeBuilder<MIN, N - 1, N - 1, Is...> {};

				/// \brief IndexRange that returns a [MIN, MAX) index range
				/// \tparam MIN is the starting index in the tuple
				/// \tparam MAX is the size of the tuple
				template <size_t MIN, size_t MAX>
				struct IndexRange : RangeBuilder<MIN, MAX>::type {};

				/// append_base
				/// \brief unpacking the elements of the input tuple t and creating a new tuple
				/// by adding element a at the end of it.
				///\tparam Args... the type of the elements inside the tuple t
				/// \tparam T the type of the new element going to be added at the end of tuple
				/// \tparam I... is the list of index from [0 to sizeof...(t))
				/// \param t the tuple on which we want to append a.
				/// \param a the new elements going to be added to the tuple
				/// \return Tuple<Args..., T>
				template <typename... Args, typename T, size_t... I>
				Tuple<Args..., T> append_base(Tuple<Args...> t, T a, IndexList<I...>) {
				  return utility::tuple::make_tuple(get<I>(t)..., a);
				}

				/// append
				/// \brief the deduction function for \ref append_base that automatically
				/// generate the \ref IndexRange
				///\tparam Args... the type of the elements inside the tuple t
				/// \tparam T the type of the new element going to be added at the end of tuple
				/// \param t the tuple on which we want to append a.
				/// \param a the new elements going to be added to the tuple
				/// \return Tuple<Args..., T>
				template <typename... Args, typename T>
				Tuple<Args..., T> append(Tuple<Args...> t, T a) {
				  return utility::tuple::append_base(t, a, IndexRange<0, sizeof...(Args)>());
				}

				/// append_base
				/// \brief This is a specialisation of \ref append_base when we want to
				/// concatenate
				/// tuple t2 at the end of the tuple t1. Here we unpack both tuples, generate
				/// the IndexRange for each of them and create an output tuple T that contains
				/// both elements of t1 and t2.
				///\tparam Args1... the type of the elements inside the tuple t1
				///\tparam Args2... the type of the elements inside the tuple t2
				/// \tparam I1... is the list of index from [0 to sizeof...(t1))
				/// \tparam I2... is the list of index from [0 to sizeof...(t2))
				/// \param t1 is the tuple on which we want to append t2.
				/// \param t2 is the tuple that is going to be added on t1.
				/// \return Tuple<Args1..., Args2...>
				template <typename... Args1, typename... Args2, size_t... I1, size_t... I2>
				Tuple<Args1..., Args2...> append_base(Tuple<Args1...> t1, Tuple<Args2...> t2,
				                                      IndexList<I1...>, IndexList<I2...>) {
				  return utility::tuple::make_tuple(get<I1>(t1)..., get<I2>(t2)...);
				}

				/// append
				/// \brief deduction function for \ref append_base when we are appending tuple
				/// t1 by tuple t2. In this case the \ref IndexRange for both tuple are
				/// automatically generated.
				///\tparam Args1... the type of the elements inside the tuple t1
				///\tparam Args2... the type of the elements inside the tuple t2
				/// \param t1 is the tuple on which we want to append t2.
				/// \param t2 is the tuple that is going to be added on t1.
				/// \return Tuple<Args1..., Args2...>
				template <typename... Args1, typename... Args2>
				Tuple<Args1..., Args2...> append(Tuple<Args1...> t1, Tuple<Args2...> t2) {
				  return utility::tuple::append_base(t1, t2, IndexRange<0, sizeof...(Args1)>(),
				                                     IndexRange<0, sizeof...(Args2)>());
				}

				}  // namespace tuple
				}  // namespace utility

				#if BOOST_COMP_CLANG
				#   pragma clang diagnostic pop
				#endif
				#if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
				#    pragma warning(pop)
				#endif
				// ==
				// == ./include/alpaka/core/STLTuple/STLTuple.hpp ==
				// ============================================================================

			// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/core/Tuple.hpp ==
				// ==
				/* Copyright 2022 Jeffrey Kelling, Jan Stephan, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/STLTuple/STLTuple.hpp"    // amalgamate: file already expanded

				// #include <cstddef>    // amalgamate: file already included
				// #include <utility>    // amalgamate: file already included

				namespace alpaka::core
				{
				    using namespace ::utility::tuple;

				    namespace detail
				    {
				        template<typename TFunc, typename... TArgs, std::size_t... Is>
				        constexpr auto apply_impl(TFunc&& f, Tuple<TArgs...>&& t, std::index_sequence<Is...>)
				        {
				            return f(get<Is>(std::forward<Tuple<TArgs...>&&>(t))...);
				        }
				    } // namespace detail

				    template<typename TFunc, typename... TArgs>
				    constexpr auto apply(TFunc&& f, Tuple<TArgs...> t)
				    {
				        return detail::apply_impl(
				            std::forward<TFunc>(f),
				            std::forward<Tuple<TArgs...>&&>(t),
				            std::make_index_sequence<sizeof...(TArgs)>{});
				    }
				} // namespace alpaka::core
				// ==
				// == ./include/alpaka/core/Tuple.hpp ==
				// ============================================================================

			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/mem/buf/sycl/Accessor.hpp ==
				// ==
				/* Copyright 2023 Jan Stephan, Andrea Bocci
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/mem/view/Accessor.hpp ==
					// ==
					/* Copyright 2022 Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/core/Utility.hpp"    // amalgamate: file already expanded
					// #include "alpaka/mem/buf/Traits.hpp"    // amalgamate: file already expanded
					// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
					// #include "alpaka/meta/DependentFalseType.hpp"    // amalgamate: file already expanded
						// ============================================================================
						// == ./include/alpaka/meta/TypeListOps.hpp ==
						// ==
						/* Copyright 2022 Bernhard Manfred Gruber
						 * SPDX-License-Identifier: MPL-2.0
						 */

						// #pragma once
						// #include <type_traits>    // amalgamate: file already included

						namespace alpaka::meta
						{
						    namespace detail
						    {
						        template<typename List>
						        struct Front
						        {
						        };

						        template<template<typename...> class List, typename Head, typename... Tail>
						        struct Front<List<Head, Tail...>>
						        {
						            using type = Head;
						        };
						    } // namespace detail

						    template<typename List>
						    using Front = typename detail::Front<List>::type;

						    template<typename List, typename Value>
						    struct Contains : std::false_type
						    {
						    };

						    template<template<typename...> class List, typename Head, typename... Tail, typename Value>
						    struct Contains<List<Head, Tail...>, Value>
						    {
						        static constexpr bool value = std::is_same_v<Head, Value> || Contains<List<Tail...>, Value>::value;
						    };
						} // namespace alpaka::meta
						// ==
						// == ./include/alpaka/meta/TypeListOps.hpp ==
						// ============================================================================


					// #include <tuple>    // amalgamate: file already included

					namespace alpaka::experimental
					{
					    //! Access tag type indicating read-only access.
					    struct ReadAccess
					    {
					    };

					    //! Access tag type indicating write-only access.
					    struct WriteAccess
					    {
					    };

					    //! Access tag type indicating read-write access.
					    struct ReadWriteAccess
					    {
					    };

					    //! An accessor is an abstraction for accessing memory objects such as views and buffers.
					    //! @tparam TMemoryHandle A handle to a memory object.
					    //! @tparam TElem The type of the element stored by the memory object. Values and references to this type are
					    //! returned on access.
					    //! @tparam TBufferIdx The integral type used for indexing and index computations.
					    //! @tparam TDim The dimensionality of the accessed data.
					    //! @tparam TAccessModes Either a single access tag type or a `std::tuple` containing multiple access tag
					    //! types.
					    template<typename TMemoryHandle, typename TElem, typename TBufferIdx, std::size_t TDim, typename TAccessModes>
					    struct Accessor;

					    namespace trait
					    {
					        //! The customization point for how to build an accessor for a given memory object.
					        template<typename TMemoryObject, typename SFINAE = void>
					        struct BuildAccessor
					        {
					            template<typename... TAccessModes, typename TMemoryObjectForwardRef>
					            ALPAKA_FN_HOST static auto buildAccessor(TMemoryObjectForwardRef&&)
					            {
					                static_assert(
					                    meta::DependentFalseType<TMemoryObject>::value,
					                    "BuildAccessor<TMemoryObject> is not specialized for your TMemoryObject.");
					            }
					        };
					    } // namespace trait

					    namespace internal
					    {
					        template<typename AccessorOrBuffer>
					        struct MemoryHandle
					        {
					        };

					        template<typename TMemoryHandle, typename TElem, typename TBufferIdx, std::size_t TDim, typename TAccessModes>
					        struct MemoryHandle<Accessor<TMemoryHandle, TElem, TBufferIdx, TDim, TAccessModes>>
					        {
					            using type = TMemoryHandle;
					        };
					    } // namespace internal

					    /// Get the memory handle type of the given accessor or buffer type.
					    template<typename Accessor>
					    using MemoryHandle = typename internal::MemoryHandle<Accessor>::type;

					    namespace internal
					    {
					        template<typename T>
					        struct IsAccessor : std::false_type
					        {
					        };

					        template<typename TMemoryHandle, typename TElem, typename TBufferIdx, std::size_t Dim, typename TAccessModes>
					        struct IsAccessor<Accessor<TMemoryHandle, TElem, TBufferIdx, Dim, TAccessModes>> : std::true_type
					        {
					        };
					    } // namespace internal

					    //! Creates an accessor for the given memory object using the specified access modes. Memory objects are e.g.
					    //! alpaka views and buffers.
					    template<
					        typename... TAccessModes,
					        typename TMemoryObject,
					        typename = std::enable_if_t<!internal::IsAccessor<std::decay_t<TMemoryObject>>::value>>
					    ALPAKA_FN_HOST auto accessWith(TMemoryObject&& memoryObject)
					    {
					        return trait::BuildAccessor<std::decay_t<TMemoryObject>>::template buildAccessor<TAccessModes...>(
					            memoryObject);
					    }

					    //! Constrains an existing accessor with multiple access modes to the specified access modes.
					    // TODO: currently only allows constraining down to 1 access mode
					    template<
					        typename TNewAccessMode,
					        typename TMemoryHandle,
					        typename TElem,
					        typename TBufferIdx,
					        std::size_t TDim,
					        typename... TPrevAccessModes>
					    ALPAKA_FN_HOST auto accessWith(
					        Accessor<TMemoryHandle, TElem, TBufferIdx, TDim, std::tuple<TPrevAccessModes...>> const& acc)
					    {
					        static_assert(
					            meta::Contains<std::tuple<TPrevAccessModes...>, TNewAccessMode>::value,
					            "The accessed accessor must already contain the requested access mode");
					        return Accessor<TMemoryHandle, TElem, TBufferIdx, TDim, TNewAccessMode>{acc};
					    }

					    //! Constrains an existing accessor to the specified access modes.
					    // constraining accessor to the same access mode again just passes through
					    template<typename TNewAccessMode, typename TMemoryHandle, typename TElem, typename TBufferIdx, std::size_t TDim>
					    ALPAKA_FN_HOST auto accessWith(Accessor<TMemoryHandle, TElem, TBufferIdx, TDim, TNewAccessMode> const& acc)
					    {
					        return acc;
					    }

					    //! Creates a read-write accessor for the given memory object (view, buffer, ...) or accessor.
					    template<typename TMemoryObjectOrAccessor>
					    ALPAKA_FN_HOST auto access(TMemoryObjectOrAccessor&& viewOrAccessor)
					    {
					        return accessWith<ReadWriteAccess>(std::forward<TMemoryObjectOrAccessor>(viewOrAccessor));
					    }

					    //! Creates a read-only accessor for the given memory object (view, buffer, ...) or accessor.
					    template<typename TMemoryObjectOrAccessor>
					    ALPAKA_FN_HOST auto readAccess(TMemoryObjectOrAccessor&& viewOrAccessor)
					    {
					        return accessWith<ReadAccess>(std::forward<TMemoryObjectOrAccessor>(viewOrAccessor));
					    }

					    //! Creates a write-only accessor for the given memory object (view, buffer, ...) or accessor.
					    template<typename TMemoryObjectOrAccessor>
					    ALPAKA_FN_HOST auto writeAccess(TMemoryObjectOrAccessor&& viewOrAccessor)
					    {
					        return accessWith<WriteAccess>(std::forward<TMemoryObjectOrAccessor>(viewOrAccessor));
					    }

					    //! An alias for an accessor accessing a buffer on the given accelerator.
					    template<
					        typename TAcc,
					        typename TElem,
					        std::size_t TDim,
					        typename TAccessModes = ReadWriteAccess,
					        typename TIdx = Idx<TAcc>>
					    using BufferAccessor = Accessor<
					        MemoryHandle<decltype(accessWith<TAccessModes>(core::declval<Buf<TAcc, TElem, DimInt<TDim>, TIdx>>()))>,
					        TElem,
					        TIdx,
					        TDim,
					        TAccessModes>;
					} // namespace alpaka::experimental
					// ==
					// == ./include/alpaka/mem/view/Accessor.hpp ==
					// ============================================================================

					// ============================================================================
					// == ./include/alpaka/mem/view/ViewAccessor.hpp ==
					// ==
					/* Copyright 2022 Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
					// #include "alpaka/mem/view/Accessor.hpp"    // amalgamate: file already expanded

					// #include <type_traits>    // amalgamate: file already included

					namespace alpaka::experimental
					{
					    namespace internal
					    {
					        template<typename T>
					        ALPAKA_FN_HOST_ACC auto asBytePtr(T* p)
					        {
					            return reinterpret_cast<char*>(p);
					        }

					        template<typename T>
					        struct WriteOnlyProxy
					        {
					            ALPAKA_FN_HOST_ACC WriteOnlyProxy(T& location) : loc(location)
					            {
					            }

					            template<typename U>
					            ALPAKA_FN_HOST_ACC auto operator=(U&& value) -> auto&
					            {
					                loc = std::forward<U>(value);
					                return *this;
					            }

					        private:
					            T& loc;
					        };

					        template<typename TElem, typename TAccessModes>
					        struct AccessReturnTypeImpl;

					        template<typename TElem>
					        struct AccessReturnTypeImpl<TElem, ReadAccess>
					        {
					            using type = TElem;
					        };

					        template<typename TElem>
					        struct AccessReturnTypeImpl<TElem, WriteAccess>
					        {
					            using type = WriteOnlyProxy<TElem>;
					        };

					        template<typename TElem>
					        struct AccessReturnTypeImpl<TElem, ReadWriteAccess>
					        {
					            using type = TElem&;
					        };

					        template<typename TElem, typename THeadAccessMode, typename... TTailAccessModes>
					        struct AccessReturnTypeImpl<TElem, std::tuple<THeadAccessMode, TTailAccessModes...>>
					            : AccessReturnTypeImpl<TElem, THeadAccessMode>
					        {
					        };

					        template<typename TElem, typename TAccessModes>
					        using AccessReturnType = typename internal::AccessReturnTypeImpl<TElem, TAccessModes>::type;
					    } // namespace internal

					    //! 1D accessor to memory objects represented by a pointer.
					    // We keep this specialization to not store the zero-dim pitch vector and provide one more operator[].
					    template<typename TElem, typename TBufferIdx, typename TAccessModes>
					    struct Accessor<TElem*, TElem, TBufferIdx, 1, TAccessModes>
					    {
					        using ReturnType = internal::AccessReturnType<TElem, TAccessModes>;

					        ALPAKA_FN_HOST_ACC Accessor(
					            TElem* p_,
					            Vec<DimInt<0>, TBufferIdx> pitchesInBytes_,
					            Vec<DimInt<1>, TBufferIdx> extents_)
					            : p(p_)
					            , extents(extents_)
					        {
					            (void) pitchesInBytes_;
					        }

					        template<typename TOtherAccessModes>
					        ALPAKA_FN_HOST_ACC Accessor(Accessor<TElem*, TElem, TBufferIdx, 1, TOtherAccessModes> const& other)
					            : p(other.p)
					            , extents(other.extents)
					        {
					        }

					        ALPAKA_FN_HOST_ACC auto operator[](Vec<DimInt<1>, TBufferIdx> i) const -> ReturnType
					        {
					            return (*this)(i[0]);
					        }

					        ALPAKA_FN_HOST_ACC auto operator[](TBufferIdx i) const -> ReturnType
					        {
					            return (*this)(i);
					        }

					        ALPAKA_FN_HOST_ACC auto operator()(TBufferIdx i) const -> ReturnType
					        {
					            return p[i];
					        }

					        TElem* p;
					        Vec<DimInt<1>, TBufferIdx> extents;
					    };

					    //! Higher than 1D accessor to memory objects represented by a pointer.
					    template<typename TElem, typename TBufferIdx, std::size_t TDim, typename TAccessModes>
					    struct Accessor<TElem*, TElem, TBufferIdx, TDim, TAccessModes>
					    {
					        using ReturnType = internal::AccessReturnType<TElem, TAccessModes>;

					        ALPAKA_FN_HOST_ACC Accessor(
					            TElem* p_,
					            Vec<DimInt<TDim - 1>, TBufferIdx> pitchesInBytes_,
					            Vec<DimInt<TDim>, TBufferIdx> extents_)
					            : p(p_)
					            , pitchesInBytes(pitchesInBytes_)
					            , extents(extents_)
					        {
					        }

					        template<typename TOtherAccessModes>
					        ALPAKA_FN_HOST_ACC Accessor(Accessor<TElem*, TElem, TBufferIdx, TDim, TOtherAccessModes> const& other)
					            : p(other.p)
					            , pitchesInBytes(other.pitchesInBytes)
					            , extents(other.extents)
					        {
					        }

					    private:
					        template<std::size_t... TIs>
					        [[nodiscard]] ALPAKA_FN_HOST_ACC auto subscript(Vec<DimInt<TDim>, TBufferIdx> index) const -> ReturnType
					        {
					#if BOOST_COMP_GNUC
					#    pragma GCC diagnostic push
					#    pragma GCC diagnostic ignored "-Wcast-align"
					#endif
					            auto bp = internal::asBytePtr(p);
					            for(std::size_t i = 0u; i < TDim; i++)
					            {
					                auto const pitch = i < TDim - 1 ? pitchesInBytes[i] : static_cast<TBufferIdx>(sizeof(TElem));
					                bp += index[i] * pitch;
					            }
					            return *reinterpret_cast<TElem*>(bp);
					#if BOOST_COMP_GNUC
					#    pragma GCC diagnostic pop
					#endif
					        }

					    public:
					        ALPAKA_FN_HOST_ACC auto operator[](Vec<DimInt<TDim>, TBufferIdx> i) const -> ReturnType
					        {
					            return subscript(i);
					        }

					        template<typename... Ts>
					        ALPAKA_FN_HOST_ACC auto operator()(Ts... i) const -> ReturnType
					        {
					            static_assert(sizeof...(Ts) == TDim, "You need to specify TDim indices.");
					            return subscript(Vec<DimInt<TDim>, TBufferIdx>{static_cast<TBufferIdx>(i)...});
					        }

					        TElem* p;
					        Vec<DimInt<TDim - 1>, TBufferIdx> pitchesInBytes;
					        Vec<DimInt<TDim>, TBufferIdx> extents;
					    };

					    namespace trait
					    {
					        namespace internal
					        {
					            template<typename T, typename SFINAE = void>
					            struct IsView : std::false_type
					            {
					            };

					            // TODO: replace this by a concept in C++20
					            template<typename TView>
					            struct IsView<
					                TView,
					                std::void_t<
					                    Idx<TView>,
					                    Dim<TView>,
					                    decltype(getPtrNative(std::declval<TView>())),
					                    decltype(getPitchBytes<0>(std::declval<TView>())),
					                    decltype(getExtent<0>(std::declval<TView>()))>> : std::true_type
					            {
					            };

					            template<typename... TAccessModes>
					            struct BuildAccessModeList;

					            template<typename TAccessMode>
					            struct BuildAccessModeList<TAccessMode>
					            {
					                using type = TAccessMode;
					            };

					            template<typename TAccessMode1, typename TAccessMode2, typename... TAccessModes>
					            struct BuildAccessModeList<TAccessMode1, TAccessMode2, TAccessModes...>
					            {
					                using type = std::tuple<TAccessMode1, TAccessMode2, TAccessModes...>;
					            };

					            template<
					                typename... TAccessModes,
					                typename TViewForwardRef,
					                std::size_t... TPitchIs,
					                std::size_t... TExtentIs>
					            ALPAKA_FN_HOST auto buildViewAccessor(
					                TViewForwardRef&& view,
					                std::index_sequence<TPitchIs...>,
					                std::index_sequence<TExtentIs...>)
					            {
					                using TView = std::decay_t<TViewForwardRef>;
					                static_assert(IsView<TView>::value);
					                using TBufferIdx = Idx<TView>;
					                constexpr auto dim = Dim<TView>::value;
					                using Elem = Elem<TView>;
					                auto p = getPtrNative(view);
					                static_assert(
					                    std::is_same_v<decltype(p), Elem const*> || std::is_same_v<decltype(p), Elem*>,
					                    "We assume that getPtrNative() returns a raw pointer to the view's elements");
					                static_assert(
					                    !std::is_same_v<
					                        decltype(p),
					                        Elem const*> || std::is_same_v<std::tuple<TAccessModes...>, std::tuple<ReadAccess>>,
					                    "When getPtrNative() returns a const raw pointer, the access mode must be ReadAccess");
					                using AccessModeList = typename BuildAccessModeList<TAccessModes...>::type;
					                return Accessor<Elem*, Elem, TBufferIdx, dim, AccessModeList>{
					                    const_cast<Elem*>(p), // strip constness, this is handled the the access modes
					                    {getPitchBytes<TPitchIs + 1>(view)...},
					                    {getExtent<TExtentIs>(view)...}};
					            }
					        } // namespace internal

					        //! Builds an accessor from view like memory objects.
					        template<typename TView>
					        struct BuildAccessor<TView, std::enable_if_t<internal::IsView<TView>::value>>
					        {
					            template<typename... TAccessModes, typename TViewForwardRef>
					            ALPAKA_FN_HOST static auto buildAccessor(TViewForwardRef&& view)
					            {
					                using Dim = Dim<std::decay_t<TView>>;
					                return internal::buildViewAccessor<TAccessModes...>(
					                    std::forward<TViewForwardRef>(view),
					                    std::make_index_sequence<Dim::value - 1>{},
					                    std::make_index_sequence<Dim::value>{});
					            }
					        };
					    } // namespace trait
					} // namespace alpaka::experimental
					// ==
					// == ./include/alpaka/mem/view/ViewAccessor.hpp ==
					// ============================================================================


				// #include <cstddef>    // amalgamate: file already included
				// #include <utility>    // amalgamate: file already included

				#ifdef ALPAKA_ACC_SYCL_ENABLED

				// #    include <CL/sycl.hpp>    // amalgamate: file already included

				namespace alpaka
				{
				    template<typename TElem, typename TDim, typename TIdx, typename TDev>
				    class BufGenericSycl;

				    namespace detail
				    {
				        template<typename... TAlpakaAccessModes>
				        inline constexpr auto sycl_access_mode = sycl::access_mode::read_write;

				        template<>
				        inline constexpr auto sycl_access_mode<experimental::ReadAccess> = sycl::access_mode::read;

				        template<>
				        inline constexpr auto sycl_access_mode<experimental::WriteAccess> = sycl::access_mode::write;

				        template<typename TElem, int TDim, typename... TAlpakaAccessModes>
				        using SyclAccessor = sycl::accessor<
				            TElem,
				            TDim,
				            sycl_access_mode<TAlpakaAccessModes...>,
				            sycl::target::device,
				            sycl::access::placeholder::true_t>;
				    } // namespace detail

				    template<typename TElem, typename TIdx, typename TAccessModes>
				    struct experimental::
				        Accessor<detail::SyclAccessor<TElem, 1, TAccessModes>, TElem, TIdx, std::size_t{1}, TAccessModes>
				    {
				        static constexpr auto sycl_access_mode = detail::sycl_access_mode<TAccessModes>;
				        using SyclAccessor = detail::SyclAccessor<TElem, 1, TAccessModes>;
				        using VecType = Vec<DimInt<1>, TIdx>;
				        using ReturnType = std::conditional_t<
				            std::is_same_v<TAccessModes, ReadAccess>,
				            typename SyclAccessor::const_reference,
				            typename SyclAccessor::reference>;

				        Accessor(SyclAccessor accessor, Vec<DimInt<1>, TIdx> ext) : m_accessor{accessor}, extents{ext}
				        {
				        }

				        auto operator[](VecType const& i) const -> ReturnType
				        {
				            auto const range = sycl::id<1>{i[0]};
				            return m_accessor[range];
				        }

				        auto operator[](TIdx i) const -> ReturnType
				        {
				            return m_accessor[i];
				        }

				        template<typename... TIs>
				        auto operator()(TIs... is) const
				        {
				            static_assert(sizeof...(TIs) == 1, "Number of indices must match the dimensionality.");
				            return operator[](VecType{static_cast<TIdx>(is)...});
				        }

				        SyclAccessor m_accessor;
				        VecType extents;
				    };

				    template<typename TElem, typename TIdx, std::size_t TDim, typename TAccessModes>
				    struct experimental::
				        Accessor<detail::SyclAccessor<TElem, DimInt<TDim>::value, TAccessModes>, TElem, TIdx, TDim, TAccessModes>
				    {
				        static constexpr auto sycl_access_mode = detail::sycl_access_mode<TAccessModes>;
				        using SyclAccessor = detail::SyclAccessor<TElem, DimInt<TDim>::value, TAccessModes>;
				        using VecType = Vec<DimInt<TDim>, TIdx>;
				        using ReturnType = std::conditional_t<
				            std::is_same_v<TAccessModes, ReadAccess>,
				            typename SyclAccessor::const_reference,
				            typename SyclAccessor::reference>;


				        Accessor(SyclAccessor accessor, Vec<DimInt<TDim>, TIdx> ext) : m_accessor{accessor}, extents{ext}
				        {
				        }

				        auto operator[](VecType const& i) const -> ReturnType
				        {
				            using IdType = sycl::id<DimInt<TDim>::value>;
				            auto const id = (TDim == 2) ? IdType{i[1], i[0]} : IdType{i[2], i[1], i[0]};
				            return m_accessor[id];
				        }

				        template<typename... TIs>
				        auto operator()(TIs... is) const -> ReturnType
				        {
				            static_assert(sizeof...(TIs) == TDim, "Number of indices must match the dimensionality.");
				            return operator[](VecType{static_cast<TIdx>(is)...});
				        }

				        SyclAccessor m_accessor;
				        VecType extents;
				    };

				    namespace experimental::trait
				    {
				        namespace internal
				        {
				            template<typename TElem, typename TDim, typename TIdx, typename TDev>
				            struct IsView<BufGenericSycl<TElem, TDim, TIdx, TDev>> : std::false_type
				            {
				            };
				        } // namespace internal

				        template<typename TElem, typename TDim, typename TIdx, typename TDev>
				        struct BuildAccessor<BufGenericSycl<TElem, TDim, TIdx, TDev>>
				        {
				            template<typename... TAccessModes>
				            static auto buildAccessor(BufGenericSycl<TElem, TDim, TIdx, TDev>& buffer)
				            {
				                using SyclAccessor = detail::SyclAccessor<TElem, TDim::value, TAccessModes...>;
				                return Accessor<SyclAccessor, TElem, TIdx, TDim::value, TAccessModes...>{
				                    SyclAccessor{buffer.m_buffer},
				                    buffer.m_extentElements};
				            }
				        };
				    } // namespace experimental::trait
				} // namespace alpaka

				#endif
				// ==
				// == ./include/alpaka/mem/buf/sycl/Accessor.hpp ==
				// ============================================================================

			// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

			// #include <cassert>    // amalgamate: file already included
			// #include <functional>    // amalgamate: file already included
			// #include <memory>    // amalgamate: file already included
			// #include <stdexcept>    // amalgamate: file already included
			// #include <tuple>    // amalgamate: file already included
			// #include <type_traits>    // amalgamate: file already included
			// #include <utility>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka::detail
			{
			    template<typename TAcc, typename TKernelFnObj, typename... TArgs>
			    struct kernel
			    {
			    }; // SYCL kernel names must be globally visible

			    // Helpers for assigning placeholder accessors to the command group of our kernel
			    struct general
			    {
			    };
			    struct special : general
			    {
			    };

			    template<typename TElem, typename TIdx, std::size_t TDim, typename TAccessModes>
			    inline auto require(
			        sycl::handler& cgh,
			        experimental::
			            Accessor<detail::SyclAccessor<TElem, DimInt<TDim>::value, TAccessModes>, TElem, TIdx, TDim, TAccessModes>
			                acc,
			        special)
			    {
			        cgh.require(acc.m_accessor);
			    }

			    template<typename TParam>
			    inline auto require(sycl::handler&, TParam&&, general)
			    {
			    }

			    template<typename... TArgs>
			    inline auto require(sycl::handler& cgh, core::Tuple<TArgs...> const& args)
			    {
			        core::apply([&](auto&&... ps) { (require(cgh, std::forward<decltype(ps)>(ps), special{}), ...); }, args);
			    }
			} // namespace alpaka::detail

			namespace alpaka
			{
			    //! The SYCL accelerator execution task.
			    template<typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
			    class TaskKernelGenericSycl final : public WorkDivMembers<TDim, TIdx>
			    {
			    public:
			        static_assert(TDim::value > 0 && TDim::value <= 3, "Invalid kernel dimensionality");

			        template<typename TWorkDiv>
			        TaskKernelGenericSycl(TWorkDiv&& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
			            : WorkDivMembers<TDim, TIdx>(std::forward<TWorkDiv>(workDiv))
			            , m_kernelFnObj{kernelFnObj}
			            , m_args{std::forward<TArgs>(args)...}
			        {
			        }

			        auto operator()(sycl::handler& cgh, sycl::buffer<int, 1>& global_fence_buf) const -> void
			        {
			            // Assign placeholder accessors to this command group
			            detail::require(cgh, m_args);

			            auto const work_groups = WorkDivMembers<TDim, TIdx>::m_gridBlockExtent;
			            auto const group_items = WorkDivMembers<TDim, TIdx>::m_blockThreadExtent;
			            auto const item_elements = WorkDivMembers<TDim, TIdx>::m_threadElemExtent;

			            auto const global_size = get_global_size(work_groups, group_items);
			            auto const local_size = get_local_size(group_items);

			            // allocate dynamic shared memory -- needs at least 1 byte to make the Xilinx Runtime happy
			            auto const dyn_shared_mem_bytes = std::max(
			                1ul,
			                core::apply(
			                    [&](std::decay_t<TArgs> const&... args) {
			                        return getBlockSharedMemDynSizeBytes<TAcc>(m_kernelFnObj, group_items, item_elements, args...);
			                    },
			                    m_args));

			            auto dyn_shared_accessor = sycl::local_accessor<std::byte>{sycl::range<1>{dyn_shared_mem_bytes}, cgh};

			            // allocate static shared memory -- value comes from the build system
			            constexpr auto st_shared_mem_bytes = std::size_t{ALPAKA_BLOCK_SHARED_DYN_MEMBER_ALLOC_KIB * 1024};
			            auto st_shared_accessor = sycl::local_accessor<std::byte>{sycl::range<1>{st_shared_mem_bytes}, cgh};

			            // register memory fence dummies
			            auto global_fence_dummy = global_fence_buf.get_access(cgh); // Exists once per queue
			            auto local_fence_dummy = sycl::local_accessor<int>{sycl::range<1>{1}, cgh};

			            // copy-by-value so we don't access 'this' on the device
			            auto k_func = m_kernelFnObj;
			            auto k_args = m_args;

			#    ifdef ALPAKA_SYCL_IOSTREAM_ENABLED
			            // Set up device-side printing with (user-chosen value) KiB per block for the output buffer.
			            constexpr auto buf_size = std::size_t{ALPAKA_SYCL_IOSTREAM_KIB * 1024};
			            auto buf_per_work_item = std::size_t{};
			            if constexpr(TDim::value == 1)
			                buf_per_work_item = buf_size / static_cast<std::size_t>(group_items[0]);
			            else if constexpr(TDim::value == 2)
			                buf_per_work_item = buf_size / static_cast<std::size_t>(group_items[0] * group_items[1]);
			            else
			                buf_per_work_item
			                    = buf_size / static_cast<std::size_t>(group_items[0] * group_items[1] * group_items[2]);

			            assert(buf_per_work_item > 0);

			            auto output_stream = sycl::stream{buf_size, buf_per_work_item, cgh};
			#    endif
			            cgh.parallel_for<detail::kernel<TAcc, TKernelFnObj, TArgs...>>(
			                sycl::nd_range<TDim::value>{global_size, local_size},
			                [=](sycl::nd_item<TDim::value> work_item)
			                {
			#    ifdef ALPAKA_SYCL_IOSTREAM_ENABLED
			                    auto acc = TAcc{
			                        item_elements,
			                        work_item,
			                        dyn_shared_accessor,
			                        st_shared_accessor,
			                        global_fence_dummy,
			                        local_fence_dummy,
			                        output_stream};
			#    else
			                    auto acc = TAcc{
			                        item_elements,
			                        work_item,
			                        dyn_shared_accessor,
			                        st_shared_accessor,
			                        global_fence_dummy,
			                        local_fence_dummy};
			#    endif
			                    core::apply(
			                        [k_func, &acc](typename std::decay_t<TArgs> const&... args) { k_func(acc, args...); },
			                        k_args);
			                });
			        }

			        static constexpr auto is_sycl_task = true;
			        // Distinguish from other tasks
			        static constexpr auto is_sycl_kernel = true;

			    private:
			        auto get_global_size(Vec<TDim, TIdx> const& work_groups, Vec<TDim, TIdx> const& group_items) const
			        {
			            if constexpr(TDim::value == 1)
			                return sycl::range<1>{static_cast<std::size_t>(work_groups[0] * group_items[0])};
			            else if constexpr(TDim::value == 2)
			                return sycl::range<2>{
			                    static_cast<std::size_t>(work_groups[1] * group_items[1]),
			                    static_cast<std::size_t>(work_groups[0] * group_items[0])};
			            else
			                return sycl::range<3>{
			                    static_cast<std::size_t>(work_groups[2] * group_items[2]),
			                    static_cast<std::size_t>(work_groups[1] * group_items[1]),
			                    static_cast<std::size_t>(work_groups[0] * group_items[0])};
			        }

			        auto get_local_size(Vec<TDim, TIdx> const& group_items) const
			        {
			            if constexpr(TDim::value == 1)
			                return sycl::range<1>{static_cast<std::size_t>(group_items[0])};
			            else if constexpr(TDim::value == 2)
			                return sycl::range<2>{
			                    static_cast<std::size_t>(group_items[1]),
			                    static_cast<std::size_t>(group_items[0])};
			            else
			                return sycl::range<3>{
			                    static_cast<std::size_t>(group_items[2]),
			                    static_cast<std::size_t>(group_items[1]),
			                    static_cast<std::size_t>(group_items[0])};
			        }

			    public:
			        TKernelFnObj m_kernelFnObj;
			        core::Tuple<std::decay_t<TArgs>...> m_args;
			    };
			} // namespace alpaka

			namespace alpaka::trait
			{
			    //! The SYCL execution task accelerator type trait specialization.
			    template<typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
			    struct AccType<TaskKernelGenericSycl<TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
			    {
			        using type = TAcc;
			    };

			    //! The SYCL execution task device type trait specialization.
			    template<typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
			    struct DevType<TaskKernelGenericSycl<TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
			    {
			        using type = typename DevType<TAcc>::type;
			    };

			    //! The SYCL execution task platform type trait specialization.
			    template<typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
			    struct PltfType<TaskKernelGenericSycl<TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
			    {
			        using type = typename PltfType<TAcc>::type;
			    };

			    //! The SYCL execution task dimension getter trait specialization.
			    template<typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
			    struct DimType<TaskKernelGenericSycl<TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
			    {
			        using type = TDim;
			    };

			    //! The SYCL execution task idx type trait specialization.
			    template<typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
			    struct IdxType<TaskKernelGenericSycl<TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
			    {
			        using type = TIdx;
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/kernel/TaskKernelGenericSycl.hpp ==
			// ============================================================================


		#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_CPU)

		namespace alpaka
		{
		    template<typename TDim, typename TIdx>
		    class AccCpuSyclIntel;

		    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		    using TaskKernelCpuSyclIntel
		        = TaskKernelGenericSycl<AccCpuSyclIntel<TDim, TIdx>, TDim, TIdx, TKernelFnObj, TArgs...>;
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/kernel/TaskKernelCpuSyclIntel.hpp ==
		// ============================================================================

	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/PltfCpuSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

	// #include <cstddef>    // amalgamate: file already included
	// #include <string>    // amalgamate: file already included
	// #include <utility>    // amalgamate: file already included

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_CPU)

	// #    include <CL/sycl.hpp>    // amalgamate: file already included

	namespace alpaka
	{
	    //! The Intel CPU SYCL accelerator.
	    //!
	    //! This accelerator allows parallel kernel execution on a oneAPI-capable Intel CPU target device.
	    template<typename TDim, typename TIdx>
	    class AccCpuSyclIntel final
	        : public AccGenericSycl<TDim, TIdx>
	        , public concepts::Implements<ConceptAcc, AccCpuSyclIntel<TDim, TIdx>>
	    {
	    public:
	        using AccGenericSycl<TDim, TIdx>::AccGenericSycl;
	    };
	} // namespace alpaka

	namespace alpaka::trait
	{
	    //! The Intel CPU SYCL accelerator name trait specialization.
	    template<typename TDim, typename TIdx>
	    struct GetAccName<AccCpuSyclIntel<TDim, TIdx>>
	    {
	        static auto getAccName() -> std::string
	        {
	            return "AccCpuSyclIntel<" + std::to_string(TDim::value) + "," + core::demangled<TIdx> + ">";
	        }
	    };

	    //! The Intel CPU SYCL accelerator device type trait specialization.
	    template<typename TDim, typename TIdx>
	    struct DevType<AccCpuSyclIntel<TDim, TIdx>>
	    {
	        using type = DevCpuSyclIntel;
	    };

	    //! The Intel CPU SYCL accelerator execution task type trait specialization.
	    template<typename TDim, typename TIdx, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
	    struct CreateTaskKernel<AccCpuSyclIntel<TDim, TIdx>, TWorkDiv, TKernelFnObj, TArgs...>
	    {
	        static auto createTaskKernel(TWorkDiv const& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
	        {
	            return TaskKernelCpuSyclIntel<TDim, TIdx, TKernelFnObj, TArgs...>{
	                workDiv,
	                kernelFnObj,
	                std::forward<TArgs>(args)...};
	        }
	    };

	    //! The Intel CPU SYCL execution task platform type trait specialization.
	    template<typename TDim, typename TIdx>
	    struct PltfType<AccCpuSyclIntel<TDim, TIdx>>
	    {
	        using type = PltfCpuSyclIntel;
	    };

	    template<typename TDim, typename TIdx>
	    struct AccToTag<alpaka::AccCpuSyclIntel<TDim, TIdx>>
	    {
	        using type = alpaka::TagCpuSyclIntel;
	    };

	    template<typename TDim, typename TIdx>
	    struct TagToAcc<alpaka::TagCpuSyclIntel, TDim, TIdx>
	    {
	        using type = alpaka::AccCpuSyclIntel<TDim, TIdx>;
	    };
	} // namespace alpaka::trait

	#endif
	// ==
	// == ./include/alpaka/acc/AccCpuSyclIntel.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/acc/AccCpuTbbBlocks.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Erik Zenker, René Widera, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Base classes.
	// #include "alpaka/atomic/AtomicCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/atomic/AtomicHierarchy.hpp"    // amalgamate: file already expanded
	// #include "alpaka/atomic/AtomicNoOp.hpp"    // amalgamate: file already expanded
	// #include "alpaka/block/shared/dyn/BlockSharedMemDynMember.hpp"    // amalgamate: file already expanded
	// #include "alpaka/block/shared/st/BlockSharedMemStMember.hpp"    // amalgamate: file already expanded
	// #include "alpaka/block/sync/BlockSyncNoOp.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/bt/IdxBtZero.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/gb/IdxGbRef.hpp"    // amalgamate: file already expanded
	// #include "alpaka/intrinsic/IntrinsicCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/math/MathStdLib.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/mem/fence/MemFenceCpu.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/fence/Traits.hpp"    // amalgamate: file already expanded

		// #include <atomic>    // amalgamate: file already included

		namespace alpaka
		{
		    //! The default CPU memory fence.
		    class MemFenceCpu : public concepts::Implements<ConceptMemFence, MemFenceCpu>
		    {
		    };

		    namespace trait
		    {
		        template<typename TMemScope>
		        struct MemFence<MemFenceCpu, TMemScope>
		        {
		            static auto mem_fence(MemFenceCpu const&, TMemScope const&)
		            {
		                /*
		                 * Intuitively, std::atomic_thread_fence creates a fence on the block level.
		                 *
		                 * Creating a block fence is enough for the whole device because the blocks are executed serially. By
		                 * definition of fences, preceding blocks don't have a guarantee to see the results of this block's
		                 * STORE operations (only that they will be ordered correctly); the following blocks see the results
		                 * once they start. Consider the following code:
		                 *
		                 * int x = 1;
		                 * int y = 2;
		                 *
		                 * void foo()
		                 * {
		                 *     x = 10;
		                 *     alpaka::mem_fence(acc, memory_scope::device);
		                 *     y = 20;
		                 * }
		                 *
		                 * void bar()
		                 * {
		                 *     auto b = y;
		                 *     alpaka::mem_fence(acc, memory_scope::device);
		                 *     auto a = x;
		                 * }
		                 *
		                 * The following are all valid outcomes:
		                 *   a == 1 && b == 2
		                 *   a == 10 && b == 2
		                 *   a == 10 && b == 20
		                 */

		                static auto dummy = std::atomic<int>{42};

		                /* ISO C++ fences are only clearly defined if there are atomic operations surrounding them. So we use
		                 * these dummy operations to ensure this.*/
		                auto x = dummy.load(std::memory_order_relaxed);
		                std::atomic_thread_fence(std::memory_order_acq_rel);
		                dummy.store(x, std::memory_order_relaxed);
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/mem/fence/MemFenceCpu.hpp ==
		// ============================================================================

	// #include "alpaka/rand/RandStdLib.hpp"    // amalgamate: file already expanded
	// #include "alpaka/warp/WarpSingleThread.hpp"    // amalgamate: file already expanded
	// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

	// Specialized traits.
	// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
	// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded

	// #include <memory>    // amalgamate: file already included
	// #include <typeinfo>    // amalgamate: file already included

	#ifdef ALPAKA_ACC_CPU_B_TBB_T_SEQ_ENABLED

	namespace alpaka
	{
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuTbbBlocks;

	    //! The CPU TBB block accelerator.
	    template<
	        typename TDim,
	        typename TIdx>
	    class AccCpuTbbBlocks final :
	        public WorkDivMembers<TDim, TIdx>,
	        public gb::IdxGbRef<TDim, TIdx>,
	        public bt::IdxBtZero<TDim, TIdx>,
	        public AtomicHierarchy<
	            AtomicCpu, // grid atomics
	            AtomicCpu, // block atomics
	            AtomicNoOp         // thread atomics
	        >,
	        public math::MathStdLib,
	        public BlockSharedMemDynMember<>,
	        public BlockSharedMemStMember<>,
	        public BlockSyncNoOp,
	        public IntrinsicCpu,
	        public MemFenceCpu,
	        public rand::RandStdLib,
	        public warp::WarpSingleThread,
	        public concepts::Implements<ConceptAcc, AccCpuTbbBlocks<TDim, TIdx>>
	    {
	        static_assert(
	            sizeof(TIdx) >= sizeof(int),
	            "Index type is not supported, consider using int or a larger type.");

	    public:
	        // Partial specialization with the correct TDim and TIdx is not allowed.
	        template<typename TDim2, typename TIdx2, typename TKernelFnObj, typename... TArgs>
	        friend class ::alpaka::TaskKernelCpuTbbBlocks;

	        AccCpuTbbBlocks(AccCpuTbbBlocks const&) = delete;
	        AccCpuTbbBlocks(AccCpuTbbBlocks&&) = delete;
	        auto operator=(AccCpuTbbBlocks const&) -> AccCpuTbbBlocks& = delete;
	        auto operator=(AccCpuTbbBlocks&&) -> AccCpuTbbBlocks& = delete;

	    private:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST AccCpuTbbBlocks(TWorkDiv const& workDiv, std::size_t const& blockSharedMemDynSizeBytes)
	            : WorkDivMembers<TDim, TIdx>(workDiv)
	            , gb::IdxGbRef<TDim, TIdx>(m_gridBlockIdx)
	            , bt::IdxBtZero<TDim, TIdx>()
	            , AtomicHierarchy<
	                  AtomicCpu, // atomics between grids
	                  AtomicCpu, // atomics between blocks
	                  AtomicNoOp // atomics between threads
	                  >()
	            , math::MathStdLib()
	            , BlockSharedMemDynMember<>(blockSharedMemDynSizeBytes)
	            , BlockSharedMemStMember<>(staticMemBegin(), staticMemCapacity())
	            , BlockSyncNoOp()
	            , MemFenceCpu()
	            , rand::RandStdLib()
	            , m_gridBlockIdx(Vec<TDim, TIdx>::zeros())
	        {
	        }

	    private:
	        // getIdx
	        Vec<TDim, TIdx> mutable m_gridBlockIdx; //!< The index of the currently executed block.
	    };

	    namespace trait
	    {
	        //! The CPU TBB block accelerator type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct AccType<AccCpuTbbBlocks<TDim, TIdx>>
	        {
	            using type = AccCpuTbbBlocks<TDim, TIdx>;
	        };
	        //! The CPU TBB block accelerator device properties get trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccDevProps<AccCpuTbbBlocks<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccDevProps(DevCpu const& /* dev */) -> AccDevProps<TDim, TIdx>
	            {
	                return {// m_multiProcessorCount
	                        static_cast<TIdx>(1),
	                        // m_gridBlockExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_gridBlockCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_blockThreadExtentMax
	                        Vec<TDim, TIdx>::ones(),
	                        // m_blockThreadCountMax
	                        static_cast<TIdx>(1),
	                        // m_threadElemExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_threadElemCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_sharedMemSizeBytes
	                        static_cast<size_t>(AccCpuTbbBlocks<TDim, TIdx>::staticAllocBytes())};
	            }
	        };
	        //! The CPU TBB block accelerator name trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccName<AccCpuTbbBlocks<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccName() -> std::string
	            {
	                return "AccCpuTbbBlocks<" + std::to_string(TDim::value) + "," + core::demangled<TIdx> + ">";
	            }
	        };

	        //! The CPU TBB block accelerator device type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DevType<AccCpuTbbBlocks<TDim, TIdx>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU TBB block accelerator dimension getter trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DimType<AccCpuTbbBlocks<TDim, TIdx>>
	        {
	            using type = TDim;
	        };

	        //! The CPU TBB block accelerator execution task type trait specialization.
	        template<typename TDim, typename TIdx, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
	        struct CreateTaskKernel<AccCpuTbbBlocks<TDim, TIdx>, TWorkDiv, TKernelFnObj, TArgs...>
	        {
	            ALPAKA_FN_HOST static auto createTaskKernel(
	                TWorkDiv const& workDiv,
	                TKernelFnObj const& kernelFnObj,
	                TArgs&&... args)
	            {
	                return TaskKernelCpuTbbBlocks<TDim, TIdx, TKernelFnObj, TArgs...>(
	                    workDiv,
	                    kernelFnObj,
	                    std::forward<TArgs>(args)...);
	            }
	        };

	        //! The CPU TBB block execution task platform type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct PltfType<AccCpuTbbBlocks<TDim, TIdx>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU TBB block accelerator idx type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct IdxType<AccCpuTbbBlocks<TDim, TIdx>>
	        {
	            using type = TIdx;
	        };

	        template<typename TDim, typename TIdx>
	        struct AccToTag<alpaka::AccCpuTbbBlocks<TDim, TIdx>>
	        {
	            using type = alpaka::TagCpuTbbBlocks;
	        };

	        template<typename TDim, typename TIdx>
	        struct TagToAcc<alpaka::TagCpuTbbBlocks, TDim, TIdx>
	        {
	            using type = alpaka::AccCpuTbbBlocks<TDim, TIdx>;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/acc/AccCpuTbbBlocks.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/acc/AccCpuThreads.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, René Widera, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Base classes.
	// #include "alpaka/atomic/AtomicCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/atomic/AtomicHierarchy.hpp"    // amalgamate: file already expanded
	// #include "alpaka/block/shared/dyn/BlockSharedMemDynMember.hpp"    // amalgamate: file already expanded
	// #include "alpaka/block/shared/st/BlockSharedMemStMemberMasterSync.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/block/sync/BlockSyncBarrierThread.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/block/sync/Traits.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/core/BarrierThread.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Matthias Werner, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// Uncomment this to disable the standard spinlock behaviour of the threads
			//#define ALPAKA_THREAD_BARRIER_DISABLE_SPINLOCK

			// #include "alpaka/block/sync/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

			// #include <condition_variable>    // amalgamate: file already included
			// #include <mutex>    // amalgamate: file already included
			#ifndef ALPAKA_THREAD_BARRIER_DISABLE_SPINLOCK
			// #    include <atomic>    // amalgamate: file already included
			// #    include <thread>    // amalgamate: file already included
			#endif

			namespace alpaka::core
			{
			    namespace threads
			    {
			        //! A self-resetting barrier.
			        template<typename TIdx>
			        class BarrierThread final
			        {
			        public:
			            explicit BarrierThread(TIdx const& threadCount)
			                : m_threadCount(threadCount)
			                , m_curThreadCount(threadCount)
			                , m_generation(0)
			            {
			            }

			            //! Waits for all the other threads to reach the barrier.
			            auto wait() -> void
			            {
			                TIdx const generationWhenEnteredTheWait = m_generation;
			#ifdef ALPAKA_THREAD_BARRIER_DISABLE_SPINLOCK
			                std::unique_lock<std::mutex> lock(m_mtxBarrier);
			#endif
			                if(--m_curThreadCount == 0)
			                {
			                    m_curThreadCount = m_threadCount;
			                    ++m_generation;
			#ifdef ALPAKA_THREAD_BARRIER_DISABLE_SPINLOCK
			                    m_cvAllThreadsReachedBarrier.notify_all();
			#endif
			                }
			                else
			                {
			#ifdef ALPAKA_THREAD_BARRIER_DISABLE_SPINLOCK
			                    m_cvAllThreadsReachedBarrier.wait(
			                        lock,
			                        [this, generationWhenEnteredTheWait] { return generationWhenEnteredTheWait != m_generation; });
			#else
			                    while(generationWhenEnteredTheWait == m_generation)
			                    {
			                        std::this_thread::yield();
			                    }
			#endif
			                }
			            }

			        private:
			#ifdef ALPAKA_THREAD_BARRIER_DISABLE_SPINLOCK
			            std::mutex m_mtxBarrier;
			            std::condition_variable m_cvAllThreadsReachedBarrier;
			#endif
			            const TIdx m_threadCount;
			#ifdef ALPAKA_THREAD_BARRIER_DISABLE_SPINLOCK
			            TIdx m_curThreadCount;
			            TIdx m_generation;
			#else
			            std::atomic<TIdx> m_curThreadCount;
			            std::atomic<TIdx> m_generation;
			#endif
			        };

			        namespace detail
			        {
			            template<typename TOp>
			            struct AtomicOp;
			            template<>
			            struct AtomicOp<BlockCount>
			            {
			                void operator()(std::atomic<int>& result, bool value)
			                {
			                    result += static_cast<int>(value);
			                }
			            };
			            template<>
			            struct AtomicOp<BlockAnd>
			            {
			                void operator()(std::atomic<int>& result, bool value)
			                {
			                    result &= static_cast<int>(value);
			                }
			            };
			            template<>
			            struct AtomicOp<BlockOr>
			            {
			                void operator()(std::atomic<int>& result, bool value)
			                {
			                    result |= static_cast<int>(value);
			                }
			            };
			        } // namespace detail

			        //! A self-resetting barrier with barrier.
			        template<typename TIdx>
			        class BarrierThreadWithPredicate final
			        {
			        public:
			            explicit BarrierThreadWithPredicate(TIdx const& threadCount)
			                : m_threadCount(threadCount)
			                , m_curThreadCount(threadCount)
			                , m_generation(0)
			            {
			            }

			            //! Waits for all the other threads to reach the barrier.
			            template<typename TOp>
			            ALPAKA_FN_HOST auto wait(int predicate) -> int
			            {
			                TIdx const generationWhenEnteredTheWait = m_generation;
			                std::unique_lock<std::mutex> lock(m_mtxBarrier);

			                auto const generationMod2 = m_generation % static_cast<TIdx>(2u);
			                if(m_curThreadCount == m_threadCount)
			                {
			                    m_result[generationMod2] = TOp::InitialValue;
			                }

			                std::atomic<int>& result(m_result[generationMod2]);
			                bool const predicateBool(predicate != 0);

			                detail::AtomicOp<TOp>()(result, predicateBool);

			                if(--m_curThreadCount == 0)
			                {
			                    m_curThreadCount = m_threadCount;
			                    ++m_generation;
			                    m_cvAllThreadsReachedBarrier.notify_all();
			                }
			                else
			                {
			                    m_cvAllThreadsReachedBarrier.wait(
			                        lock,
			                        [this, generationWhenEnteredTheWait] { return generationWhenEnteredTheWait != m_generation; });
			                }
			                return m_result[generationMod2];
			            }

			        private:
			            std::mutex m_mtxBarrier;
			            std::condition_variable m_cvAllThreadsReachedBarrier;
			            const TIdx m_threadCount;
			            TIdx m_curThreadCount;
			            TIdx m_generation;
			            std::atomic<int> m_result[2];
			        };
			    } // namespace threads
			} // namespace alpaka::core
			// ==
			// == ./include/alpaka/core/BarrierThread.hpp ==
			// ============================================================================

		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

		// #include <map>    // amalgamate: file already included
		// #include <mutex>    // amalgamate: file already included
		// #include <thread>    // amalgamate: file already included

		#ifdef ALPAKA_ACC_CPU_B_SEQ_T_THREADS_ENABLED

		namespace alpaka
		{
		    //! The thread id map barrier block synchronization.
		    template<typename TIdx>
		    class BlockSyncBarrierThread : public concepts::Implements<ConceptBlockSync, BlockSyncBarrierThread<TIdx>>
		    {
		    public:
		        using Barrier = core::threads::BarrierThread<TIdx>;
		        using BarrierWithPredicate = core::threads::BarrierThreadWithPredicate<TIdx>;

		        ALPAKA_FN_HOST BlockSyncBarrierThread(TIdx const& blockThreadCount)
		            : m_barrier(blockThreadCount)
		            , m_barrierWithPredicate(blockThreadCount)
		        {
		        }

		        Barrier mutable m_barrier;
		        BarrierWithPredicate mutable m_barrierWithPredicate;
		    };

		    namespace trait
		    {
		        template<typename TIdx>
		        struct SyncBlockThreads<BlockSyncBarrierThread<TIdx>>
		        {
		            ALPAKA_FN_HOST static auto syncBlockThreads(BlockSyncBarrierThread<TIdx> const& blockSync) -> void
		            {
		                blockSync.m_barrier.wait();
		            }
		        };

		        template<typename TOp, typename TIdx>
		        struct SyncBlockThreadsPredicate<TOp, BlockSyncBarrierThread<TIdx>>
		        {
		            ALPAKA_NO_HOST_ACC_WARNING
		            ALPAKA_FN_ACC static auto syncBlockThreadsPredicate(
		                BlockSyncBarrierThread<TIdx> const& blockSync,
		                int predicate) -> int
		            {
		                return blockSync.m_barrierWithPredicate.template wait<TOp>(predicate);
		            }
		        };
		    } // namespace trait
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/block/sync/BlockSyncBarrierThread.hpp ==
		// ============================================================================

	// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/idx/bt/IdxBtRefThreadIdMap.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
		// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

		// #include <map>    // amalgamate: file already included
		// #include <thread>    // amalgamate: file already included

		#ifdef ALPAKA_ACC_CPU_B_SEQ_T_THREADS_ENABLED

		namespace alpaka
		{
		    namespace bt
		    {
		        //! The threads accelerator index provider.
		        template<typename TDim, typename TIdx>
		        class IdxBtRefThreadIdMap : public concepts::Implements<ConceptIdxBt, IdxBtRefThreadIdMap<TDim, TIdx>>
		        {
		        public:
		            using ThreadIdToIdxMap = std::map<std::thread::id, Vec<TDim, TIdx>>;

		            ALPAKA_FN_HOST IdxBtRefThreadIdMap(ThreadIdToIdxMap const& mThreadToIndices)
		                : m_threadToIndexMap(mThreadToIndices)
		            {
		            }
		            ALPAKA_FN_HOST IdxBtRefThreadIdMap(IdxBtRefThreadIdMap const&) = delete;
		            ALPAKA_FN_HOST auto operator=(IdxBtRefThreadIdMap const&) -> IdxBtRefThreadIdMap& = delete;

		        public:
		            ThreadIdToIdxMap const& m_threadToIndexMap; //!< The mapping of thread id's to thread indices.
		        };
		    } // namespace bt

		    namespace trait
		    {
		        //! The CPU threads accelerator index dimension get trait specialization.
		        template<typename TDim, typename TIdx>
		        struct DimType<bt::IdxBtRefThreadIdMap<TDim, TIdx>>
		        {
		            using type = TDim;
		        };

		        //! The CPU threads accelerator block thread index get trait specialization.
		        template<typename TDim, typename TIdx>
		        struct GetIdx<bt::IdxBtRefThreadIdMap<TDim, TIdx>, origin::Block, unit::Threads>
		        {
		            //! \return The index of the current thread in the block.
		            template<typename TWorkDiv>
		            ALPAKA_FN_HOST static auto getIdx(
		                bt::IdxBtRefThreadIdMap<TDim, TIdx> const& idx,
		                TWorkDiv const& /* workDiv */) -> Vec<TDim, TIdx>
		            {
		                auto const threadId = std::this_thread::get_id();
		                auto const threadEntry = idx.m_threadToIndexMap.find(threadId);
		                ALPAKA_ASSERT(threadEntry != std::end(idx.m_threadToIndexMap));
		                return threadEntry->second;
		            }
		        };

		        //! The CPU threads accelerator block thread index idx type trait specialization.
		        template<typename TDim, typename TIdx>
		        struct IdxType<bt::IdxBtRefThreadIdMap<TDim, TIdx>>
		        {
		            using type = TIdx;
		        };
		    } // namespace trait
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/idx/bt/IdxBtRefThreadIdMap.hpp ==
		// ============================================================================

	// #include "alpaka/idx/gb/IdxGbRef.hpp"    // amalgamate: file already expanded
	// #include "alpaka/intrinsic/IntrinsicCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/math/MathStdLib.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/fence/MemFenceCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/rand/RandStdLib.hpp"    // amalgamate: file already expanded
	// #include "alpaka/warp/WarpSingleThread.hpp"    // amalgamate: file already expanded
	// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

	// Specialized traits.
	// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
	// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/ClipCast.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded

	// #include <memory>    // amalgamate: file already included
	// #include <thread>    // amalgamate: file already included
	// #include <typeinfo>    // amalgamate: file already included

	#ifdef ALPAKA_ACC_CPU_B_SEQ_T_THREADS_ENABLED

	namespace alpaka
	{
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuThreads;

	    //! The CPU threads accelerator.
	    //!
	    //! This accelerator allows parallel kernel execution on a CPU device.
	    //! It uses std::thread to implement the parallelism.
	    template<
	        typename TDim,
	        typename TIdx>
	    class AccCpuThreads final :
	        public WorkDivMembers<TDim, TIdx>,
	        public gb::IdxGbRef<TDim, TIdx>,
	        public bt::IdxBtRefThreadIdMap<TDim, TIdx>,
	        public AtomicHierarchy<
	            AtomicCpu, // grid atomics
	            AtomicCpu, // block atomics
	            AtomicCpu  // thread atomics
	        >,
	        public math::MathStdLib,
	        public BlockSharedMemDynMember<>,
	        public BlockSharedMemStMemberMasterSync<>,
	        public BlockSyncBarrierThread<TIdx>,
	        public IntrinsicCpu,
	        public MemFenceCpu,
	        public rand::RandStdLib,
	        public warp::WarpSingleThread,
	        public concepts::Implements<ConceptAcc, AccCpuThreads<TDim, TIdx>>
	    {
	        static_assert(
	            sizeof(TIdx) >= sizeof(int),
	            "Index type is not supported, consider using int or a larger type.");

	    public:
	        // Partial specialization with the correct TDim and TIdx is not allowed.
	        template<typename TDim2, typename TIdx2, typename TKernelFnObj, typename... TArgs>
	        friend class ::alpaka::TaskKernelCpuThreads;

	        AccCpuThreads(AccCpuThreads const&) = delete;
	        AccCpuThreads(AccCpuThreads&&) = delete;
	        auto operator=(AccCpuThreads const&) -> AccCpuThreads& = delete;
	        auto operator=(AccCpuThreads&&) -> AccCpuThreads& = delete;

	    private:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST AccCpuThreads(TWorkDiv const& workDiv, std::size_t const& blockSharedMemDynSizeBytes)
	            : WorkDivMembers<TDim, TIdx>(workDiv)
	            , gb::IdxGbRef<TDim, TIdx>(m_gridBlockIdx)
	            , bt::IdxBtRefThreadIdMap<TDim, TIdx>(m_threadToIndexMap)
	            , AtomicHierarchy<
	                  AtomicCpu, // atomics between grids
	                  AtomicCpu, // atomics between blocks
	                  AtomicCpu // atomics between threads
	                  >()
	            , math::MathStdLib()
	            , BlockSharedMemDynMember<>(blockSharedMemDynSizeBytes)
	            , BlockSharedMemStMemberMasterSync<>(
	                  staticMemBegin(),
	                  staticMemCapacity(),
	                  [this]() { syncBlockThreads(*this); },
	                  [this]() noexcept { return (m_idMasterThread == std::this_thread::get_id()); })
	            , BlockSyncBarrierThread<TIdx>(getWorkDiv<Block, Threads>(workDiv).prod())
	            , MemFenceCpu()
	            , rand::RandStdLib()
	            , m_gridBlockIdx(Vec<TDim, TIdx>::zeros())
	        {
	        }

	    private:
	        // getIdx
	        std::mutex mutable m_mtxMapInsert; //!< The mutex used to secure insertion into the ThreadIdToIdxMap.
	        typename bt::IdxBtRefThreadIdMap<TDim, TIdx>::
	            ThreadIdToIdxMap mutable m_threadToIndexMap; //!< The mapping of thread id's to indices.
	        Vec<TDim, TIdx> mutable m_gridBlockIdx; //!< The index of the currently executed block.

	        // allocBlockSharedArr
	        std::thread::id mutable m_idMasterThread; //!< The id of the master thread.
	    };

	    namespace trait
	    {
	        //! The CPU threads accelerator accelerator type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct AccType<AccCpuThreads<TDim, TIdx>>
	        {
	            using type = AccCpuThreads<TDim, TIdx>;
	        };
	        //! The CPU threads accelerator device properties get trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccDevProps<AccCpuThreads<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccDevProps(DevCpu const& dev) -> AccDevProps<TDim, TIdx>
	            {
	#    ifdef ALPAKA_CI
	                auto const blockThreadCountMax(static_cast<TIdx>(8));
	#    else
	                // \TODO: Magic number. What is the maximum? Just set a reasonable value? There is a implementation
	                // defined maximum where the creation of a new thread crashes. std::thread::hardware_concurrency can
	                // return 0, so 1 is the default case?
	                auto const blockThreadCountMax = std::max(
	                    static_cast<TIdx>(1),
	                    alpaka::core::clipCast<TIdx>(std::thread::hardware_concurrency() * 8));
	#    endif
	                return {// m_multiProcessorCount
	                        static_cast<TIdx>(1),
	                        // m_gridBlockExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_gridBlockCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_blockThreadExtentMax
	                        Vec<TDim, TIdx>::all(blockThreadCountMax),
	                        // m_blockThreadCountMax
	                        blockThreadCountMax,
	                        // m_threadElemExtentMax
	                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
	                        // m_threadElemCountMax
	                        std::numeric_limits<TIdx>::max(),
	                        // m_sharedMemSizeBytes
	                        getMemBytes(dev)};
	            }
	        };
	        //! The CPU threads accelerator name trait specialization.
	        template<typename TDim, typename TIdx>
	        struct GetAccName<AccCpuThreads<TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getAccName() -> std::string
	            {
	                return "AccCpuThreads<" + std::to_string(TDim::value) + "," + core::demangled<TIdx> + ">";
	            }
	        };

	        //! The CPU threads accelerator device type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DevType<AccCpuThreads<TDim, TIdx>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU threads accelerator dimension getter trait specialization.
	        template<typename TDim, typename TIdx>
	        struct DimType<AccCpuThreads<TDim, TIdx>>
	        {
	            using type = TDim;
	        };

	        //! The CPU threads accelerator execution task type trait specialization.
	        template<typename TDim, typename TIdx, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
	        struct CreateTaskKernel<AccCpuThreads<TDim, TIdx>, TWorkDiv, TKernelFnObj, TArgs...>
	        {
	            ALPAKA_FN_HOST static auto createTaskKernel(
	                TWorkDiv const& workDiv,
	                TKernelFnObj const& kernelFnObj,
	                TArgs&&... args)
	            {
	                return TaskKernelCpuThreads<TDim, TIdx, TKernelFnObj, TArgs...>(
	                    workDiv,
	                    kernelFnObj,
	                    std::forward<TArgs>(args)...);
	            }
	        };

	        //! The CPU threads execution task platform type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct PltfType<AccCpuThreads<TDim, TIdx>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU threads accelerator idx type trait specialization.
	        template<typename TDim, typename TIdx>
	        struct IdxType<AccCpuThreads<TDim, TIdx>>
	        {
	            using type = TIdx;
	        };

	        template<typename TDim, typename TIdx>
	        struct AccToTag<alpaka::AccCpuThreads<TDim, TIdx>>
	        {
	            using type = alpaka::TagCpuThreads;
	        };

	        template<typename TDim, typename TIdx>
	        struct TagToAcc<alpaka::TagCpuThreads, TDim, TIdx>
	        {
	            using type = alpaka::AccCpuThreads<TDim, TIdx>;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/acc/AccCpuThreads.hpp ==
	// ============================================================================

// #include "alpaka/acc/AccDevProps.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/acc/AccFpgaSyclIntel.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/acc/AccGenericSycl.hpp"    // amalgamate: file already expanded
	// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/dev/DevFpgaSyclIntel.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/dev/DevGenericSycl.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/pltf/PltfFpgaSyclIntel.hpp ==
			// ==
			/* Copyright 2023 Jan Stephan, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/dev/DevGenericSycl.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/pltf/PltfGenericSycl.hpp"    // amalgamate: file already expanded

			#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_FPGA)

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			// #    include <string>    // amalgamate: file already included

			namespace alpaka
			{
			    namespace detail
			    {
			        // Prevent clang from annoying us with warnings about emitting too many vtables. These are discarded by the
			        // linker anyway.
			#    if BOOST_COMP_CLANG
			#        pragma clang diagnostic push
			#        pragma clang diagnostic ignored "-Wweak-vtables"
			#    endif
			        struct IntelFpgaSelector final
			        {
			#    ifdef ALPAKA_FPGA_EMULATION
			            static constexpr auto platform_name = "Intel(R) FPGA Emulation Platform for OpenCL(TM)";
			#    else
			            static constexpr auto platform_name = "Intel(R) FPGA SDK for OpenCL(TM)";
			#    endif

			            auto operator()(sycl::device const& dev) const -> int
			            {
			                auto const& platform = dev.get_platform().get_info<sycl::info::platform::name>();
			                auto const is_intel_fpga = dev.is_accelerator() && (platform == platform_name);

			                return is_intel_fpga ? 1 : -1;
			            }
			        };
			#    if BOOST_COMP_CLANG
			#        pragma clang diagnostic pop
			#    endif
			    } // namespace detail

			    //! The SYCL device manager.
			    using PltfFpgaSyclIntel = PltfGenericSycl<detail::IntelFpgaSelector>;
			} // namespace alpaka

			namespace alpaka::trait
			{
			    //! The SYCL device manager device type trait specialization.
			    template<>
			    struct DevType<PltfFpgaSyclIntel>
			    {
			        using type = DevGenericSycl<PltfFpgaSyclIntel>; // = DevFpgaSyclIntel
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/pltf/PltfFpgaSyclIntel.hpp ==
			// ============================================================================


		#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_FPGA)

		namespace alpaka
		{
		    using DevFpgaSyclIntel = DevGenericSycl<PltfFpgaSyclIntel>;
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/dev/DevFpgaSyclIntel.hpp ==
		// ============================================================================

	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/kernel/TaskKernelFpgaSyclIntel.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan
		 * SPDX-License-Identifier: MPL-2.0
		 */


		// #pragma once
		// #include "alpaka/kernel/TaskKernelGenericSycl.hpp"    // amalgamate: file already expanded

		#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_FPGA)

		namespace alpaka
		{
		    template<typename TDim, typename TIdx>
		    class AccFpgaSyclIntel;

		    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		    using TaskKernelFpgaSyclIntel
		        = TaskKernelGenericSycl<AccFpgaSyclIntel<TDim, TIdx>, TDim, TIdx, TKernelFnObj, TArgs...>;
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/kernel/TaskKernelFpgaSyclIntel.hpp ==
		// ============================================================================

	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/PltfFpgaSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

	// #include <string>    // amalgamate: file already included
	// #include <utility>    // amalgamate: file already included

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_FPGA)

	// #    include <CL/sycl.hpp>    // amalgamate: file already included

	namespace alpaka
	{
	    //! The Intel FPGA SYCL accelerator.
	    //!
	    //! This accelerator allows parallel kernel execution on a oneAPI-capable Intel FPGA target device.
	    template<typename TDim, typename TIdx>
	    class AccFpgaSyclIntel final
	        : public AccGenericSycl<TDim, TIdx>
	        , public concepts::Implements<ConceptAcc, AccFpgaSyclIntel<TDim, TIdx>>
	    {
	    public:
	        using AccGenericSycl<TDim, TIdx>::AccGenericSycl;
	    };
	} // namespace alpaka

	namespace alpaka::trait
	{
	    //! The Intel FPGA SYCL accelerator name trait specialization.
	    template<typename TDim, typename TIdx>
	    struct GetAccName<AccFpgaSyclIntel<TDim, TIdx>>
	    {
	        ALPAKA_FN_HOST static auto getAccName() -> std::string
	        {
	            return "AccFpgaSyclIntel<" + std::to_string(TDim::value) + "," + core::demangled<TIdx> + ">";
	        }
	    };

	    //! The Intel FPGA SYCL accelerator device type trait specialization.
	    template<typename TDim, typename TIdx>
	    struct DevType<AccFpgaSyclIntel<TDim, TIdx>>
	    {
	        using type = DevFpgaSyclIntel;
	    };

	    //! The Intel FPGA SYCL accelerator execution task type trait specialization.
	    template<typename TDim, typename TIdx, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
	    struct CreateTaskKernel<AccFpgaSyclIntel<TDim, TIdx>, TWorkDiv, TKernelFnObj, TArgs...>
	    {
	        static auto createTaskKernel(TWorkDiv const& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
	        {
	            return TaskKernelFpgaSyclIntel<TDim, TIdx, TKernelFnObj, TArgs...>{
	                workDiv,
	                kernelFnObj,
	                std::forward<TArgs>(args)...};
	        }
	    };

	    //! The Intel FPGA SYCL execution task platform type trait specialization.
	    template<typename TDim, typename TIdx>
	    struct PltfType<AccFpgaSyclIntel<TDim, TIdx>>
	    {
	        using type = PltfFpgaSyclIntel;
	    };

	    template<typename TDim, typename TIdx>
	    struct AccToTag<alpaka::AccFpgaSyclIntel<TDim, TIdx>>
	    {
	        using type = alpaka::TagFpgaSyclIntel;
	    };

	    template<typename TDim, typename TIdx>
	    struct TagToAcc<alpaka::TagFpgaSyclIntel, TDim, TIdx>
	    {
	        using type = alpaka::AccFpgaSyclIntel<TDim, TIdx>;
	    };
	} // namespace alpaka::trait

	#endif
	// ==
	// == ./include/alpaka/acc/AccFpgaSyclIntel.hpp ==
	// ============================================================================

// #include "alpaka/acc/AccGenericSycl.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/acc/AccGpuCudaRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
		// ============================================================================
		// == ./include/alpaka/acc/AccGpuUniformCudaHipRt.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, René Widera, Jan Stephan, Andrea Bocci, Bernhard Manfred Gruber, Antonio Di Pilato
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// Base classes.
		// #include "alpaka/atomic/AtomicHierarchy.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/atomic/AtomicUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, René Widera, Jan Stephan, Andrea Bocci, Bernhard Manfred Gruber, Antonio Di Pilato
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
				// ============================================================================
				// == ./include/alpaka/atomic/AtomicUniformCudaHip.hpp ==
				// ==
				/* Copyright 2022 René Widera
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/atomic/Op.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Utility.hpp"    // amalgamate: file already expanded

				// #include <type_traits>    // amalgamate: file already included

				#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

				namespace alpaka
				{
				    //! The GPU CUDA/HIP accelerator atomic ops.
				    //
				    //  Atomics can be used in the hierarchy level grids, blocks and threads.
				    //  Atomics are not guaranteed to be safe between devices.
				    class AtomicUniformCudaHipBuiltIn
				    {
				    };
				} // namespace alpaka

				#    if !defined(ALPAKA_HOST_ONLY)

				#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
				#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
				#        endif

				#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
				#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
				#        endif

				//! clang is providing a builtin for different atomic functions even if these is not supported for architectures < 6.0
				#        define CLANG_CUDA_PTX_WORKAROUND                                                                             \
				            (BOOST_COMP_CLANG && BOOST_LANG_CUDA && BOOST_ARCH_PTX < BOOST_VERSION_NUMBER(6, 0, 0))

				//! These types must be in the global namespace for checking existence of respective functions in global namespace via
				//! SFINAE, so we use inline namespace.
				inline namespace alpakaGlobal
				{
				    //! Provide an interface to builtin atomic functions.
				    //
				    // To check for the existence of builtin functions located in the global namespace :: directly.
				    // This would not be possible without having these types in global namespace.
				    // If the functor is inheriting from std::false_type an signature is explicitly not available. This can be used to
				    // explicitly disable builtin function in case the builtin is broken.
				    // If the functor is inheriting from std::true_type a specialization must implement one of the following
				    // interfaces.
				    // \code{.cpp}
				    //    // interface for all atomics except atomicCas
				    //    __device__ static T atomic( T* add, T value);
				    //    // interface for atomicCas only
				    //    __device__ static T atomic( T* add, T compare, T value);
				    // \endcode
				    template<typename TOp, typename T, typename THierarchy, typename TSfinae = void>
				    struct AlpakaBuiltInAtomic : std::false_type
				    {
				    };

				    // Cas.
				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicCas,
				        T,
				        THierarchy,
				        typename std::void_t<
				            decltype(atomicCAS(alpaka::core::declval<T*>(), alpaka::core::declval<T>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T compare, T value)
				        {
				            return atomicCAS(add, compare, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicCas,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicCAS_block(
				            alpaka::core::declval<T*>(),
				            alpaka::core::declval<T>(),
				            alpaka::core::declval<T>()))>> : std::true_type
				    {
				        __device__ static T atomic(T* add, T compare, T value)
				        {
				            return atomicCAS_block(add, compare, value);
				        }
				    };
				#        endif


				    // Add.
				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicAdd,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicAdd(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicAdd(add, value);
				        }
				    };


				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicAdd,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicAdd_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicAdd_block(add, value);
				        }
				    };
				#        endif

				#        if CLANG_CUDA_PTX_WORKAROUND
				    // clang is providing a builtin for atomicAdd even if these is not supported by the current architecture
				    template<typename THierarchy>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicAdd, double, THierarchy> : std::false_type
				    {
				    };
				#        endif

				#        if(BOOST_LANG_HIP)
				    // HIP shows bad performance with builtin atomicAdd(float*,float) for the hierarchy threads therefore we do not
				    // call the buildin method and instead use the atomicCAS emulation. For details see:
				    // https://github.com/alpaka-group/alpaka/issues/1657
				    template<>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicAdd, float, alpaka::hierarchy::Threads> : std::false_type
				    {
				    };
				#        endif

				    // Sub.

				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicSub,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicSub(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicSub(add, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicSub,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicSub_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicSub_block(add, value);
				        }
				    };
				#        endif

				    // Min.
				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicMin,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicMin(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicMin(add, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicMin,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicMin_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicMin_block(add, value);
				        }
				    };
				#        endif

				// disable HIP atomicMin: see https://github.com/ROCm-Developer-Tools/hipamd/pull/40
				#        if(BOOST_LANG_HIP)
				    template<typename THierarchy>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMin, float, THierarchy> : std::false_type
				    {
				    };

				    template<>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMin, float, alpaka::hierarchy::Threads> : std::false_type
				    {
				    };

				    template<typename THierarchy>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMin, double, THierarchy> : std::false_type
				    {
				    };

				    template<>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMin, double, alpaka::hierarchy::Threads> : std::false_type
				    {
				    };

				#            if !__has_builtin(__hip_atomic_compare_exchange_strong)
				    template<typename THierarchy>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMin, unsigned long long, THierarchy> : std::false_type
				    {
				    };

				    template<>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMin, unsigned long long, alpaka::hierarchy::Threads> : std::false_type
				    {
				    };
				#            endif
				#        endif

				    // Max.

				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicMax,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicMax(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicMax(add, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicMax,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicMax_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicMax_block(add, value);
				        }
				    };
				#        endif

				    // disable HIP atomicMax: see https://github.com/ROCm-Developer-Tools/hipamd/pull/40
				#        if(BOOST_LANG_HIP)
				    template<typename THierarchy>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMax, float, THierarchy> : std::false_type
				    {
				    };

				    template<>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMax, float, alpaka::hierarchy::Threads> : std::false_type
				    {
				    };

				    template<typename THierarchy>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMax, double, THierarchy> : std::false_type
				    {
				    };

				    template<>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMax, double, alpaka::hierarchy::Threads> : std::false_type
				    {
				    };

				#            if !__has_builtin(__hip_atomic_compare_exchange_strong)
				    template<typename THierarchy>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMax, unsigned long long, THierarchy> : std::false_type
				    {
				    };

				    template<>
				    struct AlpakaBuiltInAtomic<alpaka::AtomicMax, unsigned long long, alpaka::hierarchy::Threads> : std::false_type
				    {
				    };
				#            endif
				#        endif


				    // Exch.

				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicExch,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicExch(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicExch(add, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicExch,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicExch_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicExch_block(add, value);
				        }
				    };
				#        endif

				    // Inc.

				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicInc,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicInc(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicInc(add, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicInc,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicInc_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicInc_block(add, value);
				        }
				    };
				#        endif

				    // Dec.

				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicDec,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicDec(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicDec(add, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicDec,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicDec_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicDec_block(add, value);
				        }
				    };
				#        endif

				    // And.

				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicAnd,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicAnd(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicAnd(add, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicAnd,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicAnd_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicAnd_block(add, value);
				        }
				    };
				#        endif

				    // Or.

				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicOr,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicOr(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicOr(add, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicOr,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicOr_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicOr_block(add, value);
				        }
				    };
				#        endif

				    // Xor.

				    template<typename T, typename THierarchy>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicXor,
				        T,
				        THierarchy,
				        typename std::void_t<decltype(atomicXor(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicXor(add, value);
				        }
				    };

				#        if !CLANG_CUDA_PTX_WORKAROUND
				    template<typename T>
				    struct AlpakaBuiltInAtomic<
				        alpaka::AtomicXor,
				        T,
				        alpaka::hierarchy::Threads,
				        typename std::void_t<decltype(atomicXor_block(alpaka::core::declval<T*>(), alpaka::core::declval<T>()))>>
				        : std::true_type
				    {
				        __device__ static T atomic(T* add, T value)
				        {
				            return atomicXor_block(add, value);
				        }
				    };
				#        endif

				} // namespace alpakaGlobal

				#        undef CLANG_CUDA_PTX_WORKAROUND
				#    endif

				#endif
				// ==
				// == ./include/alpaka/atomic/AtomicUniformCudaHip.hpp ==
				// ============================================================================

			// #include "alpaka/atomic/Op.hpp"    // amalgamate: file already expanded
			// #include "alpaka/atomic/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Decay.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Unreachable.hpp"    // amalgamate: file already expanded

			// #include <limits>    // amalgamate: file already included
			// #include <type_traits>    // amalgamate: file already included

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			namespace alpaka
			{
			    namespace trait
			    {
			        namespace detail
			        {
			            struct EmulationBase
			            {
			                //! reinterprets an address as an 32bit value for atomicCas emulation usage
			                template<typename TAddressType>
			                __device__ static auto reinterpretAddress(TAddressType* address)
			                    -> std::enable_if_t<sizeof(TAddressType) == 4u, unsigned int*>
			                {
			                    return reinterpret_cast<unsigned int*>(address);
			                }

			                //! reinterprets a address as an 64bit value for atomicCas emulation usage
			                template<typename TAddressType>
			                __device__ static auto reinterpretAddress(TAddressType* address)
			                    -> std::enable_if_t<sizeof(TAddressType) == 8u, unsigned long long int*>
			                {
			                    return reinterpret_cast<unsigned long long int*>(address);
			                }

			                //! reinterprets a value to be usable for the atomicCAS emulation
			                template<typename T_Type>
			                __device__ static auto reinterpretValue(T_Type value)
			                {
			                    return *reinterpretAddress(&value);
			                }
			            };
			            //! Emulate atomic
			            //
			            // The default implementation will emulate all atomic functions with atomicCAS.
			            template<
			                typename TOp,
			                typename TAtomic,
			                typename T,
			                typename THierarchy,
			                typename TSfinae = void,
			                typename TDefer = void>
			            struct EmulateAtomic : private EmulationBase
			            {
			            public:
			                __device__ static auto atomic(
			                    alpaka::AtomicUniformCudaHipBuiltIn const& ctx,
			                    T* const addr,
			                    T const& value) -> T
			                {
			                    auto* const addressAsIntegralType = reinterpretAddress(addr);
			                    using EmulatedType = ALPAKA_DECAY_T(decltype(*addressAsIntegralType));

			                    // Emulating atomics with atomicCAS is mentioned in the programming guide too.
			                    // http://docs.nvidia.com/cuda/cuda-c-programming-guide/#atomic-functions
			#        if BOOST_LANG_HIP
			#            if __has_builtin(__hip_atomic_load)
			                    EmulatedType old{
			                        __hip_atomic_load(addressAsIntegralType, __ATOMIC_RELAXED, __HIP_MEMORY_SCOPE_AGENT)};
			#            else
			                    EmulatedType old{__atomic_load_n(addressAsIntegralType, __ATOMIC_RELAXED)};
			#            endif
			#        else
			                    EmulatedType old{*addressAsIntegralType};
			#        endif
			                    EmulatedType assumed;
			                    do
			                    {
			                        assumed = old;
			                        T v = *(reinterpret_cast<T*>(&assumed));
			                        TOp{}(&v, value);
			                        using Cas = alpaka::trait::
			                            AtomicOp<alpaka::AtomicCas, alpaka::AtomicUniformCudaHipBuiltIn, EmulatedType, THierarchy>;
			                        old = Cas::atomicOp(ctx, addressAsIntegralType, assumed, reinterpretValue(v));
			                        // Note: uses integer comparison to avoid hang in case of NaN (since NaN != NaN)
			                    } while(assumed != old);
			                    return *(reinterpret_cast<T*>(&old));
			                }
			            };

			            //! Emulate AtomicCas with equivalent unisigned integral type
			            template<typename T, typename THierarchy>
			            struct EmulateAtomic<alpaka::AtomicCas, alpaka::AtomicUniformCudaHipBuiltIn, T, THierarchy>
			                : private EmulationBase
			            {
			                __device__ static auto atomic(
			                    alpaka::AtomicUniformCudaHipBuiltIn const& ctx,
			                    T* const addr,
			                    T const& compare,
			                    T const& value) -> T
			                {
			                    auto* const addressAsIntegralType = reinterpretAddress(addr);
			                    using EmulatedType = ALPAKA_DECAY_T(decltype(*addressAsIntegralType));
			                    EmulatedType reinterpretedCompare = reinterpretValue(compare);
			                    EmulatedType reinterpretedValue = reinterpretValue(value);

			                    auto old = alpaka::trait::
			                        AtomicOp<alpaka::AtomicCas, alpaka::AtomicUniformCudaHipBuiltIn, EmulatedType, THierarchy>::
			                            atomicOp(ctx, addressAsIntegralType, reinterpretedCompare, reinterpretedValue);

			                    return *(reinterpret_cast<T*>(&old));
			                }
			            };

			            //! Emulate AtomicSub with atomicAdd
			            template<typename T, typename THierarchy>
			            struct EmulateAtomic<alpaka::AtomicSub, alpaka::AtomicUniformCudaHipBuiltIn, T, THierarchy>
			            {
			                __device__ static auto atomic(
			                    alpaka::AtomicUniformCudaHipBuiltIn const& ctx,
			                    T* const addr,
			                    T const& value) -> T
			                {
			                    return alpaka::trait::
			                        AtomicOp<alpaka::AtomicAdd, alpaka::AtomicUniformCudaHipBuiltIn, T, THierarchy>::atomicOp(
			                            ctx,
			                            addr,
			                            -value);
			                }
			            };

			            //! AtomicDec can not be implemented for floating point types!
			            template<typename T, typename THierarchy>
			            struct EmulateAtomic<
			                alpaka::AtomicDec,
			                alpaka::AtomicUniformCudaHipBuiltIn,
			                T,
			                THierarchy,
			                std::enable_if_t<std::is_floating_point_v<T>>>
			            {
			                __device__ static auto atomic(alpaka::AtomicUniformCudaHipBuiltIn const&, T* const, T const&) -> T
			                {
			                    static_assert(
			                        !sizeof(T),
			                        "EmulateAtomic<alpaka::AtomicDec> is not supported for floating point data types!");
			                    return T{};
			                }
			            };

			            //! AtomicInc can not be implemented for floating point types!
			            template<typename T, typename THierarchy>
			            struct EmulateAtomic<
			                alpaka::AtomicInc,
			                alpaka::AtomicUniformCudaHipBuiltIn,
			                T,
			                THierarchy,
			                std::enable_if_t<std::is_floating_point_v<T>>>
			            {
			                __device__ static auto atomic(alpaka::AtomicUniformCudaHipBuiltIn const&, T* const, T const&) -> T
			                {
			                    static_assert(
			                        !sizeof(T),
			                        "EmulateAtomic<alpaka::AtomicInc> is not supported for floating point data types!");
			                    return T{};
			                }
			            };

			            //! AtomicAnd can not be implemented for floating point types!
			            template<typename T, typename THierarchy>
			            struct EmulateAtomic<
			                alpaka::AtomicAnd,
			                alpaka::AtomicUniformCudaHipBuiltIn,
			                T,
			                THierarchy,
			                std::enable_if_t<std::is_floating_point_v<T>>>
			            {
			                __device__ static auto atomic(alpaka::AtomicUniformCudaHipBuiltIn const&, T* const, T const&) -> T
			                {
			                    static_assert(
			                        !sizeof(T),
			                        "EmulateAtomic<alpaka::AtomicAnd> is not supported for floating point data types!");
			                    return T{};
			                }
			            };

			            //! AtomicOr can not be implemented for floating point types!
			            template<typename T, typename THierarchy>
			            struct EmulateAtomic<
			                alpaka::AtomicOr,
			                alpaka::AtomicUniformCudaHipBuiltIn,
			                T,
			                THierarchy,
			                std::enable_if_t<std::is_floating_point_v<T>>>
			            {
			                __device__ static auto atomic(alpaka::AtomicUniformCudaHipBuiltIn const&, T* const, T const&) -> T
			                {
			                    static_assert(
			                        !sizeof(T),
			                        "EmulateAtomic<alpaka::AtomicOr> is not supported for floating point data types!");
			                    return T{};
			                }
			            };

			            //! AtomicXor can not be implemented for floating point types!
			            template<typename T, typename THierarchy>
			            struct EmulateAtomic<
			                alpaka::AtomicXor,
			                alpaka::AtomicUniformCudaHipBuiltIn,
			                T,
			                THierarchy,
			                std::enable_if_t<std::is_floating_point_v<T>>>
			            {
			                __device__ static auto atomic(alpaka::AtomicUniformCudaHipBuiltIn const&, T* const, T const&) -> T
			                {
			                    static_assert(
			                        !sizeof(T),
			                        "EmulateAtomic<alpaka::AtomicXor> is not supported for floating point data types!");
			                    return T{};
			                }
			            };

			        } // namespace detail

			        //! Generic atomic implementation
			        //
			        // - unsigned long int will be redirected to unsigned long long int or unsigned int implementation depending if
			        //   unsigned long int is a 64 or 32bit data type.
			        // - Atomics which are not available as builtin atomic will be emulated.
			        template<typename TOp, typename T, typename THierarchy>
			        struct AtomicOp<TOp, AtomicUniformCudaHipBuiltIn, T, THierarchy>
			        {
			            __device__ static auto atomicOp(
			                AtomicUniformCudaHipBuiltIn const& ctx,
			                [[maybe_unused]] T* const addr,
			                [[maybe_unused]] T const& value) -> T
			            {
			                static_assert(
			                    sizeof(T) == 4u || sizeof(T) == 8u,
			                    "atomicOp<TOp, AtomicUniformCudaHipBuiltIn, T>(atomic, addr, value) is not supported! Only 64 and "
			                    "32bit atomics are supported.");

			                if constexpr(::AlpakaBuiltInAtomic<TOp, T, THierarchy>::value)
			                    return ::AlpakaBuiltInAtomic<TOp, T, THierarchy>::atomic(addr, value);

			                else if constexpr(std::is_same_v<unsigned long int, T>)
			                {
			                    if constexpr(sizeof(T) == 4u && ::AlpakaBuiltInAtomic<TOp, unsigned int, THierarchy>::value)
			                        return ::AlpakaBuiltInAtomic<TOp, unsigned int, THierarchy>::atomic(
			                            reinterpret_cast<unsigned int*>(addr),
			                            static_cast<unsigned int>(value));
			                    else if constexpr(
			                        sizeof(T) == 8u
			                        && ::AlpakaBuiltInAtomic<TOp, unsigned long long int, THierarchy>::value) // LP64
			                    {
			                        return ::AlpakaBuiltInAtomic<TOp, unsigned long long int, THierarchy>::atomic(
			                            reinterpret_cast<unsigned long long int*>(addr),
			                            static_cast<unsigned long long int>(value));
			                    }
			                }

			                return detail::EmulateAtomic<TOp, AtomicUniformCudaHipBuiltIn, T, THierarchy>::atomic(
			                    ctx,
			                    addr,
			                    value);
			            }
			        };

			        template<typename T, typename THierarchy>
			        struct AtomicOp<AtomicCas, AtomicUniformCudaHipBuiltIn, T, THierarchy>
			        {
			            __device__ static auto atomicOp(
			                [[maybe_unused]] AtomicUniformCudaHipBuiltIn const& ctx,
			                [[maybe_unused]] T* const addr,
			                [[maybe_unused]] T const& compare,
			                [[maybe_unused]] T const& value) -> T
			            {
			                static_assert(
			                    sizeof(T) == 4u || sizeof(T) == 8u,
			                    "atomicOp<AtomicCas, AtomicUniformCudaHipBuiltIn, T>(atomic, addr, compare, value) is not "
			                    "supported! Only 64 and "
			                    "32bit atomics are supported.");

			                if constexpr(::AlpakaBuiltInAtomic<AtomicCas, T, THierarchy>::value)
			                    return ::AlpakaBuiltInAtomic<AtomicCas, T, THierarchy>::atomic(addr, compare, value);

			                else if constexpr(std::is_same_v<unsigned long int, T>)
			                {
			                    if constexpr(sizeof(T) == 4u && ::AlpakaBuiltInAtomic<AtomicCas, unsigned int, THierarchy>::value)
			                        return ::AlpakaBuiltInAtomic<AtomicCas, unsigned int, THierarchy>::atomic(
			                            reinterpret_cast<unsigned int*>(addr),
			                            static_cast<unsigned int>(compare),
			                            static_cast<unsigned int>(value));
			                    else if constexpr(
			                        sizeof(T) == 8u
			                        && ::AlpakaBuiltInAtomic<AtomicCas, unsigned long long int, THierarchy>::value) // LP64
			                    {
			                        return ::AlpakaBuiltInAtomic<AtomicCas, unsigned long long int, THierarchy>::atomic(
			                            reinterpret_cast<unsigned long long int*>(addr),
			                            static_cast<unsigned long long int>(compare),
			                            static_cast<unsigned long long int>(value));
			                    }
			                }

			                return detail::EmulateAtomic<AtomicCas, AtomicUniformCudaHipBuiltIn, T, THierarchy>::atomic(
			                    ctx,
			                    addr,
			                    compare,
			                    value);
			            }
			        };

			    } // namespace trait
			} // namespace alpaka
			#    endif
			#endif
			// ==
			// == ./include/alpaka/atomic/AtomicUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/block/shared/dyn/BlockSharedMemDynUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, René Widera, Andrea Bocci, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/block/shared/dyn/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    //! The GPU CUDA/HIP block shared memory allocator.
			    class BlockSharedMemDynUniformCudaHipBuiltIn
			        : public concepts::Implements<ConceptBlockSharedDyn, BlockSharedMemDynUniformCudaHipBuiltIn>
			    {
			    };

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace trait
			    {
			        template<typename T>
			        struct GetDynSharedMem<T, BlockSharedMemDynUniformCudaHipBuiltIn>
			        {
			            __device__ static auto getMem(BlockSharedMemDynUniformCudaHipBuiltIn const&) -> T*
			            {
			                // Because unaligned access to variables is not allowed in device code,
			                // we have to use the widest possible type to have all types aligned correctly.
			                // See: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared
			                // http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#vector-types
			                extern __shared__ float4 shMem[];
			                return reinterpret_cast<T*>(shMem);
			            }
			        };
			    } // namespace trait

			#    endif

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/block/shared/dyn/BlockSharedMemDynUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/block/shared/st/BlockSharedMemStUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Erik Zenker, René Widera, Matthias Werner, Andrea Bocci, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/block/shared/st/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			// #include <cstdint>    // amalgamate: file already included
			// #include <type_traits>    // amalgamate: file already included

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    //! The GPU CUDA/HIP block shared memory allocator.
			    class BlockSharedMemStUniformCudaHipBuiltIn
			        : public concepts::Implements<ConceptBlockSharedSt, BlockSharedMemStUniformCudaHipBuiltIn>
			    {
			    };

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace trait
			    {
			        template<typename T, std::size_t TuniqueId>
			        struct DeclareSharedVar<T, TuniqueId, BlockSharedMemStUniformCudaHipBuiltIn>
			        {
			            __device__ static auto declareVar(BlockSharedMemStUniformCudaHipBuiltIn const&) -> T&
			            {
			                __shared__ uint8_t shMem alignas(alignof(T))[sizeof(T)];
			                return *(reinterpret_cast<T*>(shMem));
			            }
			        };
			        template<>
			        struct FreeSharedVars<BlockSharedMemStUniformCudaHipBuiltIn>
			        {
			            __device__ static auto freeVars(BlockSharedMemStUniformCudaHipBuiltIn const&) -> void
			            {
			                // Nothing to do. CUDA/HIP block shared memory is automatically freed when all threads left the block.
			            }
			        };
			    } // namespace trait

			#    endif

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/block/shared/st/BlockSharedMemStUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/block/sync/BlockSyncUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Matthias Werner, Andrea Bocci, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/block/sync/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    //! The GPU CUDA/HIP block synchronization.
			    class BlockSyncUniformCudaHipBuiltIn
			        : public concepts::Implements<ConceptBlockSync, BlockSyncUniformCudaHipBuiltIn>
			    {
			    };

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace trait
			    {
			        template<>
			        struct SyncBlockThreads<BlockSyncUniformCudaHipBuiltIn>
			        {
			            __device__ static auto syncBlockThreads(BlockSyncUniformCudaHipBuiltIn const& /*blockSync*/) -> void
			            {
			                __syncthreads();
			            }
			        };

			        template<>
			        struct SyncBlockThreadsPredicate<BlockCount, BlockSyncUniformCudaHipBuiltIn>
			        {
			            __device__ static auto syncBlockThreadsPredicate(
			                BlockSyncUniformCudaHipBuiltIn const& /*blockSync*/,
			                int predicate) -> int
			            {
			#        if defined(__HIP_ARCH_HAS_SYNC_THREAD_EXT__) && __HIP_ARCH_HAS_SYNC_THREAD_EXT__ == 0 && BOOST_COMP_HIP
			                // workaround for unsupported syncthreads_* operation on AMD hardware without sync extension
			                __shared__ int tmp;
			                __syncthreads();
			                if(threadIdx.x == 0)
			                    tmp = 0;
			                __syncthreads();
			                if(predicate)
			                    ::atomicAdd(&tmp, 1);
			                __syncthreads();

			                return tmp;
			#        else
			                return __syncthreads_count(predicate);
			#        endif
			            }
			        };

			        template<>
			        struct SyncBlockThreadsPredicate<BlockAnd, BlockSyncUniformCudaHipBuiltIn>
			        {
			            __device__ static auto syncBlockThreadsPredicate(
			                BlockSyncUniformCudaHipBuiltIn const& /*blockSync*/,
			                int predicate) -> int
			            {
			#        if defined(__HIP_ARCH_HAS_SYNC_THREAD_EXT__) && __HIP_ARCH_HAS_SYNC_THREAD_EXT__ == 0 && BOOST_COMP_HIP
			                // workaround for unsupported syncthreads_* operation on AMD hardware without sync extension
			                __shared__ int tmp;
			                __syncthreads();
			                if(threadIdx.x == 0)
			                    tmp = 1;
			                __syncthreads();
			                if(!predicate)
			                    ::atomicAnd(&tmp, 0);
			                __syncthreads();

			                return tmp;
			#        else
			                return __syncthreads_and(predicate);
			#        endif
			            }
			        };

			        template<>
			        struct SyncBlockThreadsPredicate<BlockOr, BlockSyncUniformCudaHipBuiltIn>
			        {
			            __device__ static auto syncBlockThreadsPredicate(
			                BlockSyncUniformCudaHipBuiltIn const& /*blockSync*/,
			                int predicate) -> int
			            {
			#        if defined(__HIP_ARCH_HAS_SYNC_THREAD_EXT__) && __HIP_ARCH_HAS_SYNC_THREAD_EXT__ == 0 && BOOST_COMP_HIP
			                // workaround for unsupported syncthreads_* operation on AMD hardware without sync extension
			                __shared__ int tmp;
			                __syncthreads();
			                if(threadIdx.x == 0)
			                    tmp = 0;
			                __syncthreads();
			                if(predicate)
			                    ::atomicOr(&tmp, 1);
			                __syncthreads();

			                return tmp;
			#        else
			                return __syncthreads_or(predicate);
			#        endif
			            }
			        };
			    } // namespace trait

			#    endif

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/block/sync/BlockSyncUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

		// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/idx/bt/IdxBtUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, René Widera, Jan Stephan, Andrea Bocci, Bernhard
			 * Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/core/Cuda.hpp ==
				// ==
				/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, René Widera, Andrea Bocci, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
					// ============================================================================
					// == ./include/alpaka/core/CudaHipCommon.hpp ==
					// ==
					/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, René Widera, Andrea Bocci, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/elem/Traits.hpp"    // amalgamate: file already expanded
					// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
					// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
						// ============================================================================
						// == ./include/alpaka/meta/Concatenate.hpp ==
						// ==
						/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
						 * SPDX-License-Identifier: MPL-2.0
						 */

						// #pragma once
						namespace alpaka::meta
						{
						    namespace detail
						    {
						        template<typename... T>
						        struct ConcatenateImpl;
						        template<typename T>
						        struct ConcatenateImpl<T>
						        {
						            using type = T;
						        };
						        template<template<typename...> class TList, typename... As, typename... Bs, typename... TRest>
						        struct ConcatenateImpl<TList<As...>, TList<Bs...>, TRest...>
						        {
						            using type = typename ConcatenateImpl<TList<As..., Bs...>, TRest...>::type;
						        };
						    } // namespace detail
						    template<typename... T>
						    using Concatenate = typename detail::ConcatenateImpl<T...>::type;
						} // namespace alpaka::meta
						// ==
						// == ./include/alpaka/meta/Concatenate.hpp ==
						// ============================================================================

					// #include "alpaka/meta/TypeListOps.hpp"    // amalgamate: file already expanded
					// #include "alpaka/offset/Traits.hpp"    // amalgamate: file already expanded
					// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

					// #include <tuple>    // amalgamate: file already included

					#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

					#    ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
					#        include <cuda.h>
					#        include <cuda_runtime.h>
					#    endif

					#    ifdef ALPAKA_ACC_GPU_HIP_ENABLED
					// #        include <hip/hip_runtime.h>    // amalgamate: file already included
					#    endif

					namespace alpaka
					{
					    namespace detail
					    {
					        using CudaHipBuiltinTypes1 = std::
					            tuple<char1, double1, float1, int1, long1, longlong1, short1, uchar1, uint1, ulong1, ulonglong1, ushort1>;
					        using CudaHipBuiltinTypes2 = std::
					            tuple<char2, double2, float2, int2, long2, longlong2, short2, uchar2, uint2, ulong2, ulonglong2, ushort2>;
					        using CudaHipBuiltinTypes3 = std::tuple<
					            char3,
					            dim3,
					            double3,
					            float3,
					            int3,
					            long3,
					            longlong3,
					            short3,
					            uchar3,
					            uint3,
					            ulong3,
					            ulonglong3,
					            ushort3
					// CUDA built-in variables have special types in clang native CUDA compilation
					// defined in cuda_builtin_vars.h
					#    if BOOST_COMP_CLANG_CUDA
					            ,
					            __cuda_builtin_threadIdx_t,
					            __cuda_builtin_blockIdx_t,
					            __cuda_builtin_blockDim_t,
					            __cuda_builtin_gridDim_t
					#    endif
					            >;
					        using CudaHipBuiltinTypes4 = std::
					            tuple<char4, double4, float4, int4, long4, longlong4, short4, uchar4, uint4, ulong4, ulonglong4, ushort4>;
					        using CudaHipBuiltinTypes = meta::
					            Concatenate<CudaHipBuiltinTypes1, CudaHipBuiltinTypes2, CudaHipBuiltinTypes3, CudaHipBuiltinTypes4>;

					        template<typename T>
					        inline constexpr auto isCudaHipBuiltInType = meta::Contains<CudaHipBuiltinTypes, T>::value;
					    } // namespace detail

					#    ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
					    namespace cuda::trait
					    {
					        template<typename T>
					        inline constexpr auto isCudaBuiltInType = alpaka::detail::isCudaHipBuiltInType<T>;
					    } // namespace cuda::trait
					#    endif

					#    ifdef ALPAKA_ACC_GPU_HIP_ENABLED
					    namespace hip::trait
					    {
					        template<typename T>
					        inline constexpr auto isHipBuiltInType = alpaka::detail::isCudaHipBuiltInType<T>;
					    } // namespace hip::trait
					#    endif

					    namespace trait
					    {
					        //! The CUDA/HIP vectors 1D dimension get trait specialization.
					        template<typename T>
					        struct DimType<T, std::enable_if_t<meta::Contains<alpaka::detail::CudaHipBuiltinTypes1, T>::value>>
					        {
					            using type = DimInt<1u>;
					        };
					        //! The CUDA/HIP vectors 2D dimension get trait specialization.
					        template<typename T>
					        struct DimType<T, std::enable_if_t<meta::Contains<alpaka::detail::CudaHipBuiltinTypes2, T>::value>>
					        {
					            using type = DimInt<2u>;
					        };
					        //! The CUDA/HIP vectors 3D dimension get trait specialization.
					        template<typename T>
					        struct DimType<T, std::enable_if_t<meta::Contains<alpaka::detail::CudaHipBuiltinTypes3, T>::value>>
					        {
					            using type = DimInt<3u>;
					        };
					        //! The CUDA/HIP vectors 4D dimension get trait specialization.
					        template<typename T>
					        struct DimType<T, std::enable_if_t<meta::Contains<alpaka::detail::CudaHipBuiltinTypes4, T>::value>>
					        {
					            using type = DimInt<4u>;
					        };

					        //! The CUDA/HIP vectors elem type trait specialization.
					        template<typename T>
					        struct ElemType<T, std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<T>>>
					        {
					            using type = decltype(std::declval<T>().x);
					        };

					        //! The CUDA/HIP vectors extent get trait specialization.
					        template<typename TExtent>
					        struct GetExtent<
					            DimInt<Dim<TExtent>::value - 1u>,
					            TExtent,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TExtent> && (Dim<TExtent>::value >= 1)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const& extent)
					            {
					                return extent.x;
					            }
					        };
					        //! The CUDA/HIP vectors extent get trait specialization.
					        template<typename TExtent>
					        struct GetExtent<
					            DimInt<Dim<TExtent>::value - 2u>,
					            TExtent,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TExtent> && (Dim<TExtent>::value >= 2)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const& extent)
					            {
					                return extent.y;
					            }
					        };
					        //! The CUDA/HIP vectors extent get trait specialization.
					        template<typename TExtent>
					        struct GetExtent<
					            DimInt<Dim<TExtent>::value - 3u>,
					            TExtent,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TExtent> && (Dim<TExtent>::value >= 3)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const& extent)
					            {
					                return extent.z;
					            }
					        };
					        //! The CUDA/HIP vectors extent get trait specialization.
					        template<typename TExtent>
					        struct GetExtent<
					            DimInt<Dim<TExtent>::value - 4u>,
					            TExtent,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TExtent> && (Dim<TExtent>::value >= 4)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const& extent)
					            {
					                return extent.w;
					            }
					        };
					        //! The CUDA/HIP vectors extent set trait specialization.
					        template<typename TExtent, typename TExtentVal>
					        struct SetExtent<
					            DimInt<Dim<TExtent>::value - 1u>,
					            TExtent,
					            TExtentVal,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TExtent> && (Dim<TExtent>::value >= 1)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto setExtent(TExtent const& extent, TExtentVal const& extentVal) -> void
					            {
					                extent.x = extentVal;
					            }
					        };
					        //! The CUDA/HIP vectors extent set trait specialization.
					        template<typename TExtent, typename TExtentVal>
					        struct SetExtent<
					            DimInt<Dim<TExtent>::value - 2u>,
					            TExtent,
					            TExtentVal,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TExtent> && (Dim<TExtent>::value >= 2)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto setExtent(TExtent const& extent, TExtentVal const& extentVal) -> void
					            {
					                extent.y = extentVal;
					            }
					        };
					        //! The CUDA/HIP vectors extent set trait specialization.
					        template<typename TExtent, typename TExtentVal>
					        struct SetExtent<
					            DimInt<Dim<TExtent>::value - 3u>,
					            TExtent,
					            TExtentVal,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TExtent> && (Dim<TExtent>::value >= 3)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto setExtent(TExtent const& extent, TExtentVal const& extentVal) -> void
					            {
					                extent.z = extentVal;
					            }
					        };
					        //! The CUDA/HIP vectors extent set trait specialization.
					        template<typename TExtent, typename TExtentVal>
					        struct SetExtent<
					            DimInt<Dim<TExtent>::value - 4u>,
					            TExtent,
					            TExtentVal,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TExtent> && (Dim<TExtent>::value >= 4)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto setExtent(TExtent const& extent, TExtentVal const& extentVal) -> void
					            {
					                extent.w = extentVal;
					            }
					        };

					        //! The CUDA/HIP vectors offset get trait specialization.
					        template<typename TOffsets>
					        struct GetOffset<
					            DimInt<Dim<TOffsets>::value - 1u>,
					            TOffsets,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 1)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const& offsets)
					            {
					                return offsets.x;
					            }
					        };
					        //! The CUDA/HIP vectors offset get trait specialization.
					        template<typename TOffsets>
					        struct GetOffset<
					            DimInt<Dim<TOffsets>::value - 2u>,
					            TOffsets,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 2)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const& offsets)
					            {
					                return offsets.y;
					            }
					        };
					        //! The CUDA/HIP vectors offset get trait specialization.
					        template<typename TOffsets>
					        struct GetOffset<
					            DimInt<Dim<TOffsets>::value - 3u>,
					            TOffsets,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 3)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const& offsets)
					            {
					                return offsets.z;
					            }
					        };
					        //! The CUDA/HIP vectors offset get trait specialization.
					        template<typename TOffsets>
					        struct GetOffset<
					            DimInt<Dim<TOffsets>::value - 4u>,
					            TOffsets,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 4)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const& offsets)
					            {
					                return offsets.w;
					            }
					        };
					        //! The CUDA/HIP vectors offset set trait specialization.
					        template<typename TOffsets, typename TOffset>
					        struct SetOffset<
					            DimInt<Dim<TOffsets>::value - 1u>,
					            TOffsets,
					            TOffset,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 1)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
					            {
					                offsets.x = offset;
					            }
					        };
					        //! The CUDA/HIP vectors offset set trait specialization.
					        template<typename TOffsets, typename TOffset>
					        struct SetOffset<
					            DimInt<Dim<TOffsets>::value - 2u>,
					            TOffsets,
					            TOffset,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 2)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
					            {
					                offsets.y = offset;
					            }
					        };
					        //! The CUDA/HIP vectors offset set trait specialization.
					        template<typename TOffsets, typename TOffset>
					        struct SetOffset<
					            DimInt<Dim<TOffsets>::value - 3u>,
					            TOffsets,
					            TOffset,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 3)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
					            {
					                offsets.z = offset;
					            }
					        };
					        //! The CUDA/HIP vectors offset set trait specialization.
					        template<typename TOffsets, typename TOffset>
					        struct SetOffset<
					            DimInt<Dim<TOffsets>::value - 4u>,
					            TOffsets,
					            TOffset,
					            std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 4)>>
					        {
					            ALPAKA_NO_HOST_ACC_WARNING
					            ALPAKA_FN_HOST_ACC static auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
					            {
					                offsets.w = offset;
					            }
					        };

					        //! The CUDA/HIP vectors idx type trait specialization.
					        template<typename TIdx>
					        struct IdxType<TIdx, std::enable_if_t<alpaka::detail::isCudaHipBuiltInType<TIdx>>>
					        {
					            using type = std::size_t;
					        };
					    } // namespace trait
					} // namespace alpaka

					#endif
					// ==
					// == ./include/alpaka/core/CudaHipCommon.hpp ==
					// ============================================================================


				// #include <iostream>    // amalgamate: file already included
				// #include <stdexcept>    // amalgamate: file already included
				// #include <string>    // amalgamate: file already included

				#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

				namespace alpaka::cuda::detail
				{
				    //! CUDA driver API error checking with log and exception, ignoring specific error values
				    ALPAKA_FN_HOST inline auto cudaDrvCheck(CUresult const& error, char const* desc, char const* file, int const& line)
				        -> void
				    {
				        if(error == CUDA_SUCCESS)
				            return;

				        char const* cu_err_name = nullptr;
				        char const* cu_err_string = nullptr;
				        CUresult cu_result_name = cuGetErrorName(error, &cu_err_name);
				        CUresult cu_result_string = cuGetErrorString(error, &cu_err_string);
				        std::string sError = std::string(file) + "(" + std::to_string(line) + ") " + std::string(desc) + " : '";
				        if(cu_result_name == CUDA_SUCCESS && cu_result_string == CUDA_SUCCESS)
				        {
				            sError += std::string(cu_err_name) + "': '" + std::string(cu_err_string) + "'!";
				        }
				        else
				        {
				            // cuGetError*() failed, so append corresponding error message
				            if(cu_result_name == CUDA_ERROR_INVALID_VALUE)
				            {
				                sError += " cuGetErrorName: 'Invalid Value'!";
				            }
				            if(cu_result_string == CUDA_ERROR_INVALID_VALUE)
				            {
				                sError += " cuGetErrorString: 'Invalid Value'!";
				            }
				        }
				#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
				        std::cerr << sError << std::endl;
				#    endif
				        ALPAKA_DEBUG_BREAK;
				        throw std::runtime_error(sError);
				    }
				} // namespace alpaka::cuda::detail

				//! CUDA driver error checking with log and exception.
				#    define ALPAKA_CUDA_DRV_CHECK(cmd) ::alpaka::cuda::detail::cudaDrvCheck(cmd, #    cmd, __FILE__, __LINE__)

					// ============================================================================
					// == ./include/alpaka/core/UniformCudaHip.hpp ==
					// ==
					/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, René Widera, Jan Stephan, Andrea Bocci, Bernhard
					 * Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
					// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
						// ============================================================================
						// == ./include/alpaka/core/Hip.hpp ==
						// ==
						/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, René Widera, Bernhard Manfred Gruber
						 * SPDX-License-Identifier: MPL-2.0
						 */

						// #pragma once
						// #include "alpaka/core/CudaHipCommon.hpp"    // amalgamate: file already expanded
						// #include "alpaka/core/UniformCudaHip.hpp"    // amalgamate: file already expanded

						#ifdef ALPAKA_ACC_GPU_HIP_ENABLED
						#    if !BOOST_LANG_HIP && !defined(ALPAKA_HOST_ONLY)
						#        error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
						#    endif
						#endif
						// ==
						// == ./include/alpaka/core/Hip.hpp ==
						// ============================================================================


					// #include <array>    // amalgamate: file already included
					// #include <stdexcept>    // amalgamate: file already included
					// #include <string>    // amalgamate: file already included
					// #include <tuple>    // amalgamate: file already included
					// #include <type_traits>    // amalgamate: file already included

					#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

					namespace alpaka::uniform_cuda_hip::detail
					{
					    //! CUDA/HIP runtime API error checking with log and exception, ignoring specific error values
					    template<typename TApi, bool TThrow>
					    ALPAKA_FN_HOST inline void rtCheck(
					        typename TApi::Error_t const& error,
					        char const* desc,
					        char const* file,
					        int const& line) noexcept(!TThrow)
					    {
					        if(error != TApi::success)
					        {
					            auto const sError = std::string{
					                std::string(file) + "(" + std::to_string(line) + ") " + std::string(desc) + " : '"
					                + TApi::getErrorName(error) + "': '" + std::string(TApi::getErrorString(error)) + "'!"};

					            if constexpr(!TThrow || ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL)
					                std::cerr << sError << std::endl;

					            ALPAKA_DEBUG_BREAK;
					            // reset the last error to allow user side error handling. Using std::ignore to discard unneeded
					            // return values is suggested by the C++ core guidelines.
					            std::ignore = TApi::getLastError();

					            if constexpr(TThrow)
					                throw std::runtime_error(sError);
					        }
					    }

					    //! CUDA/HIP runtime API error checking with log and exception, ignoring specific error values
					    // NOTE: All ignored errors have to be convertible to TApi::Error_t.
					    template<typename TApi, bool TThrow, typename... TErrors>
					    ALPAKA_FN_HOST inline void rtCheckIgnore(
					        typename TApi::Error_t const& error,
					        char const* cmd,
					        char const* file,
					        int const& line,
					        TErrors&&... ignoredErrorCodes) noexcept(!TThrow)
					    {
					        if(error != TApi::success)
					        {
					            std::array<typename TApi::Error_t, sizeof...(ignoredErrorCodes)> const aIgnoredErrorCodes{
					                {ignoredErrorCodes...}};

					            // If the error code is not one of the ignored ones.
					            if(std::find(std::cbegin(aIgnoredErrorCodes), std::cend(aIgnoredErrorCodes), error)
					               == std::cend(aIgnoredErrorCodes))
					            {
					                rtCheck<TApi, TThrow>(error, ("'" + std::string(cmd) + "' returned error ").c_str(), file, line);
					            }
					            else
					            {
					                // reset the last error to avoid propagation to the next CUDA/HIP API call. Using std::ignore
					                // to discard unneeded return values is recommended by the C++ core guidelines.
					                std::ignore = TApi::getLastError();
					            }
					        }
					    }

					    //! CUDA/HIP runtime API last error checking with log and exception.
					    template<typename TApi, bool TThrow>
					    ALPAKA_FN_HOST inline void rtCheckLastError(char const* desc, char const* file, int const& line) noexcept(!TThrow)
					    {
					        typename TApi::Error_t const error(TApi::getLastError());
					        rtCheck<TApi, TThrow>(error, desc, file, line);
					    }
					} // namespace alpaka::uniform_cuda_hip::detail

					#    if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
					//! CUDA/HIP runtime error checking with log and exception, ignoring specific error values
					#        define ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_IGNORE(cmd, ...)                                                     \
					            do                                                                                                        \
					            {                                                                                                         \
					                ::alpaka::uniform_cuda_hip::detail::rtCheckLastError<TApi, true>(                                     \
					                    "'" #cmd "' A previous API call (not this one) set the error ",                                   \
					                    __FILE__,                                                                                         \
					                    __LINE__);                                                                                        \
					                ::alpaka::uniform_cuda_hip::detail::rtCheckIgnore<TApi, true>(                                        \
					                    cmd,                                                                                              \
					                    #cmd,                                                                                             \
					                    __FILE__,                                                                                         \
					                    __LINE__,                                                                                         \
					                    __VA_ARGS__);                                                                                     \
					            } while(0)
					#    else
					#        if BOOST_COMP_CLANG
					#            pragma clang diagnostic push
					#            pragma clang diagnostic ignored "-Wgnu-zero-variadic-macro-arguments"
					#        endif
					//! CUDA/HIP runtime error checking with log and exception, ignoring specific error values
					#        define ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_IGNORE(cmd, ...)                                                     \
					            do                                                                                                        \
					            {                                                                                                         \
					                ::alpaka::uniform_cuda_hip::detail::rtCheckLastError<TApi, true>(                                     \
					                    "'" #cmd "' A previous API call (not this one) set the error ",                                   \
					                    __FILE__,                                                                                         \
					                    __LINE__);                                                                                        \
					                ::alpaka::uniform_cuda_hip::detail::rtCheckIgnore<TApi, true>(                                        \
					                    cmd,                                                                                              \
					                    #cmd,                                                                                             \
					                    __FILE__,                                                                                         \
					                    __LINE__,                                                                                         \
					                    ##__VA_ARGS__);                                                                                   \
					            } while(0)
					#        if BOOST_COMP_CLANG
					#            pragma clang diagnostic pop
					#        endif
					#    endif

					//! CUDA/HIP runtime error checking with log and exception.
					#    define ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(cmd) ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_IGNORE(cmd)

					#    if BOOST_COMP_MSVC || defined(BOOST_COMP_MSVC_EMULATED)
					//! CUDA/HIP runtime error checking with log, ignoring specific error values
					#        define ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_IGNORE_NOEXCEPT(cmd, ...)                                            \
					            do                                                                                                        \
					            {                                                                                                         \
					                ::alpaka::uniform_cuda_hip::detail::rtCheckLastError<TApi, false>(                                    \
					                    "'" #cmd "' A previous API call (not this one) set the error ",                                   \
					                    __FILE__,                                                                                         \
					                    __LINE__);                                                                                        \
					                ::alpaka::uniform_cuda_hip::detail::rtCheckIgnore<TApi, false>(                                       \
					                    cmd,                                                                                              \
					                    #cmd,                                                                                             \
					                    __FILE__,                                                                                         \
					                    __LINE__,                                                                                         \
					                    __VA_ARGS__);                                                                                     \
					            } while(0)
					#    else
					#        if BOOST_COMP_CLANG
					#            pragma clang diagnostic push
					#            pragma clang diagnostic ignored "-Wgnu-zero-variadic-macro-arguments"
					#        endif
					//! CUDA/HIP runtime error checking with log and exception, ignoring specific error values
					#        define ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_IGNORE_NOEXCEPT(cmd, ...)                                            \
					            do                                                                                                        \
					            {                                                                                                         \
					                ::alpaka::uniform_cuda_hip::detail::rtCheckLastError<TApi, false>(                                    \
					                    "'" #cmd "' A previous API call (not this one) set the error ",                                   \
					                    __FILE__,                                                                                         \
					                    __LINE__);                                                                                        \
					                ::alpaka::uniform_cuda_hip::detail::rtCheckIgnore<TApi, false>(                                       \
					                    cmd,                                                                                              \
					                    #cmd,                                                                                             \
					                    __FILE__,                                                                                         \
					                    __LINE__,                                                                                         \
					                    ##__VA_ARGS__);                                                                                   \
					            } while(0)
					#        if BOOST_COMP_CLANG
					#            pragma clang diagnostic pop
					#        endif
					#    endif

					//! CUDA/HIP runtime error checking with log.
					#    define ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_NOEXCEPT(cmd) ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_IGNORE_NOEXCEPT(cmd)
					#endif
					// ==
					// == ./include/alpaka/core/UniformCudaHip.hpp ==
					// ============================================================================


				#endif
				// ==
				// == ./include/alpaka/core/Cuda.hpp ==
				// ============================================================================

			// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    namespace bt
			    {
			        //! The CUDA/HIP accelerator ND index provider.
			        template<typename TDim, typename TIdx>
			        class IdxBtUniformCudaHipBuiltIn
			            : public concepts::Implements<ConceptIdxBt, IdxBtUniformCudaHipBuiltIn<TDim, TIdx>>
			        {
			        };
			    } // namespace bt

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace trait
			    {
			        //! The GPU CUDA/HIP accelerator index dimension get trait specialization.
			        template<typename TDim, typename TIdx>
			        struct DimType<bt::IdxBtUniformCudaHipBuiltIn<TDim, TIdx>>
			        {
			            using type = TDim;
			        };

			        //! The GPU CUDA/HIP accelerator block thread index get trait specialization.
			        template<typename TDim, typename TIdx>
			        struct GetIdx<bt::IdxBtUniformCudaHipBuiltIn<TDim, TIdx>, origin::Block, unit::Threads>
			        {
			            //! \return The index of the current thread in the block.
			            template<typename TWorkDiv>
			            __device__ static auto getIdx(bt::IdxBtUniformCudaHipBuiltIn<TDim, TIdx> const& /* idx */, TWorkDiv const&)
			                -> Vec<TDim, TIdx>
			            {
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                return castVec<TIdx>(getOffsetVecEnd<TDim>(threadIdx));
			#        else
			                return getOffsetVecEnd<TDim>(Vec<std::integral_constant<typename TDim::value_type, 3>, TIdx>(
			                    static_cast<TIdx>(hipThreadIdx_z),
			                    static_cast<TIdx>(hipThreadIdx_y),
			                    static_cast<TIdx>(hipThreadIdx_x)));
			#        endif
			            }
			        };

			        //! The GPU CUDA/HIP accelerator block thread index idx type trait specialization.
			        template<typename TDim, typename TIdx>
			        struct IdxType<bt::IdxBtUniformCudaHipBuiltIn<TDim, TIdx>>
			        {
			            using type = TIdx;
			        };
			    } // namespace trait

			#    endif

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/idx/bt/IdxBtUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/idx/gb/IdxGbUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, René Widera, Matthias Werner, Jan Stephan, Andrea Bocci, Bernhard
			 * Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    namespace gb
			    {
			        //! The CUDA/HIP accelerator ND index provider.
			        template<typename TDim, typename TIdx>
			        class IdxGbUniformCudaHipBuiltIn
			            : public concepts::Implements<ConceptIdxGb, IdxGbUniformCudaHipBuiltIn<TDim, TIdx>>
			        {
			        };
			    } // namespace gb

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace trait
			    {
			        //! The GPU CUDA/HIP accelerator index dimension get trait specialization.
			        template<typename TDim, typename TIdx>
			        struct DimType<gb::IdxGbUniformCudaHipBuiltIn<TDim, TIdx>>
			        {
			            using type = TDim;
			        };

			        //! The GPU CUDA/HIP accelerator grid block index get trait specialization.
			        template<typename TDim, typename TIdx>
			        struct GetIdx<gb::IdxGbUniformCudaHipBuiltIn<TDim, TIdx>, origin::Grid, unit::Blocks>
			        {
			            //! \return The index of the current block in the grid.
			            template<typename TWorkDiv>
			            __device__ static auto getIdx(gb::IdxGbUniformCudaHipBuiltIn<TDim, TIdx> const& /* idx */, TWorkDiv const&)
			                -> Vec<TDim, TIdx>
			            {
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                return castVec<TIdx>(getOffsetVecEnd<TDim>(blockIdx));
			#        else
			                return getOffsetVecEnd<TDim>(Vec<std::integral_constant<typename TDim::value_type, 3>, TIdx>(
			                    static_cast<TIdx>(hipBlockIdx_z),
			                    static_cast<TIdx>(hipBlockIdx_y),
			                    static_cast<TIdx>(hipBlockIdx_x)));
			#        endif
			            }
			        };

			        //! The GPU CUDA/HIP accelerator grid block index idx type trait specialization.
			        template<typename TDim, typename TIdx>
			        struct IdxType<gb::IdxGbUniformCudaHipBuiltIn<TDim, TIdx>>
			        {
			            using type = TIdx;
			        };
			    } // namespace trait

			#    endif

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/idx/gb/IdxGbUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/intrinsic/IntrinsicUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Sergei Bastrakov, Andrea Bocci, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/intrinsic/Traits.hpp"    // amalgamate: file already expanded

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    //! The GPU CUDA/HIP intrinsic.
			    class IntrinsicUniformCudaHipBuiltIn
			        : public concepts::Implements<ConceptIntrinsic, IntrinsicUniformCudaHipBuiltIn>
			    {
			    };

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace trait
			    {
			        template<>
			        struct Popcount<IntrinsicUniformCudaHipBuiltIn>
			        {
			            __device__ static auto popcount(IntrinsicUniformCudaHipBuiltIn const& /*intrinsic*/, std::uint32_t value)
			                -> std::int32_t
			            {
			#        if BOOST_COMP_CLANG && BOOST_LANG_CUDA
			                return __popc(static_cast<int>(value));
			#        else
			                return static_cast<std::int32_t>(__popc(static_cast<unsigned int>(value)));
			#        endif
			            }

			            __device__ static auto popcount(IntrinsicUniformCudaHipBuiltIn const& /*intrinsic*/, std::uint64_t value)
			                -> std::int32_t
			            {
			#        if BOOST_COMP_CLANG && BOOST_LANG_CUDA
			                return __popcll(static_cast<long long>(value));
			#        else
			                return static_cast<std::int32_t>(__popcll(static_cast<unsigned long long>(value)));
			#        endif
			            }
			        };

			        template<>
			        struct Ffs<IntrinsicUniformCudaHipBuiltIn>
			        {
			            __device__ static auto ffs(IntrinsicUniformCudaHipBuiltIn const& /*intrinsic*/, std::int32_t value)
			                -> std::int32_t
			            {
			                return static_cast<std::int32_t>(__ffs(static_cast<int>(value)));
			            }

			            __device__ static auto ffs(IntrinsicUniformCudaHipBuiltIn const& /*intrinsic*/, std::int64_t value)
			                -> std::int32_t
			            {
			                return static_cast<std::int32_t>(__ffsll(static_cast<long long>(value)));
			            }
			        };
			    } // namespace trait

			#    endif

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/intrinsic/IntrinsicUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/math/MathUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Bert Wesarg, Valentin Gehrke, René Widera,
			 * Jan Stephan, Andrea Bocci, Bernhard Manfred Gruber, Jeffrey Kelling, Sergei Bastrakov
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/CudaHipCommon.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Decay.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/UniformCudaHip.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Unreachable.hpp"    // amalgamate: file already expanded
			// #include "alpaka/math/Complex.hpp"    // amalgamate: file already expanded
			// #include "alpaka/math/Traits.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka::math
			{
			    //! The CUDA built in abs.
			    class AbsUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathAbs, AbsUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in acos.
			    class AcosUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathAcos, AcosUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in acosh.
			    class AcoshUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathAcosh, AcoshUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in arg.
			    class ArgUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathArg, ArgUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in asin.
			    class AsinUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathAsin, AsinUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in asinh.
			    class AsinhUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathAsinh, AsinhUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in atan.
			    class AtanUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathAtan, AtanUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in atanh.
			    class AtanhUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathAtanh, AtanhUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in atan2.
			    class Atan2UniformCudaHipBuiltIn : public concepts::Implements<ConceptMathAtan2, Atan2UniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in cbrt.
			    class CbrtUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathCbrt, CbrtUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in ceil.
			    class CeilUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathCeil, CeilUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in conj.
			    class ConjUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathConj, ConjUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in cos.
			    class CosUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathCos, CosUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in cosh.
			    class CoshUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathCosh, CoshUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in erf.
			    class ErfUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathErf, ErfUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in exp.
			    class ExpUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathExp, ExpUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in floor.
			    class FloorUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathFloor, FloorUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in fmod.
			    class FmodUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathFmod, FmodUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in isfinite.
			    class IsfiniteUniformCudaHipBuiltIn
			        : public concepts::Implements<ConceptMathIsfinite, IsfiniteUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in isinf.
			    class IsinfUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathIsinf, IsinfUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in isnan.
			    class IsnanUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathIsnan, IsnanUniformCudaHipBuiltIn>
			    {
			    };

			    // ! The CUDA built in log.
			    class LogUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathLog, LogUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in max.
			    class MaxUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathMax, MaxUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in min.
			    class MinUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathMin, MinUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in pow.
			    class PowUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathPow, PowUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA built in remainder.
			    class RemainderUniformCudaHipBuiltIn
			        : public concepts::Implements<ConceptMathRemainder, RemainderUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA round.
			    class RoundUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathRound, RoundUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA rsqrt.
			    class RsqrtUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathRsqrt, RsqrtUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA sin.
			    class SinUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathSin, SinUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA sinh.
			    class SinhUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathSinh, SinhUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA sincos.
			    class SinCosUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathSinCos, SinCosUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA sqrt.
			    class SqrtUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathSqrt, SqrtUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA tan.
			    class TanUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathTan, TanUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA tanh.
			    class TanhUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathTanh, TanhUniformCudaHipBuiltIn>
			    {
			    };

			    //! The CUDA trunc.
			    class TruncUniformCudaHipBuiltIn : public concepts::Implements<ConceptMathTrunc, TruncUniformCudaHipBuiltIn>
			    {
			    };

			    //! The standard library math trait specializations.
			    class MathUniformCudaHipBuiltIn
			        : public AbsUniformCudaHipBuiltIn
			        , public AcosUniformCudaHipBuiltIn
			        , public AcoshUniformCudaHipBuiltIn
			        , public ArgUniformCudaHipBuiltIn
			        , public AsinUniformCudaHipBuiltIn
			        , public AsinhUniformCudaHipBuiltIn
			        , public AtanUniformCudaHipBuiltIn
			        , public AtanhUniformCudaHipBuiltIn
			        , public Atan2UniformCudaHipBuiltIn
			        , public CbrtUniformCudaHipBuiltIn
			        , public CeilUniformCudaHipBuiltIn
			        , public ConjUniformCudaHipBuiltIn
			        , public CosUniformCudaHipBuiltIn
			        , public CoshUniformCudaHipBuiltIn
			        , public ErfUniformCudaHipBuiltIn
			        , public ExpUniformCudaHipBuiltIn
			        , public FloorUniformCudaHipBuiltIn
			        , public FmodUniformCudaHipBuiltIn
			        , public LogUniformCudaHipBuiltIn
			        , public MaxUniformCudaHipBuiltIn
			        , public MinUniformCudaHipBuiltIn
			        , public PowUniformCudaHipBuiltIn
			        , public RemainderUniformCudaHipBuiltIn
			        , public RoundUniformCudaHipBuiltIn
			        , public RsqrtUniformCudaHipBuiltIn
			        , public SinUniformCudaHipBuiltIn
			        , public SinhUniformCudaHipBuiltIn
			        , public SinCosUniformCudaHipBuiltIn
			        , public SqrtUniformCudaHipBuiltIn
			        , public TanUniformCudaHipBuiltIn
			        , public TanhUniformCudaHipBuiltIn
			        , public TruncUniformCudaHipBuiltIn
			        , public IsnanUniformCudaHipBuiltIn
			        , public IsinfUniformCudaHipBuiltIn
			        , public IsfiniteUniformCudaHipBuiltIn
			    {
			    };

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && defined(__CUDA_ARCH__)
			// #            include <cuda_runtime.h>    // amalgamate: file already included
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && defined(__HIP_DEVICE_COMPILE__)
			#            include <hip/math_functions.h>
			#        endif

			    namespace trait
			    {
			        //! The CUDA abs trait specialization for real types.
			        template<typename TArg>
			        struct Abs<AbsUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_signed_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(AbsUniformCudaHipBuiltIn const& /* abs_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::fabsf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::fabs(arg);
			                else if constexpr(is_decayed_v<TArg, int>)
			                    return ::abs(arg);
			                else if constexpr(is_decayed_v<TArg, long int>)
			                    return ::labs(arg);
			                else if constexpr(is_decayed_v<TArg, long long int>)
			                    return ::llabs(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA abs trait specialization for complex types.
			        template<typename T>
			        struct Abs<AbsUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                return sqrt(ctx, arg.real() * arg.real() + arg.imag() * arg.imag());
			            }
			        };

			        //! The CUDA acos trait specialization for real types.
			        template<typename TArg>
			        struct Acos<AcosUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(AcosUniformCudaHipBuiltIn const& /* acos_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::acosf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::acos(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA acos trait specialization for complex types.
			        template<typename T>
			        struct Acos<AcosUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // This holds everywhere, including the branch cuts: acos(z) = -i * ln(z + i * sqrt(1 - z^2))
			                return Complex<T>{0.0, -1.0} * log(ctx, arg + Complex<T>{0.0, 1.0} * sqrt(ctx, T(1.0) - arg * arg));
			            }
			        };

			        //! The CUDA acosh trait specialization for real types.
			        template<typename TArg>
			        struct Acosh<AcoshUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(AcoshUniformCudaHipBuiltIn const& /* acosh_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::acoshf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::acosh(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA acosh trait specialization for complex types.
			        template<typename T>
			        struct Acosh<AcoshUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // acos(z) = ln(z + sqrt(z-1) * sqrt(z+1))
			                return log(ctx, arg + sqrt(ctx, arg - static_cast<T>(1.0)) * sqrt(ctx, arg + static_cast<T>(1.0)));
			            }
			        };

			        //! The CUDA arg trait specialization for real types.
			        template<typename TArgument>
			        struct Arg<ArgUniformCudaHipBuiltIn, TArgument, std::enable_if_t<std::is_floating_point_v<TArgument>>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, TArgument const& argument)
			            {
			                // Fall back to atan2 so that boundary cases are resolved consistently
			                return atan2(ctx, TArgument{0.0}, argument);
			            }
			        };

			        //! The CUDA arg Complex<T> specialization for complex types.
			        template<typename T>
			        struct Arg<ArgUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& argument)
			            {
			                return atan2(ctx, argument.imag(), argument.real());
			            }
			        };

			        //! The CUDA asin trait specialization for real types.
			        template<typename TArg>
			        struct Asin<AsinUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(AsinUniformCudaHipBuiltIn const& /* asin_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::asinf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::asin(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA asin trait specialization for complex types.
			        template<typename T>
			        struct Asin<AsinUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // This holds everywhere, including the branch cuts: asin(z) = i * ln(sqrt(1 - z^2) - i * z)
			                return Complex<T>{0.0, 1.0} * log(ctx, sqrt(ctx, T(1.0) - arg * arg) - Complex<T>{0.0, 1.0} * arg);
			            }
			        };

			        //! The CUDA asinh trait specialization for real types.
			        template<typename TArg>
			        struct Asinh<AsinhUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(AsinhUniformCudaHipBuiltIn const& /* asinh_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::asinhf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::asinh(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA asinh trait specialization for complex types.
			        template<typename T>
			        struct Asinh<AsinhUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // asinh(z) = ln(z + sqrt(z^2 + 1))
			                return log(ctx, arg + sqrt(ctx, arg * arg + static_cast<T>(1.0)));
			            }
			        };

			        //! The CUDA atan trait specialization for real types.
			        template<typename TArg>
			        struct Atan<AtanUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(AtanUniformCudaHipBuiltIn const& /* atan_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::atanf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::atan(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA atan trait specialization for complex types.
			        template<typename T>
			        struct Atan<AtanUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // This holds everywhere, including the branch cuts: atan(z) = -i/2 * ln((i - z) / (i + z))
			                return Complex<T>{0.0, -0.5} * log(ctx, (Complex<T>{0.0, 1.0} - arg) / (Complex<T>{0.0, 1.0} + arg));
			            }
			        };

			        //! The CUDA atanh trait specialization for real types.
			        template<typename TArg>
			        struct Atanh<AtanhUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(AtanhUniformCudaHipBuiltIn const& /* atanh_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::atanhf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::atanh(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA atanh trait specialization for complex types.
			        template<typename T>
			        struct Atanh<AtanhUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                //  atanh(z) = 0.5 * (ln(1 + z) - ln(1 - z))
			                return static_cast<T>(0.5)
			                       * (log(ctx, static_cast<T>(1.0) + arg) - log(ctx, static_cast<T>(1.0) - arg));
			            }
			        };

			        //! The CUDA atan2 trait specialization.
			        template<typename Ty, typename Tx>
			        struct Atan2<
			            Atan2UniformCudaHipBuiltIn,
			            Ty,
			            Tx,
			            std::enable_if_t<std::is_floating_point_v<Ty> && std::is_floating_point_v<Tx>>>
			        {
			            __host__ __device__ auto operator()(
			                Atan2UniformCudaHipBuiltIn const& /* atan2_ctx */,
			                Ty const& y,
			                Tx const& x)
			            {
			                if constexpr(is_decayed_v<Ty, float> && is_decayed_v<Tx, float>)
			                    return ::atan2f(y, x);
			                else if constexpr(is_decayed_v<Ty, double> || is_decayed_v<Tx, double>)
			                    return ::atan2(y, x);
			                else
			                    static_assert(!sizeof(Ty), "Unsupported data type");

			                ALPAKA_UNREACHABLE(Ty{});
			            }
			        };

			        //! The CUDA cbrt trait specialization.
			        template<typename TArg>
			        struct Cbrt<CbrtUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_arithmetic_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(CbrtUniformCudaHipBuiltIn const& /* cbrt_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::cbrtf(arg);
			                else if constexpr(is_decayed_v<TArg, double> || std::is_integral_v<TArg>)
			                    return ::cbrt(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA ceil trait specialization.
			        template<typename TArg>
			        struct Ceil<CeilUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(CeilUniformCudaHipBuiltIn const& /* ceil_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::ceilf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::ceil(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA conj trait specialization for real types.
			        template<typename TArg>
			        struct Conj<ConjUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(ConjUniformCudaHipBuiltIn const& /* conj_ctx */, TArg const& arg)
			            {
			                return Complex<TArg>{arg, TArg{0.0}};
			            }
			        };

			        //! The CUDA conj specialization for complex types.
			        template<typename T>
			        struct Conj<ConjUniformCudaHipBuiltIn, Complex<T>>
			        {
			            __host__ __device__ auto operator()(ConjUniformCudaHipBuiltIn const& /* conj_ctx */, Complex<T> const& arg)
			            {
			                return Complex<T>{arg.real(), -arg.imag()};
			            }
			        };

			        //! The CUDA cos trait specialization for real types.
			        template<typename TArg>
			        struct Cos<CosUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(CosUniformCudaHipBuiltIn const& /* cos_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::cosf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::cos(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA cos trait specialization for complex types.
			        template<typename T>
			        struct Cos<CosUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // cos(z) = 0.5 * (exp(i * z) + exp(-i * z))
			                return T(0.5) * (exp(ctx, Complex<T>{0.0, 1.0} * arg) + exp(ctx, Complex<T>{0.0, -1.0} * arg));
			            }
			        };

			        //! The CUDA cosh trait specialization for real types.
			        template<typename TArg>
			        struct Cosh<CoshUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(CoshUniformCudaHipBuiltIn const& /* cos_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::coshf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::cosh(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA cosh trait specialization for complex types.
			        template<typename T>
			        struct Cosh<CoshUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // cosh(z) = 0.5 * (exp(z) + exp(-z))
			                return T(0.5) * (exp(ctx, arg) + exp(ctx, static_cast<T>(-1.0) * arg));
			            }
			        };

			        //! The CUDA erf trait specialization.
			        template<typename TArg>
			        struct Erf<ErfUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(ErfUniformCudaHipBuiltIn const& /* erf_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::erff(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::erf(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA exp trait specialization for real types.
			        template<typename TArg>
			        struct Exp<ExpUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(ExpUniformCudaHipBuiltIn const& /* exp_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::expf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::exp(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA exp trait specialization for complex types.
			        template<typename T>
			        struct Exp<ExpUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // exp(z) = exp(x + iy) = exp(x) * (cos(y) + i * sin(y))
			                auto re = T{}, im = T{};
			                sincos(ctx, arg.imag(), im, re);
			                return exp(ctx, arg.real()) * Complex<T>{re, im};
			            }
			        };

			        //! The CUDA floor trait specialization.
			        template<typename TArg>
			        struct Floor<FloorUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(FloorUniformCudaHipBuiltIn const& /* floor_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::floorf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::floor(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA fmod trait specialization.
			        template<typename Tx, typename Ty>
			        struct Fmod<
			            FmodUniformCudaHipBuiltIn,
			            Tx,
			            Ty,
			            std::enable_if_t<std::is_floating_point_v<Tx> && std::is_floating_point_v<Ty>>>
			        {
			            __host__ __device__ auto operator()(
			                FmodUniformCudaHipBuiltIn const& /* fmod_ctx */,
			                Tx const& x,
			                Ty const& y)
			            {
			                if constexpr(is_decayed_v<Tx, float> && is_decayed_v<Ty, float>)
			                    return ::fmodf(x, y);
			                else if constexpr(is_decayed_v<Tx, double> || is_decayed_v<Ty, double>)
			                    return ::fmod(x, y);
			                else
			                    static_assert(!sizeof(Tx), "Unsupported data type");

			                using Ret [[maybe_unused]]
			                = std::conditional_t<is_decayed_v<Tx, float> && is_decayed_v<Ty, float>, float, double>;
			                ALPAKA_UNREACHABLE(Ret{});
			            }
			        };

			        //! The CUDA isfinite trait specialization.
			        template<typename TArg>
			        struct Isfinite<IsfiniteUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(IsfiniteUniformCudaHipBuiltIn const& /* ctx */, TArg const& arg)
			            {
			                return ::isfinite(arg);
			            }
			        };

			        //! The CUDA isinf trait specialization.
			        template<typename TArg>
			        struct Isinf<IsinfUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(IsinfUniformCudaHipBuiltIn const& /* ctx */, TArg const& arg)
			            {
			                return ::isinf(arg);
			            }
			        };

			        //! The CUDA isnan trait specialization.
			        template<typename TArg>
			        struct Isnan<IsnanUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(IsnanUniformCudaHipBuiltIn const& /* ctx */, TArg const& arg)
			            {
			                return ::isnan(arg);
			            }
			        };

			        //! The CUDA log trait specialization for real types.
			        template<typename TArg>
			        struct Log<LogUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(LogUniformCudaHipBuiltIn const& /* log_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::logf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::log(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA log trait specialization for complex types.
			        template<typename T>
			        struct Log<LogUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& argument)
			            {
			                // Branch cut along the negative real axis (same as for std::complex),
			                // principal value of ln(z) = ln(|z|) + i * arg(z)
			                return log(ctx, abs(ctx, argument)) + Complex<T>{0.0, 1.0} * arg(ctx, argument);
			            }
			        };

			        //! The CUDA max trait specialization.
			        template<typename Tx, typename Ty>
			        struct Max<
			            MaxUniformCudaHipBuiltIn,
			            Tx,
			            Ty,
			            std::enable_if_t<std::is_arithmetic_v<Tx> && std::is_arithmetic_v<Ty>>>
			        {
			            __host__ __device__ auto operator()(
			                MaxUniformCudaHipBuiltIn const& /* max_ctx */,
			                Tx const& x,
			                Ty const& y)
			            {
			                if constexpr(std::is_integral_v<Tx> && std::is_integral_v<Ty>)
			                    return ::max(x, y);
			                else if constexpr(is_decayed_v<Tx, float> && is_decayed_v<Ty, float>)
			                    return ::fmaxf(x, y);
			                else if constexpr(
			                    is_decayed_v<
			                        Tx,
			                        double> || is_decayed_v<Ty, double> || (is_decayed_v<Tx, float> && std::is_integral_v<Ty>)
			                    || (std::is_integral_v<Tx> && is_decayed_v<Ty, float>) )
			                    return ::fmax(x, y);
			                else
			                    static_assert(!sizeof(Tx), "Unsupported data type");

			                using Ret [[maybe_unused]] = std::conditional_t<
			                    std::is_integral_v<Tx> && std::is_integral_v<Ty>,
			                    decltype(::max(x, y)),
			                    std::conditional_t<is_decayed_v<Tx, float> && is_decayed_v<Ty, float>, float, double>>;
			                ALPAKA_UNREACHABLE(Ret{});
			            }
			        };

			        //! The CUDA min trait specialization.
			        template<typename Tx, typename Ty>
			        struct Min<
			            MinUniformCudaHipBuiltIn,
			            Tx,
			            Ty,
			            std::enable_if_t<std::is_arithmetic_v<Tx> && std::is_arithmetic_v<Ty>>>
			        {
			            __host__ __device__ auto operator()(
			                MinUniformCudaHipBuiltIn const& /* min_ctx */,
			                Tx const& x,
			                Ty const& y)
			            {
			                if constexpr(std::is_integral_v<Tx> && std::is_integral_v<Ty>)
			                    return ::min(x, y);
			                else if constexpr(is_decayed_v<Tx, float> && is_decayed_v<Ty, float>)
			                    return ::fminf(x, y);
			                else if constexpr(
			                    is_decayed_v<
			                        Tx,
			                        double> || is_decayed_v<Ty, double> || (is_decayed_v<Tx, float> && std::is_integral_v<Ty>)
			                    || (std::is_integral_v<Tx> && is_decayed_v<Ty, float>) )
			                    return ::fmin(x, y);
			                else
			                    static_assert(!sizeof(Tx), "Unsupported data type");

			                using Ret [[maybe_unused]] = std::conditional_t<
			                    std::is_integral_v<Tx> && std::is_integral_v<Ty>,
			                    decltype(::min(x, y)),
			                    std::conditional_t<is_decayed_v<Tx, float> && is_decayed_v<Ty, float>, float, double>>;
			                ALPAKA_UNREACHABLE(Ret{});
			            }
			        };

			        //! The CUDA pow trait specialization for real types.
			        template<typename TBase, typename TExp>
			        struct Pow<
			            PowUniformCudaHipBuiltIn,
			            TBase,
			            TExp,
			            std::enable_if_t<std::is_floating_point_v<TBase> && std::is_floating_point_v<TExp>>>
			        {
			            __host__ __device__ auto operator()(
			                PowUniformCudaHipBuiltIn const& /* pow_ctx */,
			                TBase const& base,
			                TExp const& exp)
			            {
			                if constexpr(is_decayed_v<TBase, float> && is_decayed_v<TExp, float>)
			                    return ::powf(base, exp);
			                else if constexpr(is_decayed_v<TBase, double> || is_decayed_v<TExp, double>)
			                    return ::pow(static_cast<double>(base), static_cast<double>(exp));
			                else
			                    static_assert(!sizeof(TBase), "Unsupported data type");

			                using Ret [[maybe_unused]]
			                = std::conditional_t<is_decayed_v<TBase, float> && is_decayed_v<TExp, float>, float, double>;
			                ALPAKA_UNREACHABLE(Ret{});
			            }
			        };

			        //! The CUDA pow trait specialization for complex types.
			        template<typename T, typename U>
			        struct Pow<PowUniformCudaHipBuiltIn, Complex<T>, Complex<U>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& base, Complex<U> const& exponent)
			            {
			                // Type promotion matching rules of complex std::pow but simplified given our math only supports float
			                // and double, no long double.
			                using Promoted
			                    = Complex<std::conditional_t<is_decayed_v<T, float> && is_decayed_v<U, float>, float, double>>;
			                // pow(z1, z2) = e^(z2 * log(z1))
			                return exp(ctx, Promoted{exponent} * log(ctx, Promoted{base}));
			            }
			        };

			        //! The CUDA pow trait specialization for complex and real types.
			        template<typename T, typename U>
			        struct Pow<PowUniformCudaHipBuiltIn, Complex<T>, U>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& base, U const& exponent)
			            {
			                return pow(ctx, base, Complex<U>{exponent});
			            }
			        };

			        //! The CUDA pow trait specialization for real and complex types.
			        template<typename T, typename U>
			        struct Pow<PowUniformCudaHipBuiltIn, T, Complex<U>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, T const& base, Complex<U> const& exponent)
			            {
			                return pow(ctx, Complex<T>{base}, exponent);
			            }
			        };

			        //! The CUDA remainder trait specialization.
			        template<typename Tx, typename Ty>
			        struct Remainder<
			            RemainderUniformCudaHipBuiltIn,
			            Tx,
			            Ty,
			            std::enable_if_t<std::is_floating_point_v<Tx> && std::is_floating_point_v<Ty>>>
			        {
			            __host__ __device__ auto operator()(
			                RemainderUniformCudaHipBuiltIn const& /* remainder_ctx */,
			                Tx const& x,
			                Ty const& y)
			            {
			                if constexpr(is_decayed_v<Tx, float> && is_decayed_v<Ty, float>)
			                    return ::remainderf(x, y);
			                else if constexpr(is_decayed_v<Tx, double> || is_decayed_v<Ty, double>)
			                    return ::remainder(x, y);
			                else
			                    static_assert(!sizeof(Tx), "Unsupported data type");

			                using Ret [[maybe_unused]]
			                = std::conditional_t<is_decayed_v<Tx, float> && is_decayed_v<Ty, float>, float, double>;
			                ALPAKA_UNREACHABLE(Ret{});
			            }
			        };


			        //! The CUDA round trait specialization.
			        template<typename TArg>
			        struct Round<RoundUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(RoundUniformCudaHipBuiltIn const& /* round_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::roundf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::round(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA lround trait specialization.
			        template<typename TArg>
			        struct Lround<RoundUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(RoundUniformCudaHipBuiltIn const& /* lround_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::lroundf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::lround(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(long{});
			            }
			        };

			        //! The CUDA llround trait specialization.
			        template<typename TArg>
			        struct Llround<RoundUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(RoundUniformCudaHipBuiltIn const& /* llround_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::llroundf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::llround(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                // NVCC versions before 11.3 are unable to compile 'long long{}': "type name is not allowed".
			                using Ret [[maybe_unused]] = long long;
			                ALPAKA_UNREACHABLE(Ret{});
			            }
			        };

			        //! The CUDA rsqrt trait specialization for real types.
			        template<typename TArg>
			        struct Rsqrt<RsqrtUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_arithmetic_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(RsqrtUniformCudaHipBuiltIn const& /* rsqrt_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::rsqrtf(arg);
			                else if constexpr(is_decayed_v<TArg, double> || std::is_integral_v<TArg>)
			                    return ::rsqrt(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA rsqrt trait specialization for complex types.
			        template<typename T>
			        struct Rsqrt<RsqrtUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                return T{1.0} / sqrt(ctx, arg);
			            }
			        };

			        //! The CUDA sin trait specialization for real types.
			        template<typename TArg>
			        struct Sin<SinUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(SinUniformCudaHipBuiltIn const& /* sin_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::sinf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::sin(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA sin trait specialization for complex types.
			        template<typename T>
			        struct Sin<SinUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // sin(z) = (exp(i * z) - exp(-i * z)) / 2i
			                return (exp(ctx, Complex<T>{0.0, 1.0} * arg) - exp(ctx, Complex<T>{0.0, -1.0} * arg))
			                       / Complex<T>{0.0, 2.0};
			            }
			        };

			        //! The CUDA sinh trait specialization for real types.
			        template<typename TArg>
			        struct Sinh<SinhUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(SinhUniformCudaHipBuiltIn const& /* sinh_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::sinhf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::sinh(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA sinh trait specialization for complex types.
			        template<typename T>
			        struct Sinh<SinhUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // sinh(z) = (exp(z) - exp(-i * z)) / 2
			                return (exp(ctx, arg) - exp(ctx, static_cast<T>(-1.0) * arg)) / static_cast<T>(2.0);
			            }
			        };

			        //! The CUDA sincos trait specialization for real types.
			        template<typename TArg>
			        struct SinCos<SinCosUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(
			                SinCosUniformCudaHipBuiltIn const& /* sincos_ctx */,
			                TArg const& arg,
			                TArg& result_sin,
			                TArg& result_cos) -> void
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    ::sincosf(arg, &result_sin, &result_cos);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    ::sincos(arg, &result_sin, &result_cos);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");
			            }
			        };

			        //! The CUDA sincos trait specialization for complex types.
			        template<typename T>
			        struct SinCos<SinCosUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(
			                TCtx const& ctx,
			                Complex<T> const& arg,
			                Complex<T>& result_sin,
			                Complex<T>& result_cos) -> void
			            {
			                result_sin = sin(ctx, arg);
			                result_cos = cos(ctx, arg);
			            }
			        };

			        //! The CUDA sqrt trait specialization for real types.
			        template<typename TArg>
			        struct Sqrt<SqrtUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_arithmetic_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(SqrtUniformCudaHipBuiltIn const& /* sqrt_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::sqrtf(arg);
			                else if constexpr(is_decayed_v<TArg, double> || std::is_integral_v<TArg>)
			                    return ::sqrt(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA sqrt trait specialization for complex types.
			        template<typename T>
			        struct Sqrt<SqrtUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& argument)
			            {
			                // Branch cut along the negative real axis (same as for std::complex),
			                // principal value of sqrt(z) = sqrt(|z|) * e^(i * arg(z) / 2)
			                auto const halfArg = T(0.5) * arg(ctx, argument);
			                auto re = T{}, im = T{};
			                sincos(ctx, halfArg, im, re);
			                return sqrt(ctx, abs(ctx, argument)) * Complex<T>(re, im);
			            }
			        };

			        //! The CUDA tan trait specialization for real types.
			        template<typename TArg>
			        struct Tan<TanUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(TanUniformCudaHipBuiltIn const& /* tan_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::tanf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::tan(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA tan trait specialization for complex types.
			        template<typename T>
			        struct Tan<TanUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // tan(z) = i * (e^-iz - e^iz) / (e^-iz + e^iz) = i * (1 - e^2iz) / (1 + e^2iz)
			                // Warning: this straightforward implementation can easily result in NaN as 0/0 or inf/inf.
			                auto const expValue = exp(ctx, Complex<T>{0.0, 2.0} * arg);
			                return Complex<T>{0.0, 1.0} * (T{1.0} - expValue) / (T{1.0} + expValue);
			            }
			        };

			        //! The CUDA tanh trait specialization for real types.
			        template<typename TArg>
			        struct Tanh<TanhUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(TanhUniformCudaHipBuiltIn const& /* tanh_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::tanhf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::tanh(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };

			        //! The CUDA tanh trait specialization for complex types.
			        template<typename T>
			        struct Tanh<TanhUniformCudaHipBuiltIn, Complex<T>>
			        {
			            //! Take context as original (accelerator) type, since we call other math functions
			            template<typename TCtx>
			            __host__ __device__ auto operator()(TCtx const& ctx, Complex<T> const& arg)
			            {
			                // tanh(z) = (e^z - e^-z)/(e^z+e^-z)
			                return (exp(ctx, arg) - exp(ctx, static_cast<T>(-1.0) * arg))
			                       / (exp(ctx, arg) + exp(ctx, static_cast<T>(-1.0) * arg));
			            }
			        };

			        //! The CUDA trunc trait specialization.
			        template<typename TArg>
			        struct Trunc<TruncUniformCudaHipBuiltIn, TArg, std::enable_if_t<std::is_floating_point_v<TArg>>>
			        {
			            __host__ __device__ auto operator()(TruncUniformCudaHipBuiltIn const& /* trunc_ctx */, TArg const& arg)
			            {
			                if constexpr(is_decayed_v<TArg, float>)
			                    return ::truncf(arg);
			                else if constexpr(is_decayed_v<TArg, double>)
			                    return ::trunc(arg);
			                else
			                    static_assert(!sizeof(TArg), "Unsupported data type");

			                ALPAKA_UNREACHABLE(TArg{});
			            }
			        };
			    } // namespace trait
			#    endif
			} // namespace alpaka::math

			#endif
			// ==
			// == ./include/alpaka/math/MathUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/mem/fence/MemFenceUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan, Andrea Bocci, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/mem/fence/Traits.hpp"    // amalgamate: file already expanded

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    //! The GPU CUDA/HIP memory fence.
			    class MemFenceUniformCudaHipBuiltIn : public concepts::Implements<ConceptMemFence, MemFenceUniformCudaHipBuiltIn>
			    {
			    };

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace trait
			    {
			        template<>
			        struct MemFence<MemFenceUniformCudaHipBuiltIn, memory_scope::Block>
			        {
			            __device__ static auto mem_fence(MemFenceUniformCudaHipBuiltIn const&, memory_scope::Block const&)
			            {
			                __threadfence_block();
			            }
			        };

			        template<>
			        struct MemFence<MemFenceUniformCudaHipBuiltIn, memory_scope::Grid>
			        {
			            __device__ static auto mem_fence(MemFenceUniformCudaHipBuiltIn const&, memory_scope::Grid const&)
			            {
			                // CUDA and HIP do not have a per-grid memory fence, so a device-level fence is used
			                __threadfence();
			            }
			        };

			        template<>
			        struct MemFence<MemFenceUniformCudaHipBuiltIn, memory_scope::Device>
			        {
			            __device__ static auto mem_fence(MemFenceUniformCudaHipBuiltIn const&, memory_scope::Device const&)
			            {
			                __threadfence();
			            }
			        };
			    } // namespace trait

			#    endif

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/mem/fence/MemFenceUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/rand/RandUniformCudaHipRand.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, René Widera, Andrea Bocci, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/dev/DevUniformCudaHipRt.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, Andrea Bocci, Bernhard Manfred Gruber, Antonio Di Pilato, Jan Stephan
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
				// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/mem/buf/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/queue/Properties.hpp"    // amalgamate: file already expanded
				// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/traits/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

				// #include <cstddef>    // amalgamate: file already included
				// #include <string>    // amalgamate: file already included
				// #include <vector>    // amalgamate: file already included

				#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

				namespace alpaka
				{
				    namespace trait
				    {
				        template<typename TPltf, typename TSfinae>
				        struct GetDevByIdx;
				    }

				    namespace uniform_cuda_hip::detail
				    {
				        template<typename TApi, bool TBlocking>
				        class QueueUniformCudaHipRt;
				    }

				    template<typename TApi>
				    using QueueUniformCudaHipRtBlocking = uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, true>;

				    template<typename TApi>
				    using QueueUniformCudaHipRtNonBlocking = uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, false>;

				    template<typename TApi>
				    class PltfUniformCudaHipRt;

				    template<typename TApi, typename TElem, typename TDim, typename TIdx>
				    class BufUniformCudaHipRt;

				    //! The CUDA/HIP RT device handle.
				    template<typename TApi>
				    class DevUniformCudaHipRt
				        : public concepts::Implements<ConceptCurrentThreadWaitFor, DevUniformCudaHipRt<TApi>>
				        , public concepts::Implements<ConceptDev, DevUniformCudaHipRt<TApi>>
				    {
				        friend struct trait::GetDevByIdx<PltfUniformCudaHipRt<TApi>>;

				    protected:
				        DevUniformCudaHipRt() = default;

				    public:
				        ALPAKA_FN_HOST auto operator==(DevUniformCudaHipRt const& rhs) const -> bool
				        {
				            return m_iDevice == rhs.m_iDevice;
				        }
				        ALPAKA_FN_HOST auto operator!=(DevUniformCudaHipRt const& rhs) const -> bool
				        {
				            return !((*this) == rhs);
				        }

				        [[nodiscard]] auto getNativeHandle() const noexcept -> int
				        {
				            return m_iDevice;
				        }

				    private:
				        DevUniformCudaHipRt(int iDevice) : m_iDevice(iDevice)
				        {
				        }
				        int m_iDevice;
				    };

				    namespace trait
				    {
				        //! The CUDA/HIP RT device name get trait specialization.
				        template<typename TApi>
				        struct GetName<DevUniformCudaHipRt<TApi>>
				        {
				            ALPAKA_FN_HOST static auto getName(DevUniformCudaHipRt<TApi> const& dev) -> std::string
				            {
				                // There is cuda/hip-DeviceGetAttribute as faster alternative to cuda/hip-GetDeviceProperties to get a
				                // single device property but it has no option to get the name
				                typename TApi::DeviceProp_t devProp;
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::getDeviceProperties(&devProp, dev.getNativeHandle()));

				                return std::string(devProp.name);
				            }
				        };

				        //! The CUDA/HIP RT device available memory get trait specialization.
				        template<typename TApi>
				        struct GetMemBytes<DevUniformCudaHipRt<TApi>>
				        {
				            ALPAKA_FN_HOST static auto getMemBytes(DevUniformCudaHipRt<TApi> const& dev) -> std::size_t
				            {
				                // Set the current device to wait for.
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(dev.getNativeHandle()));

				                std::size_t freeInternal(0u);
				                std::size_t totalInternal(0u);

				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::memGetInfo(&freeInternal, &totalInternal));

				                return totalInternal;
				            }
				        };

				        //! The CUDA/HIP RT device free memory get trait specialization.
				        template<typename TApi>
				        struct GetFreeMemBytes<DevUniformCudaHipRt<TApi>>
				        {
				            ALPAKA_FN_HOST static auto getFreeMemBytes(DevUniformCudaHipRt<TApi> const& dev) -> std::size_t
				            {
				                // Set the current device to wait for.
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(dev.getNativeHandle()));

				                std::size_t freeInternal(0u);
				                std::size_t totalInternal(0u);

				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::memGetInfo(&freeInternal, &totalInternal));

				                return freeInternal;
				            }
				        };

				        //! The CUDA/HIP RT device warp size get trait specialization.
				        template<typename TApi>
				        struct GetWarpSizes<DevUniformCudaHipRt<TApi>>
				        {
				            ALPAKA_FN_HOST static auto getWarpSizes(DevUniformCudaHipRt<TApi> const& dev) -> std::vector<std::size_t>
				            {
				                typename TApi::DeviceProp_t devProp;
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::getDeviceProperties(&devProp, dev.getNativeHandle()));

				                return {static_cast<std::size_t>(devProp.warpSize)};
				            }
				        };

				        //! The CUDA/HIP RT device reset trait specialization.
				        template<typename TApi>
				        struct Reset<DevUniformCudaHipRt<TApi>>
				        {
				            ALPAKA_FN_HOST static auto reset(DevUniformCudaHipRt<TApi> const& dev) -> void
				            {
				                ALPAKA_DEBUG_FULL_LOG_SCOPE;

				                // Set the current device to wait for.
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(dev.getNativeHandle()));
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceReset());
				            }
				        };

				        //! The CUDA/HIP RT device native handle trait specialization.
				        template<typename TApi>
				        struct NativeHandle<DevUniformCudaHipRt<TApi>>
				        {
				            [[nodiscard]] static auto getNativeHandle(DevUniformCudaHipRt<TApi> const& dev)
				            {
				                return dev.getNativeHandle();
				            }
				        };

				        //! The CUDA/HIP RT device memory buffer type trait specialization.
				        template<typename TApi, typename TElem, typename TDim, typename TIdx>
				        struct BufType<DevUniformCudaHipRt<TApi>, TElem, TDim, TIdx>
				        {
				            using type = BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>;
				        };

				        //! The CUDA/HIP RT device platform type trait specialization.
				        template<typename TApi>
				        struct PltfType<DevUniformCudaHipRt<TApi>>
				        {
				            using type = PltfUniformCudaHipRt<TApi>;
				        };

				        //! The thread CUDA/HIP device wait specialization.
				        //!
				        //! Blocks until the device has completed all preceding requested tasks.
				        //! Tasks that are enqueued or queues that are created after this call is made are not waited for.
				        template<typename TApi>
				        struct CurrentThreadWaitFor<DevUniformCudaHipRt<TApi>>
				        {
				            ALPAKA_FN_HOST static auto currentThreadWaitFor(DevUniformCudaHipRt<TApi> const& dev) -> void
				            {
				                ALPAKA_DEBUG_FULL_LOG_SCOPE;

				                // Set the current device to wait for.
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(dev.getNativeHandle()));
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceSynchronize());
				            }
				        };

				        template<typename TApi>
				        struct QueueType<DevUniformCudaHipRt<TApi>, Blocking>
				        {
				            using type = QueueUniformCudaHipRtBlocking<TApi>;
				        };

				        template<typename TApi>
				        struct QueueType<DevUniformCudaHipRt<TApi>, NonBlocking>
				        {
				            using type = QueueUniformCudaHipRtNonBlocking<TApi>;
				        };
				    } // namespace trait
				} // namespace alpaka

				#endif
				// ==
				// == ./include/alpaka/dev/DevUniformCudaHipRt.hpp ==
				// ============================================================================

			// #include "alpaka/rand/Traits.hpp"    // amalgamate: file already expanded

			// #include <type_traits>    // amalgamate: file already included

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			#    if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			#        include <curand_kernel.h>
			#    elif defined(ALPAKA_ACC_GPU_HIP_ENABLED)
			#        if BOOST_COMP_CLANG
			#            pragma clang diagnostic push
			#            pragma clang diagnostic ignored "-Wduplicate-decl-specifier"
			#        endif

			#        if HIP_VERSION >= 50200000
			#            include <hiprand/hiprand_kernel.h>
			#        else
			#            include <hiprand_kernel.h>
			#        endif

			#        if BOOST_COMP_CLANG
			#            pragma clang diagnostic pop
			#        endif
			#    endif

			namespace alpaka::rand
			{
			    //! The CUDA/HIP rand implementation.
			    template<typename TApi>
			    class RandUniformCudaHipRand : public concepts::Implements<ConceptRand, RandUniformCudaHipRand<TApi>>
			    {
			    };

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace distribution::uniform_cuda_hip
			    {
			        //! The CUDA/HIP random number floating point normal distribution.
			        template<typename T>
			        class NormalReal;

			        //! The CUDA/HIP random number floating point uniform distribution.
			        template<typename T>
			        class UniformReal;

			        //! The CUDA/HIP random number integer uniform distribution.
			        template<typename T>
			        class UniformUint;
			    } // namespace distribution::uniform_cuda_hip

			    namespace engine::uniform_cuda_hip
			    {
			        //! The CUDA/HIP Xor random number generator engine.
			        class Xor
			        {
			        public:
			            // After calling this constructor the instance is not valid initialized and
			            // need to be overwritten with a valid object
			            Xor() = default;

			            __device__ Xor(
			                std::uint32_t const& seed,
			                std::uint32_t const& subsequence = 0,
			                std::uint32_t const& offset = 0)
			            {
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			                curand_init(seed, subsequence, offset, &state);
			#        else
			                hiprand_init(seed, subsequence, offset, &state);
			#        endif
			            }

			        private:
			            template<typename T>
			            friend class distribution::uniform_cuda_hip::NormalReal;
			            template<typename T>
			            friend class distribution::uniform_cuda_hip::UniformReal;
			            template<typename T>
			            friend class distribution::uniform_cuda_hip::UniformUint;

			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			            curandStateXORWOW_t state = curandStateXORWOW_t{};
			#        else
			            hiprandStateXORWOW_t state = hiprandStateXORWOW_t{};
			#        endif

			        public:
			            // STL UniformRandomBitGenerator concept. This is not strictly necessary as the distributions
			            // contained in this file are aware of the API specifics of the CUDA/HIP XORWOW engine and STL
			            // distributions might not work on the device, but it servers a compatibility bridge to other
			            // potentially compatible alpaka distributions.
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			            using result_type = decltype(curand(&state));
			#        else
			            using result_type = decltype(hiprand(&state));
			#        endif
			            ALPAKA_FN_HOST_ACC constexpr static result_type min()
			            {
			                return std::numeric_limits<result_type>::min();
			            }
			            ALPAKA_FN_HOST_ACC constexpr static result_type max()
			            {
			                return std::numeric_limits<result_type>::max();
			            }
			            __device__ result_type operator()()
			            {
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			                return curand(&state);
			#        else
			                return hiprand(&state);
			#        endif
			            }
			        };
			    } // namespace engine::uniform_cuda_hip

			    namespace distribution::uniform_cuda_hip
			    {
			        //! The CUDA/HIP random number float normal distribution.
			        template<>
			        class NormalReal<float>
			        {
			        public:
			            template<typename TEngine>
			            __device__ auto operator()(TEngine& engine) -> float
			            {
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			                return curand_normal(&engine.state);
			#        else
			                return hiprand_normal(&engine.state);
			#        endif
			            }
			        };

			        //! The CUDA/HIP random number float normal distribution.
			        template<>
			        class NormalReal<double>
			        {
			        public:
			            template<typename TEngine>
			            __device__ auto operator()(TEngine& engine) -> double
			            {
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			                return curand_normal_double(&engine.state);
			#        else
			                return hiprand_normal_double(&engine.state);
			#        endif
			            }
			        };

			        //! The CUDA/HIP random number float uniform distribution.
			        template<>
			        class UniformReal<float>
			        {
			        public:
			            template<typename TEngine>
			            __device__ auto operator()(TEngine& engine) -> float
			            {
			                // (0.f, 1.0f]
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			                float const fUniformRand(curand_uniform(&engine.state));
			#        else
			                float const fUniformRand(hiprand_uniform(&engine.state));
			#        endif
			                // NOTE: (1.0f - curand_uniform) does not work, because curand_uniform seems to return
			                // denormalized floats around 0.f. [0.f, 1.0f)
			                return fUniformRand * static_cast<float>(fUniformRand != 1.0f);
			            }
			        };

			        //! The CUDA/HIP random number float uniform distribution.
			        template<>
			        class UniformReal<double>
			        {
			        public:
			            template<typename TEngine>
			            __device__ auto operator()(TEngine& engine) -> double
			            {
			                // (0.f, 1.0f]
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			                double const fUniformRand(curand_uniform_double(&engine.state));
			#        else
			                double const fUniformRand(hiprand_uniform_double(&engine.state));
			#        endif
			                // NOTE: (1.0f - curand_uniform_double) does not work, because curand_uniform_double seems to
			                // return denormalized floats around 0.f. [0.f, 1.0f)
			                return fUniformRand * static_cast<double>(fUniformRand != 1.0);
			            }
			        };

			        //! The CUDA/HIP random number unsigned integer uniform distribution.
			        template<>
			        class UniformUint<unsigned int>
			        {
			        public:
			            template<typename TEngine>
			            __device__ auto operator()(TEngine& engine) -> unsigned int
			            {
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			                return curand(&engine.state);
			#        else
			                return hiprand(&engine.state);
			#        endif
			            }
			        };
			    } // namespace distribution::uniform_cuda_hip

			    namespace distribution::trait
			    {
			        //! The CUDA/HIP random number float normal distribution get trait specialization.
			        template<typename TApi, typename T>
			        struct CreateNormalReal<RandUniformCudaHipRand<TApi>, T, std::enable_if_t<std::is_floating_point_v<T>>>
			        {
			            __device__ static auto createNormalReal(RandUniformCudaHipRand<TApi> const& /*rand*/)
			                -> uniform_cuda_hip::NormalReal<T>
			            {
			                return {};
			            }
			        };

			        //! The CUDA/HIP random number float uniform distribution get trait specialization.
			        template<typename TApi, typename T>
			        struct CreateUniformReal<RandUniformCudaHipRand<TApi>, T, std::enable_if_t<std::is_floating_point_v<T>>>
			        {
			            __device__ static auto createUniformReal(RandUniformCudaHipRand<TApi> const& /*rand*/)
			                -> uniform_cuda_hip::UniformReal<T>
			            {
			                return {};
			            }
			        };

			        //! The CUDA/HIP random number integer uniform distribution get trait specialization.
			        template<typename TApi, typename T>
			        struct CreateUniformUint<RandUniformCudaHipRand<TApi>, T, std::enable_if_t<std::is_integral_v<T>>>
			        {
			            __device__ static auto createUniformUint(RandUniformCudaHipRand<TApi> const& /*rand*/)
			                -> uniform_cuda_hip::UniformUint<T>
			            {
			                return {};
			            }
			        };
			    } // namespace distribution::trait

			    namespace engine::trait
			    {
			        //! The CUDA/HIP random number default generator get trait specialization.
			        template<typename TApi>
			        struct CreateDefault<RandUniformCudaHipRand<TApi>>
			        {
			            __device__ static auto createDefault(
			                RandUniformCudaHipRand<TApi> const& /*rand*/,
			                std::uint32_t const& seed = 0,
			                std::uint32_t const& subsequence = 0,
			                std::uint32_t const& offset = 0) -> uniform_cuda_hip::Xor
			            {
			                return {seed, subsequence, offset};
			            }
			        };
			    } // namespace engine::trait
			#    endif
			} // namespace alpaka::rand

			#endif
			// ==
			// == ./include/alpaka/rand/RandUniformCudaHipRand.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/warp/WarpUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Sergei Bastrakov, David M. Rogers, Jan Stephan, Andrea Bocci, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/warp/Traits.hpp"    // amalgamate: file already expanded

			// #include <cstdint>    // amalgamate: file already included

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka::warp
			{
			    //! The GPU CUDA/HIP warp.
			    class WarpUniformCudaHipBuiltIn : public concepts::Implements<ConceptWarp, WarpUniformCudaHipBuiltIn>
			    {
			    };

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace trait
			    {
			        template<>
			        struct GetSize<WarpUniformCudaHipBuiltIn>
			        {
			            __device__ static auto getSize(warp::WarpUniformCudaHipBuiltIn const& /*warp*/) -> std::int32_t
			            {
			                return warpSize;
			            }
			        };

			        template<>
			        struct Activemask<WarpUniformCudaHipBuiltIn>
			        {
			            __device__ static auto activemask(warp::WarpUniformCudaHipBuiltIn const& /*warp*/)
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                -> std::uint32_t
			#        else
			                -> std::uint64_t
			#        endif
			            {
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                return __activemask();
			#        else
			                // No HIP intrinsic for it, emulate via ballot
			                return __ballot(1);
			#        endif
			            }
			        };

			        template<>
			        struct All<WarpUniformCudaHipBuiltIn>
			        {
			            __device__ static auto all(
			                [[maybe_unused]] warp::WarpUniformCudaHipBuiltIn const& warp,
			                std::int32_t predicate) -> std::int32_t
			            {
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                return __all_sync(activemask(warp), predicate);
			#        else
			                return __all(predicate);
			#        endif
			            }
			        };

			        template<>
			        struct Any<WarpUniformCudaHipBuiltIn>
			        {
			            __device__ static auto any(
			                [[maybe_unused]] warp::WarpUniformCudaHipBuiltIn const& warp,
			                std::int32_t predicate) -> std::int32_t
			            {
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                return __any_sync(activemask(warp), predicate);
			#        else
			                return __any(predicate);
			#        endif
			            }
			        };

			        template<>
			        struct Ballot<WarpUniformCudaHipBuiltIn>
			        {
			            __device__ static auto ballot(
			                [[maybe_unused]] warp::WarpUniformCudaHipBuiltIn const& warp,
			                std::int32_t predicate)
			            // return type is required by the compiler
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                -> std::uint32_t
			#        else
			                -> std::uint64_t
			#        endif
			            {
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                return __ballot_sync(activemask(warp), predicate);
			#        else
			                return __ballot(predicate);
			#        endif
			            }
			        };

			        template<>
			        struct Shfl<WarpUniformCudaHipBuiltIn>
			        {
			            //-------------------------------------------------------------
			            __device__ static auto shfl(
			                [[maybe_unused]] warp::WarpUniformCudaHipBuiltIn const& warp,
			                float val,
			                int srcLane,
			                std::int32_t width) -> float
			            {
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                return __shfl_sync(activemask(warp), val, srcLane, width);
			#        else
			                return __shfl(val, srcLane, width);
			#        endif
			            }
			            //-------------------------------------------------------------
			            __device__ static auto shfl(
			                [[maybe_unused]] warp::WarpUniformCudaHipBuiltIn const& warp,
			                std::int32_t val,
			                int srcLane,
			                std::int32_t width) -> std::int32_t
			            {
			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
			                return __shfl_sync(activemask(warp), val, srcLane, width);
			#        else
			                return __shfl(val, srcLane, width);
			#        endif
			            }
			        };
			    } // namespace trait
			#    endif
			} // namespace alpaka::warp

			#endif
			// ==
			// == ./include/alpaka/warp/WarpUniformCudaHipBuiltIn.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/workdiv/WorkDivUniformCudaHipBuiltIn.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jan Stephan, Andrea Bocci, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
			// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded
			// #include "alpaka/workdiv/Traits.hpp"    // amalgamate: file already expanded

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    //! The GPU CUDA/HIP accelerator work division.
			    template<typename TDim, typename TIdx>
			    class WorkDivUniformCudaHipBuiltIn
			        : public concepts::Implements<ConceptWorkDiv, WorkDivUniformCudaHipBuiltIn<TDim, TIdx>>
			    {
			    public:
			        ALPAKA_FN_HOST_ACC WorkDivUniformCudaHipBuiltIn(Vec<TDim, TIdx> const& threadElemExtent)
			            : m_threadElemExtent(threadElemExtent)
			        {
			        }

			        // \TODO: Optimize! Add WorkDivUniformCudaHipBuiltInNoElems that has no member m_threadElemExtent as well as
			        // AccGpuUniformCudaHipRtNoElems. Use it instead of AccGpuUniformCudaHipRt if the thread element extent is one
			        // to reduce the register usage.
			        Vec<TDim, TIdx> const& m_threadElemExtent;
			    };

			#    if !defined(ALPAKA_HOST_ONLY)

			#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
			#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
			#        endif

			#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
			#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
			#        endif

			    namespace trait
			    {
			        //! The GPU CUDA/HIP accelerator work division dimension get trait specialization.
			        template<typename TDim, typename TIdx>
			        struct DimType<WorkDivUniformCudaHipBuiltIn<TDim, TIdx>>
			        {
			            using type = TDim;
			        };

			        //! The GPU CUDA/HIP accelerator work division idx type trait specialization.
			        template<typename TDim, typename TIdx>
			        struct IdxType<WorkDivUniformCudaHipBuiltIn<TDim, TIdx>>
			        {
			            using type = TIdx;
			        };

			        //! The GPU CUDA/HIP accelerator work division grid block extent trait specialization.
			        template<typename TDim, typename TIdx>
			        struct GetWorkDiv<WorkDivUniformCudaHipBuiltIn<TDim, TIdx>, origin::Grid, unit::Blocks>
			        {
			            //! \return The number of blocks in each dimension of the grid.
			            __device__ static auto getWorkDiv(WorkDivUniformCudaHipBuiltIn<TDim, TIdx> const& /* workDiv */)
			                -> Vec<TDim, TIdx>
			            {
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			                return castVec<TIdx>(getExtentVecEnd<TDim>(gridDim));
			#        else
			                return getExtentVecEnd<TDim>(Vec<std::integral_constant<typename TDim::value_type, 3>, TIdx>(
			                    static_cast<TIdx>(hipGridDim_z),
			                    static_cast<TIdx>(hipGridDim_y),
			                    static_cast<TIdx>(hipGridDim_x)));
			#        endif
			            }
			        };

			        //! The GPU CUDA/HIP accelerator work division block thread extent trait specialization.
			        template<typename TDim, typename TIdx>
			        struct GetWorkDiv<WorkDivUniformCudaHipBuiltIn<TDim, TIdx>, origin::Block, unit::Threads>
			        {
			            //! \return The number of threads in each dimension of a block.
			            __device__ static auto getWorkDiv(WorkDivUniformCudaHipBuiltIn<TDim, TIdx> const& /* workDiv */)
			                -> Vec<TDim, TIdx>
			            {
			#        ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
			                return castVec<TIdx>(getExtentVecEnd<TDim>(blockDim));
			#        else
			                return getExtentVecEnd<TDim>(Vec<std::integral_constant<typename TDim::value_type, 3>, TIdx>(
			                    static_cast<TIdx>(hipBlockDim_z),
			                    static_cast<TIdx>(hipBlockDim_y),
			                    static_cast<TIdx>(hipBlockDim_x)));
			#        endif
			            }
			        };

			        //! The GPU CUDA/HIP accelerator work division thread element extent trait specialization.
			        template<typename TDim, typename TIdx>
			        struct GetWorkDiv<WorkDivUniformCudaHipBuiltIn<TDim, TIdx>, origin::Thread, unit::Elems>
			        {
			            //! \return The number of blocks in each dimension of the grid.
			            __device__ static auto getWorkDiv(WorkDivUniformCudaHipBuiltIn<TDim, TIdx> const& workDiv)
			                -> Vec<TDim, TIdx>
			            {
			                return workDiv.m_threadElemExtent;
			            }
			        };
			    } // namespace trait

			#    endif

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/workdiv/WorkDivUniformCudaHipBuiltIn.hpp ==
			// ============================================================================


		// Specialized traits.
		// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

		// Implementation details.
		// #include "alpaka/core/ClipCast.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded

		// #include <typeinfo>    // amalgamate: file already included

		#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

		namespace alpaka
		{
		    template<typename TApi, typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		    class TaskKernelGpuUniformCudaHipRt;

		    //! The GPU CUDA accelerator.
		    //!
		    //! This accelerator allows parallel kernel execution on devices supporting CUDA.
		    template<
		        typename TApi,
		        typename TDim,
		        typename TIdx>
		    class AccGpuUniformCudaHipRt final :
		        public WorkDivUniformCudaHipBuiltIn<TDim, TIdx>,
		        public gb::IdxGbUniformCudaHipBuiltIn<TDim, TIdx>,
		        public bt::IdxBtUniformCudaHipBuiltIn<TDim, TIdx>,
		        public AtomicHierarchy<
		            AtomicUniformCudaHipBuiltIn, // grid atomics
		            AtomicUniformCudaHipBuiltIn, // block atomics
		            AtomicUniformCudaHipBuiltIn  // thread atomics
		        >,
		        public math::MathUniformCudaHipBuiltIn,
		        public BlockSharedMemDynUniformCudaHipBuiltIn,
		        public BlockSharedMemStUniformCudaHipBuiltIn,
		        public BlockSyncUniformCudaHipBuiltIn,
		        public IntrinsicUniformCudaHipBuiltIn,
		        public MemFenceUniformCudaHipBuiltIn,
		        public rand::RandUniformCudaHipRand<TApi>,
		        public warp::WarpUniformCudaHipBuiltIn,
		        public concepts::Implements<ConceptAcc, AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>
		    {
		        static_assert(
		            sizeof(TIdx) >= sizeof(int),
		            "Index type is not supported, consider using int or a larger type.");

		    public:
		        AccGpuUniformCudaHipRt(AccGpuUniformCudaHipRt const&) = delete;
		        AccGpuUniformCudaHipRt(AccGpuUniformCudaHipRt&&) = delete;
		        auto operator=(AccGpuUniformCudaHipRt const&) -> AccGpuUniformCudaHipRt& = delete;
		        auto operator=(AccGpuUniformCudaHipRt&&) -> AccGpuUniformCudaHipRt& = delete;

		        ALPAKA_FN_HOST_ACC AccGpuUniformCudaHipRt(Vec<TDim, TIdx> const& threadElemExtent)
		            : WorkDivUniformCudaHipBuiltIn<TDim, TIdx>(threadElemExtent)
		            , gb::IdxGbUniformCudaHipBuiltIn<TDim, TIdx>()
		            , bt::IdxBtUniformCudaHipBuiltIn<TDim, TIdx>()
		            , AtomicHierarchy<
		                  AtomicUniformCudaHipBuiltIn, // atomics between grids
		                  AtomicUniformCudaHipBuiltIn, // atomics between blocks
		                  AtomicUniformCudaHipBuiltIn // atomics between threads
		                  >()
		            , math::MathUniformCudaHipBuiltIn()
		            , BlockSharedMemDynUniformCudaHipBuiltIn()
		            , BlockSharedMemStUniformCudaHipBuiltIn()
		            , BlockSyncUniformCudaHipBuiltIn()
		            , MemFenceUniformCudaHipBuiltIn()
		            , rand::RandUniformCudaHipRand<TApi>()
		        {
		        }
		    };

		    namespace trait
		    {
		        //! The GPU CUDA accelerator accelerator type trait specialization.
		        template<typename TApi, typename TDim, typename TIdx>
		        struct AccType<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>
		        {
		            using type = AccGpuUniformCudaHipRt<TApi, TDim, TIdx>;
		        };

		        //! The GPU CUDA accelerator device properties get trait specialization.
		        template<typename TApi, typename TDim, typename TIdx>
		        struct GetAccDevProps<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>
		        {
		            ALPAKA_FN_HOST static auto getAccDevProps(DevUniformCudaHipRt<TApi> const& dev) -> AccDevProps<TDim, TIdx>
		            {
		#    ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
		                // Reading only the necessary attributes with cudaDeviceGetAttribute is faster than reading all with
		                // cuda https://devblogs.nvidia.com/cuda-pro-tip-the-fast-way-to-query-device-properties/
		                int multiProcessorCount = {};
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceGetAttribute(
		                    &multiProcessorCount,
		                    TApi::deviceAttributeMultiprocessorCount,
		                    dev.getNativeHandle()));

		                int maxGridSize[3] = {};
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceGetAttribute(
		                    &maxGridSize[0],
		                    TApi::deviceAttributeMaxGridDimX,
		                    dev.getNativeHandle()));
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceGetAttribute(
		                    &maxGridSize[1],
		                    TApi::deviceAttributeMaxGridDimY,
		                    dev.getNativeHandle()));
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceGetAttribute(
		                    &maxGridSize[2],
		                    TApi::deviceAttributeMaxGridDimZ,
		                    dev.getNativeHandle()));

		                int maxBlockDim[3] = {};
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceGetAttribute(
		                    &maxBlockDim[0],
		                    TApi::deviceAttributeMaxBlockDimX,
		                    dev.getNativeHandle()));
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceGetAttribute(
		                    &maxBlockDim[1],
		                    TApi::deviceAttributeMaxBlockDimY,
		                    dev.getNativeHandle()));
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceGetAttribute(
		                    &maxBlockDim[2],
		                    TApi::deviceAttributeMaxBlockDimZ,
		                    dev.getNativeHandle()));

		                int maxThreadsPerBlock = {};
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceGetAttribute(
		                    &maxThreadsPerBlock,
		                    TApi::deviceAttributeMaxThreadsPerBlock,
		                    dev.getNativeHandle()));

		                int sharedMemSizeBytes = {};
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::deviceGetAttribute(
		                    &sharedMemSizeBytes,
		                    TApi::deviceAttributeMaxSharedMemoryPerBlock,
		                    dev.getNativeHandle()));

		                return {// m_multiProcessorCount
		                        alpaka::core::clipCast<TIdx>(multiProcessorCount),
		                        // m_gridBlockExtentMax
		                        getExtentVecEnd<TDim>(Vec<DimInt<3u>, TIdx>(
		                            alpaka::core::clipCast<TIdx>(maxGridSize[2u]),
		                            alpaka::core::clipCast<TIdx>(maxGridSize[1u]),
		                            alpaka::core::clipCast<TIdx>(maxGridSize[0u]))),
		                        // m_gridBlockCountMax
		                        std::numeric_limits<TIdx>::max(),
		                        // m_blockThreadExtentMax
		                        getExtentVecEnd<TDim>(Vec<DimInt<3u>, TIdx>(
		                            alpaka::core::clipCast<TIdx>(maxBlockDim[2u]),
		                            alpaka::core::clipCast<TIdx>(maxBlockDim[1u]),
		                            alpaka::core::clipCast<TIdx>(maxBlockDim[0u]))),
		                        // m_blockThreadCountMax
		                        alpaka::core::clipCast<TIdx>(maxThreadsPerBlock),
		                        // m_threadElemExtentMax
		                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
		                        // m_threadElemCountMax
		                        std::numeric_limits<TIdx>::max(),
		                        // m_sharedMemSizeBytes
		                        static_cast<size_t>(sharedMemSizeBytes)};

		#    else
		                typename TApi::DeviceProp_t properties;
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::getDeviceProperties(&properties, dev.getNativeHandle()));

		                return {// m_multiProcessorCount
		                        alpaka::core::clipCast<TIdx>(properties.multiProcessorCount),
		                        // m_gridBlockExtentMax
		                        getExtentVecEnd<TDim>(Vec<DimInt<3u>, TIdx>(
		                            alpaka::core::clipCast<TIdx>(properties.maxGridSize[2u]),
		                            alpaka::core::clipCast<TIdx>(properties.maxGridSize[1u]),
		                            alpaka::core::clipCast<TIdx>(properties.maxGridSize[0u]))),
		                        // m_gridBlockCountMax
		                        std::numeric_limits<TIdx>::max(),
		                        // m_blockThreadExtentMax
		                        getExtentVecEnd<TDim>(Vec<DimInt<3u>, TIdx>(
		                            alpaka::core::clipCast<TIdx>(properties.maxThreadsDim[2u]),
		                            alpaka::core::clipCast<TIdx>(properties.maxThreadsDim[1u]),
		                            alpaka::core::clipCast<TIdx>(properties.maxThreadsDim[0u]))),
		                        // m_blockThreadCountMax
		                        alpaka::core::clipCast<TIdx>(properties.maxThreadsPerBlock),
		                        // m_threadElemExtentMax
		                        Vec<TDim, TIdx>::all(std::numeric_limits<TIdx>::max()),
		                        // m_threadElemCountMax
		                        std::numeric_limits<TIdx>::max(),
		                        // m_sharedMemSizeBytes
		                        static_cast<size_t>(properties.sharedMemPerBlock)};
		#    endif
		            }
		        };

		        //! The GPU CUDA accelerator name trait specialization.
		        template<typename TApi, typename TDim, typename TIdx>
		        struct GetAccName<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>
		        {
		            ALPAKA_FN_HOST static auto getAccName() -> std::string
		            {
		                return std::string("AccGpu") + TApi::name + "Rt<" + std::to_string(TDim::value) + ","
		                       + core::demangled<TIdx> + ">";
		            }
		        };

		        //! The GPU CUDA accelerator device type trait specialization.
		        template<typename TApi, typename TDim, typename TIdx>
		        struct DevType<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>
		        {
		            using type = DevUniformCudaHipRt<TApi>;
		        };

		        //! The GPU CUDA accelerator dimension getter trait specialization.
		        template<typename TApi, typename TDim, typename TIdx>
		        struct DimType<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>
		        {
		            using type = TDim;
		        };
		    } // namespace trait

		    namespace detail
		    {
		        //! specialization of the TKernelFnObj return type evaluation
		        //
		        // It is not possible to determine the result type of a __device__ lambda for CUDA on the host side.
		        // https://github.com/alpaka-group/alpaka/pull/695#issuecomment-446103194
		        // The execution task TaskKernelGpuUniformCudaHipRt is therefore performing this check on device side.
		        template<typename TApi, typename TDim, typename TIdx>
		        struct CheckFnReturnType<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>
		        {
		            template<typename TKernelFnObj, typename... TArgs>
		            void operator()(TKernelFnObj const&, TArgs const&...)
		            {
		            }
		        };
		    } // namespace detail

		    namespace trait
		    {
		        //! The GPU CUDA accelerator execution task type trait specialization.
		        template<
		            typename TApi,
		            typename TDim,
		            typename TIdx,
		            typename TWorkDiv,
		            typename TKernelFnObj,
		            typename... TArgs>
		        struct CreateTaskKernel<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>, TWorkDiv, TKernelFnObj, TArgs...>
		        {
		            ALPAKA_FN_HOST static auto createTaskKernel(
		                TWorkDiv const& workDiv,
		                TKernelFnObj const& kernelFnObj,
		                TArgs&&... args)
		            {
		                return TaskKernelGpuUniformCudaHipRt<
		                    TApi,
		                    AccGpuUniformCudaHipRt<TApi, TDim, TIdx>,
		                    TDim,
		                    TIdx,
		                    TKernelFnObj,
		                    TArgs...>(workDiv, kernelFnObj, std::forward<TArgs>(args)...);
		            }
		        };

		        //! The CPU CUDA execution task platform type trait specialization.
		        template<typename TApi, typename TDim, typename TIdx>
		        struct PltfType<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>
		        {
		            using type = PltfUniformCudaHipRt<TApi>;
		        };

		        //! The GPU CUDA accelerator idx type trait specialization.
		        template<typename TApi, typename TDim, typename TIdx>
		        struct IdxType<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>
		        {
		            using type = TIdx;
		        };
		    } // namespace trait
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/acc/AccGpuUniformCudaHipRt.hpp ==
		// ============================================================================

	// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/core/ApiCudaRt.hpp ==
		// ==
		/* Copyright 2022 Andrea Bocci
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include <boost/predef.h>    // amalgamate: file already included

		#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED
		#    include <cuda_runtime_api.h>

		namespace alpaka
		{
		    struct ApiCudaRt
		    {
		        // Names
		        static constexpr char name[] = "Cuda";
		        static constexpr auto version = BOOST_PREDEF_MAKE_10_VVRRP(CUDART_VERSION);

		        // Types
		        using DeviceAttr_t = ::cudaDeviceAttr;
		        using DeviceProp_t = ::cudaDeviceProp;
		        using Error_t = ::cudaError_t;
		        using Event_t = ::cudaEvent_t;
		        using Extent_t = ::cudaExtent;
		        using Flag_t = unsigned int;
		        using FuncAttributes_t = ::cudaFuncAttributes;
		        using HostFn_t = void (*)(void* data); // same as cudaHostFn_t, without the CUDART_CB calling convention
		        using Limit_t = ::cudaLimit;
		        using Memcpy3DParms_t = ::cudaMemcpy3DParms;
		        using MemcpyKind_t = ::cudaMemcpyKind;
		        using PitchedPtr_t = ::cudaPitchedPtr;
		        using Pos_t = ::cudaPos;
		        using Stream_t = ::cudaStream_t;

		        // Constants
		        static constexpr Error_t success = ::cudaSuccess;
		        static constexpr Error_t errorNotReady = ::cudaErrorNotReady;
		        static constexpr Error_t errorHostMemoryAlreadyRegistered = ::cudaErrorHostMemoryAlreadyRegistered;
		        static constexpr Error_t errorHostMemoryNotRegistered = ::cudaErrorHostMemoryNotRegistered;
		        static constexpr Error_t errorUnsupportedLimit = ::cudaErrorUnsupportedLimit;
		        static constexpr Error_t errorUnknown = ::cudaErrorUnknown;

		        static constexpr Flag_t eventDefault = cudaEventDefault;
		        static constexpr Flag_t eventBlockingSync = cudaEventBlockingSync;
		        static constexpr Flag_t eventDisableTiming = cudaEventDisableTiming;
		        static constexpr Flag_t eventInterprocess = cudaEventInterprocess;

		        static constexpr Flag_t hostMallocDefault = cudaHostAllocDefault;
		        static constexpr Flag_t hostMallocMapped = cudaHostAllocMapped;
		        static constexpr Flag_t hostMallocPortable = cudaHostAllocPortable;
		        static constexpr Flag_t hostMallocWriteCombined = cudaHostAllocWriteCombined;
		        static constexpr Flag_t hostMallocCoherent = cudaHostAllocDefault; // Not supported.
		        static constexpr Flag_t hostMallocNonCoherent = cudaHostAllocDefault; // Not supported.

		        static constexpr Flag_t hostRegisterDefault = cudaHostRegisterDefault;
		        static constexpr Flag_t hostRegisterPortable = cudaHostRegisterPortable;
		        static constexpr Flag_t hostRegisterMapped = cudaHostRegisterMapped;
		        static constexpr Flag_t hostRegisterIoMemory = cudaHostRegisterIoMemory;

		        static constexpr MemcpyKind_t memcpyDefault = ::cudaMemcpyDefault;
		        static constexpr MemcpyKind_t memcpyDeviceToDevice = ::cudaMemcpyDeviceToDevice;
		        static constexpr MemcpyKind_t memcpyDeviceToHost = ::cudaMemcpyDeviceToHost;
		        static constexpr MemcpyKind_t memcpyHostToDevice = ::cudaMemcpyHostToDevice;

		        static constexpr Flag_t streamDefault = cudaStreamDefault;
		        static constexpr Flag_t streamNonBlocking = cudaStreamNonBlocking;

		        static constexpr DeviceAttr_t deviceAttributeMaxBlockDimX = ::cudaDevAttrMaxBlockDimX;
		        static constexpr DeviceAttr_t deviceAttributeMaxBlockDimY = ::cudaDevAttrMaxBlockDimY;
		        static constexpr DeviceAttr_t deviceAttributeMaxBlockDimZ = ::cudaDevAttrMaxBlockDimZ;
		        static constexpr DeviceAttr_t deviceAttributeMaxGridDimX = ::cudaDevAttrMaxGridDimX;
		        static constexpr DeviceAttr_t deviceAttributeMaxGridDimY = ::cudaDevAttrMaxGridDimY;
		        static constexpr DeviceAttr_t deviceAttributeMaxGridDimZ = ::cudaDevAttrMaxGridDimZ;
		        static constexpr DeviceAttr_t deviceAttributeMaxSharedMemoryPerBlock = ::cudaDevAttrMaxSharedMemoryPerBlock;
		        static constexpr DeviceAttr_t deviceAttributeMaxThreadsPerBlock = ::cudaDevAttrMaxThreadsPerBlock;
		        static constexpr DeviceAttr_t deviceAttributeMultiprocessorCount = ::cudaDevAttrMultiProcessorCount;

		        static constexpr Limit_t limitPrintfFifoSize = ::cudaLimitPrintfFifoSize;
		        static constexpr Limit_t limitMallocHeapSize = ::cudaLimitMallocHeapSize;

		        // Host function helper
		        // Encapsulates the different function signatures used by cudaStreamAddCallback and cudaLaunchHostFn, and the
		        // different calling conventions used by CUDA (__stdcall on Win32) and HIP (standard).
		        struct HostFnAdaptor
		        {
		            HostFn_t func_;
		            void* data_;

		            static void CUDART_CB hostFunction(void* data)
		            {
		                auto ptr = reinterpret_cast<HostFnAdaptor*>(data);
		                ptr->func_(ptr->data_);
		                delete ptr;
		            }

		            static void CUDART_CB streamCallback(Stream_t, Error_t, void* data)
		            {
		                auto ptr = reinterpret_cast<HostFnAdaptor*>(data);
		                ptr->func_(ptr->data_);
		                delete ptr;
		            }
		        };

		        // Runtime API
		        static inline Error_t deviceGetAttribute(int* value, DeviceAttr_t attr, int device)
		        {
		            return ::cudaDeviceGetAttribute(value, attr, device);
		        }

		        static inline Error_t deviceGetLimit(size_t* pValue, Limit_t limit)
		        {
		            return ::cudaDeviceGetLimit(pValue, limit);
		        }

		        static inline Error_t deviceReset()
		        {
		            return ::cudaDeviceReset();
		        }

		        static inline Error_t deviceSetLimit(Limit_t limit, size_t value)
		        {
		            return ::cudaDeviceSetLimit(limit, value);
		        }

		        static inline Error_t deviceSynchronize()
		        {
		            return ::cudaDeviceSynchronize();
		        }

		        static inline Error_t eventCreate(Event_t* event)
		        {
		            return ::cudaEventCreate(event);
		        }

		        static inline Error_t eventCreateWithFlags(Event_t* event, Flag_t flags)
		        {
		            return ::cudaEventCreateWithFlags(event, flags);
		        }

		        static inline Error_t eventDestroy(Event_t event)
		        {
		            return ::cudaEventDestroy(event);
		        }

		        static inline Error_t eventQuery(Event_t event)
		        {
		            return ::cudaEventQuery(event);
		        }

		        static inline Error_t eventRecord(Event_t event, Stream_t stream)
		        {
		            return ::cudaEventRecord(event, stream);
		        }

		        static inline Error_t eventSynchronize(Event_t event)
		        {
		            return ::cudaEventSynchronize(event);
		        }

		        static inline Error_t free(void* devPtr)
		        {
		            return ::cudaFree(devPtr);
		        }

		        static inline Error_t freeAsync([[maybe_unused]] void* devPtr, [[maybe_unused]] Stream_t stream)
		        {
		#    if CUDART_VERSION >= 11020
		            return ::cudaFreeAsync(devPtr, stream);
		#    else
		            // Not implemented.
		            return errorUnknown;
		#    endif
		        }

		        static inline Error_t funcGetAttributes(FuncAttributes_t* attr, void const* func)
		        {
		            return ::cudaFuncGetAttributes(attr, func);
		        }

		        template<typename T>
		        static inline Error_t funcGetAttributes(FuncAttributes_t* attr, T* func)
		        {
		            return ::cudaFuncGetAttributes(attr, reinterpret_cast<void const*>(func));
		        }

		        static inline Error_t getDeviceCount(int* count)
		        {
		            return ::cudaGetDeviceCount(count);
		        }

		        static inline Error_t getDeviceProperties(DeviceProp_t* prop, int device)
		        {
		            return ::cudaGetDeviceProperties(prop, device);
		        }

		        static inline char const* getErrorName(Error_t error)
		        {
		            return ::cudaGetErrorName(error);
		        }

		        static inline char const* getErrorString(Error_t error)
		        {
		            return ::cudaGetErrorString(error);
		        }

		        static inline Error_t getLastError()
		        {
		            return ::cudaGetLastError();
		        }

		        static inline Error_t getSymbolAddress(void** devPtr, void const* symbol)
		        {
		            return ::cudaGetSymbolAddress(devPtr, symbol);
		        }

		        template<class T>
		        static inline Error_t getSymbolAddress(void** devPtr, T const& symbol)
		        {
		            return ::cudaGetSymbolAddress(devPtr, symbol);
		        }

		        static inline Error_t hostGetDevicePointer(void** pDevice, void* pHost, Flag_t flags)
		        {
		            return ::cudaHostGetDevicePointer(pDevice, pHost, flags);
		        }

		        static inline Error_t hostFree(void* ptr)
		        {
		            return ::cudaFreeHost(ptr);
		        }

		        static inline Error_t hostMalloc(void** ptr, size_t size, Flag_t flags)
		        {
		            return ::cudaHostAlloc(ptr, size, flags);
		        }

		        static inline Error_t hostRegister(void* ptr, size_t size, Flag_t flags)
		        {
		            return ::cudaHostRegister(ptr, size, flags);
		        }

		        static inline Error_t hostUnregister(void* ptr)
		        {
		            return ::cudaHostUnregister(ptr);
		        }

		        static inline Error_t launchHostFunc(Stream_t stream, HostFn_t fn, void* userData)
		        {
		#    if CUDART_VERSION >= 10000
		            // Wrap the host function using the proper calling convention
		            return ::cudaLaunchHostFunc(stream, HostFnAdaptor::hostFunction, new HostFnAdaptor{fn, userData});
		#    else
		            // Emulate cudaLaunchHostFunc using cudaStreamAddCallback with a callback adaptor.
		            return ::cudaStreamAddCallback(stream, HostFnAdaptor::streamCallback, new HostFnAdaptor{fn, userData}, 0);
		#    endif
		        }

		        static inline Error_t malloc(void** devPtr, size_t size)
		        {
		            return ::cudaMalloc(devPtr, size);
		        }

		        static inline Error_t malloc3D(PitchedPtr_t* pitchedDevPtr, Extent_t extent)
		        {
		            return ::cudaMalloc3D(pitchedDevPtr, extent);
		        }

		        static inline Error_t mallocAsync(
		            [[maybe_unused]] void** devPtr,
		            [[maybe_unused]] size_t size,
		            [[maybe_unused]] Stream_t stream)
		        {
		#    if CUDART_VERSION >= 11020
		            return ::cudaMallocAsync(devPtr, size, stream);
		#    else
		            // Not implemented.
		            return errorUnknown;
		#    endif
		        }

		        static inline Error_t mallocPitch(void** devPtr, size_t* pitch, size_t width, size_t height)
		        {
		            return ::cudaMallocPitch(devPtr, pitch, width, height);
		        }

		        static inline Error_t memGetInfo(size_t* free, size_t* total)
		        {
		            return ::cudaMemGetInfo(free, total);
		        }

		        static inline Error_t memcpy(void* dst, void const* src, size_t count, MemcpyKind_t kind)
		        {
		            return ::cudaMemcpy(dst, src, count, kind);
		        }

		        static inline Error_t memcpy2DAsync(
		            void* dst,
		            size_t dpitch,
		            void const* src,
		            size_t spitch,
		            size_t width,
		            size_t height,
		            MemcpyKind_t kind,
		            Stream_t stream)
		        {
		            return ::cudaMemcpy2DAsync(dst, dpitch, src, spitch, width, height, kind, stream);
		        }

		        static inline Error_t memcpy3DAsync(Memcpy3DParms_t const* p, Stream_t stream)
		        {
		            return ::cudaMemcpy3DAsync(p, stream);
		        }

		        static inline Error_t memcpyAsync(void* dst, void const* src, size_t count, MemcpyKind_t kind, Stream_t stream)
		        {
		            return ::cudaMemcpyAsync(dst, src, count, kind, stream);
		        }

		        static inline Error_t memset2DAsync(
		            void* devPtr,
		            size_t pitch,
		            int value,
		            size_t width,
		            size_t height,
		            Stream_t stream)
		        {
		            return ::cudaMemset2DAsync(devPtr, pitch, value, width, height, stream);
		        }

		        static inline Error_t memset3DAsync(PitchedPtr_t pitchedDevPtr, int value, Extent_t extent, Stream_t stream)
		        {
		            return ::cudaMemset3DAsync(pitchedDevPtr, value, extent, stream);
		        }

		        static inline Error_t memsetAsync(void* devPtr, int value, size_t count, Stream_t stream)
		        {
		            return ::cudaMemsetAsync(devPtr, value, count, stream);
		        }

		        static inline Error_t setDevice(int device)
		        {
		            return ::cudaSetDevice(device);
		        }

		        static inline Error_t streamCreate(Stream_t* pStream)
		        {
		            return ::cudaStreamCreate(pStream);
		        }

		        static inline Error_t streamCreateWithFlags(Stream_t* pStream, Flag_t flags)
		        {
		            return ::cudaStreamCreateWithFlags(pStream, flags);
		        }

		        static inline Error_t streamDestroy(Stream_t stream)
		        {
		            return ::cudaStreamDestroy(stream);
		        }

		        static inline Error_t streamQuery(Stream_t stream)
		        {
		            return ::cudaStreamQuery(stream);
		        }

		        static inline Error_t streamSynchronize(Stream_t stream)
		        {
		            return ::cudaStreamSynchronize(stream);
		        }

		        static inline Error_t streamWaitEvent(Stream_t stream, Event_t event, Flag_t flags)
		        {
		            return ::cudaStreamWaitEvent(stream, event, flags);
		        }

		        static inline PitchedPtr_t makePitchedPtr(void* d, size_t p, size_t xsz, size_t ysz)
		        {
		            return ::make_cudaPitchedPtr(d, p, xsz, ysz);
		        }

		        static inline Pos_t makePos(size_t x, size_t y, size_t z)
		        {
		            return ::make_cudaPos(x, y, z);
		        }

		        static inline Extent_t makeExtent(size_t w, size_t h, size_t d)
		        {
		            return ::make_cudaExtent(w, h, d);
		        }
		    };

		} // namespace alpaka

		#endif // ALPAKA_ACC_GPU_CUDA_ENABLED
		// ==
		// == ./include/alpaka/core/ApiCudaRt.hpp ==
		// ============================================================================


	#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

	namespace alpaka
	{
	    template<typename TDim, typename TIdx>
	    using AccGpuCudaRt = AccGpuUniformCudaHipRt<ApiCudaRt, TDim, TIdx>;

	    namespace trait
	    {
	        template<typename TDim, typename TIdx>
	        struct AccToTag<alpaka::AccGpuCudaRt<TDim, TIdx>>
	        {
	            using type = alpaka::TagGpuCudaRt;
	        };

	        template<typename TDim, typename TIdx>
	        struct TagToAcc<alpaka::TagGpuCudaRt, TDim, TIdx>
	        {
	            using type = alpaka::AccGpuCudaRt<TDim, TIdx>;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_CUDA_ENABLED
	// ==
	// == ./include/alpaka/acc/AccGpuCudaRt.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/acc/AccGpuHipRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/acc/AccGpuUniformCudaHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/core/ApiHipRt.hpp ==
		// ==
		/* Copyright 2022 Andrea Bocci
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include <boost/predef.h>    // amalgamate: file already included

		#ifdef ALPAKA_ACC_GPU_HIP_ENABLED

		#    include <hip/hip_runtime_api.h>
		#    include <hip/hip_version.h>

		namespace alpaka
		{
		    struct ApiHipRt
		    {
		        // Names
		        static constexpr char name[] = "Hip";
		        static constexpr auto version = BOOST_VERSION_NUMBER(HIP_VERSION_MAJOR, HIP_VERSION_MINOR, 0);

		        // Types
		        using DeviceAttr_t = ::hipDeviceAttribute_t;
		        using DeviceProp_t = ::hipDeviceProp_t;
		        using Error_t = ::hipError_t;
		        using Event_t = ::hipEvent_t;
		        using Extent_t = ::hipExtent;
		        using Flag_t = unsigned int;
		        using FuncAttributes_t = ::hipFuncAttributes;
		        using HostFn_t = void (*)(void* data); // same as hipHostFn_t
		        using Limit_t = ::hipLimit_t;
		        using Memcpy3DParms_t = ::hipMemcpy3DParms;
		        using MemcpyKind_t = ::hipMemcpyKind;
		        using PitchedPtr_t = ::hipPitchedPtr;
		        using Pos_t = ::hipPos;
		        using Stream_t = ::hipStream_t;

		        // Constants
		        static constexpr Error_t success = ::hipSuccess;
		        static constexpr Error_t errorNotReady = ::hipErrorNotReady;
		        static constexpr Error_t errorHostMemoryAlreadyRegistered = ::hipErrorHostMemoryAlreadyRegistered;
		        static constexpr Error_t errorHostMemoryNotRegistered = ::hipErrorHostMemoryNotRegistered;
		        static constexpr Error_t errorUnsupportedLimit = ::hipErrorUnsupportedLimit;
		        static constexpr Error_t errorUnknown = ::hipErrorUnknown;

		        static constexpr Flag_t eventDefault = hipEventDefault;
		        static constexpr Flag_t eventBlockingSync = hipEventBlockingSync;
		        static constexpr Flag_t eventDisableTiming = hipEventDisableTiming;
		        static constexpr Flag_t eventInterprocess = hipEventInterprocess;

		        static constexpr Flag_t hostMallocDefault = hipHostMallocDefault;
		        static constexpr Flag_t hostMallocMapped = hipHostMallocMapped;
		        static constexpr Flag_t hostMallocPortable = hipHostMallocPortable;
		        static constexpr Flag_t hostMallocWriteCombined = hipHostMallocWriteCombined;
		        static constexpr Flag_t hostMallocCoherent = hipHostMallocCoherent;
		        static constexpr Flag_t hostMallocNonCoherent = hipHostMallocNonCoherent;

		        static constexpr Flag_t hostRegisterDefault = hipHostRegisterDefault;
		        static constexpr Flag_t hostRegisterPortable = hipHostRegisterPortable;
		        static constexpr Flag_t hostRegisterMapped = hipHostRegisterMapped;
		        static constexpr Flag_t hostRegisterIoMemory = hipHostRegisterIoMemory;

		        static constexpr MemcpyKind_t memcpyDefault = ::hipMemcpyDefault;
		        static constexpr MemcpyKind_t memcpyDeviceToDevice = ::hipMemcpyDeviceToDevice;
		        static constexpr MemcpyKind_t memcpyDeviceToHost = ::hipMemcpyDeviceToHost;
		        static constexpr MemcpyKind_t memcpyHostToDevice = ::hipMemcpyHostToDevice;

		        static constexpr Flag_t streamDefault = hipStreamDefault;
		        static constexpr Flag_t streamNonBlocking = hipStreamNonBlocking;

		        static constexpr DeviceAttr_t deviceAttributeMaxBlockDimX = ::hipDeviceAttributeMaxBlockDimX;
		        static constexpr DeviceAttr_t deviceAttributeMaxBlockDimY = ::hipDeviceAttributeMaxBlockDimY;
		        static constexpr DeviceAttr_t deviceAttributeMaxBlockDimZ = ::hipDeviceAttributeMaxBlockDimZ;
		        static constexpr DeviceAttr_t deviceAttributeMaxGridDimX = ::hipDeviceAttributeMaxGridDimX;
		        static constexpr DeviceAttr_t deviceAttributeMaxGridDimY = ::hipDeviceAttributeMaxGridDimY;
		        static constexpr DeviceAttr_t deviceAttributeMaxGridDimZ = ::hipDeviceAttributeMaxGridDimZ;
		        static constexpr DeviceAttr_t deviceAttributeMaxSharedMemoryPerBlock
		            = ::hipDeviceAttributeMaxSharedMemoryPerBlock;
		        static constexpr DeviceAttr_t deviceAttributeMaxThreadsPerBlock = ::hipDeviceAttributeMaxThreadsPerBlock;
		        static constexpr DeviceAttr_t deviceAttributeMultiprocessorCount = ::hipDeviceAttributeMultiprocessorCount;

		#    if HIP_VERSION >= 40500000
		        static constexpr Limit_t limitPrintfFifoSize = ::hipLimitPrintfFifoSize;
		#    else
		        static constexpr Limit_t limitPrintfFifoSize
		            = static_cast<Limit_t>(0x01); // Implemented only in ROCm 4.5.0 and later.
		#    endif
		        static constexpr Limit_t limitMallocHeapSize = ::hipLimitMallocHeapSize;

		        // Host function helper
		        // Encapsulates the different function signatures used by hipStreamAddCallback and hipLaunchHostFn, and the
		        // different calling conventions used by CUDA (__stdcall on Win32) and HIP (standard).
		        struct HostFnAdaptor
		        {
		            HostFn_t func_;
		            void* data_;

		            static void hostFunction(void* data)
		            {
		                auto ptr = reinterpret_cast<HostFnAdaptor*>(data);
		                ptr->func_(ptr->data_);
		                delete ptr;
		            }

		            static void streamCallback(Stream_t, Error_t, void* data)
		            {
		                auto ptr = reinterpret_cast<HostFnAdaptor*>(data);
		                ptr->func_(ptr->data_);
		                delete ptr;
		            }
		        };

		        // Runtime API
		        static inline Error_t deviceGetAttribute(int* value, DeviceAttr_t attr, int device)
		        {
		            return ::hipDeviceGetAttribute(value, attr, device);
		        }

		        static inline Error_t deviceGetLimit(size_t* pValue, Limit_t limit)
		        {
		#    if HIP_VERSION < 40500000
		            if(limit == limitPrintfFifoSize)
		            {
		                // Implemented only in ROCm 4.5.0 and later.
		                return errorUnsupportedLimit;
		            }
		#    endif
		            return ::hipDeviceGetLimit(pValue, limit);
		        }

		        static inline Error_t deviceReset()
		        {
		            return ::hipDeviceReset();
		        }

		        static inline Error_t deviceSetLimit(Limit_t /* limit */, size_t /* value */)
		        {
		            // Not implemented.
		            return errorUnsupportedLimit;
		        }

		        static inline Error_t deviceSynchronize()
		        {
		            return ::hipDeviceSynchronize();
		        }

		        static inline Error_t eventCreate(Event_t* event)
		        {
		            return ::hipEventCreate(event);
		        }

		        static inline Error_t eventCreateWithFlags(Event_t* event, Flag_t flags)
		        {
		            return ::hipEventCreateWithFlags(event, flags);
		        }

		        static inline Error_t eventDestroy(Event_t event)
		        {
		            return ::hipEventDestroy(event);
		        }

		        static inline Error_t eventQuery(Event_t event)
		        {
		            return ::hipEventQuery(event);
		        }

		        static inline Error_t eventRecord(Event_t event, Stream_t stream)
		        {
		            return ::hipEventRecord(event, stream);
		        }

		        static inline Error_t eventSynchronize(Event_t event)
		        {
		            return ::hipEventSynchronize(event);
		        }

		        static inline Error_t free(void* devPtr)
		        {
		            return ::hipFree(devPtr);
		        }

		        static inline Error_t freeAsync([[maybe_unused]] void* devPtr, [[maybe_unused]] Stream_t stream)
		        {
		            // hipFreeAsync is implemented only in ROCm 5.2.0 and later.
		#    if HIP_VERSION >= 50200000
		            return ::hipFreeAsync(devPtr, stream);
		#    else
		            // Not implemented.
		            return errorUnknown;
		#    endif
		        }

		        static inline Error_t funcGetAttributes(FuncAttributes_t* attr, void const* func)
		        {
		            return ::hipFuncGetAttributes(attr, func);
		        }

		        template<typename T>
		        static inline Error_t funcGetAttributes(FuncAttributes_t* attr, T* func)
		        {
		            return ::hipFuncGetAttributes(attr, reinterpret_cast<void const*>(func));
		        }

		        static inline Error_t getDeviceCount(int* count)
		        {
		            return ::hipGetDeviceCount(count);
		        }

		        static inline Error_t getDeviceProperties(DeviceProp_t* prop, int device)
		        {
		            return ::hipGetDeviceProperties(prop, device);
		        }

		        static inline char const* getErrorName(Error_t error)
		        {
		            return ::hipGetErrorName(error);
		        }

		        static inline char const* getErrorString(Error_t error)
		        {
		            return ::hipGetErrorString(error);
		        }

		        static inline Error_t getLastError()
		        {
		            return ::hipGetLastError();
		        }

		        static inline Error_t getSymbolAddress(void** devPtr, void const* symbol)
		        {
		            return ::hipGetSymbolAddress(devPtr, symbol);
		        }

		        template<class T>
		        static inline Error_t getSymbolAddress(void** devPtr, T const& symbol)
		        {
		            return ::hipGetSymbolAddress(devPtr, symbol);
		        }

		        static inline Error_t hostGetDevicePointer(void** pDevice, void* pHost, Flag_t flags)
		        {
		            return ::hipHostGetDevicePointer(pDevice, pHost, flags);
		        }

		        static inline Error_t hostFree(void* ptr)
		        {
		            return ::hipHostFree(ptr);
		        }

		        static inline Error_t hostMalloc(void** ptr, size_t size, Flag_t flags)
		        {
		            return ::hipHostMalloc(ptr, size, flags);
		        }

		        static inline Error_t hostRegister(void* ptr, size_t size, Flag_t flags)
		        {
		            return ::hipHostRegister(ptr, size, flags);
		        }

		        static inline Error_t hostUnregister(void* ptr)
		        {
		            return ::hipHostUnregister(ptr);
		        }

		        static inline Error_t launchHostFunc(Stream_t stream, HostFn_t fn, void* userData)
		        {
		            // hipLaunchHostFunc is implemented only in ROCm 5.4.0 and later.
		#    if HIP_VERSION >= 50400000
		            // Wrap the host function using the proper calling convention.
		            return ::hipLaunchHostFunc(stream, HostFnAdaptor::hostFunction, new HostFnAdaptor{fn, userData});
		#    else
		            // Emulate hipLaunchHostFunc using hipStreamAddCallback with a callback adaptor.
		            return ::hipStreamAddCallback(stream, HostFnAdaptor::streamCallback, new HostFnAdaptor{fn, userData}, 0);
		#    endif
		        }

		        static inline Error_t malloc(void** devPtr, size_t size)
		        {
		            return ::hipMalloc(devPtr, size);
		        }

		        static inline Error_t malloc3D(PitchedPtr_t* pitchedDevPtr, Extent_t extent)
		        {
		            return ::hipMalloc3D(pitchedDevPtr, extent);
		        }

		        static inline Error_t mallocAsync(
		            [[maybe_unused]] void** devPtr,
		            [[maybe_unused]] size_t size,
		            [[maybe_unused]] Stream_t stream)
		        {
		            // hipMallocAsync is implemented only in ROCm 5.2.0 and later.
		#    if HIP_VERSION >= 50200000
		            return ::hipMallocAsync(devPtr, size, stream);
		#    else
		            // Not implemented.
		            return errorUnknown;
		#    endif
		        }

		        static inline Error_t mallocPitch(void** devPtr, size_t* pitch, size_t width, size_t height)
		        {
		            return ::hipMallocPitch(devPtr, pitch, width, height);
		        }

		        static inline Error_t memGetInfo(size_t* free, size_t* total)
		        {
		            return ::hipMemGetInfo(free, total);
		        }

		        static inline Error_t memcpy(void* dst, void const* src, size_t count, MemcpyKind_t kind)
		        {
		            return ::hipMemcpy(dst, src, count, kind);
		        }

		        static inline Error_t memcpy2DAsync(
		            void* dst,
		            size_t dpitch,
		            void const* src,
		            size_t spitch,
		            size_t width,
		            size_t height,
		            MemcpyKind_t kind,
		            Stream_t stream)
		        {
		            return ::hipMemcpy2DAsync(dst, dpitch, src, spitch, width, height, kind, stream);
		        }

		        static inline Error_t memcpy3DAsync(Memcpy3DParms_t const* p, Stream_t stream)
		        {
		            return ::hipMemcpy3DAsync(p, stream);
		        }

		        static inline Error_t memcpyAsync(void* dst, void const* src, size_t count, MemcpyKind_t kind, Stream_t stream)
		        {
		            return ::hipMemcpyAsync(dst, src, count, kind, stream);
		        }

		        static inline Error_t memset2DAsync(
		            void* devPtr,
		            size_t pitch,
		            int value,
		            size_t width,
		            size_t height,
		            Stream_t stream)
		        {
		            return ::hipMemset2DAsync(devPtr, pitch, value, width, height, stream);
		        }

		        static inline Error_t memset3DAsync(PitchedPtr_t pitchedDevPtr, int value, Extent_t extent, Stream_t stream)
		        {
		            return ::hipMemset3DAsync(pitchedDevPtr, value, extent, stream);
		        }

		        static inline Error_t memsetAsync(void* devPtr, int value, size_t count, Stream_t stream)
		        {
		            return ::hipMemsetAsync(devPtr, value, count, stream);
		        }

		        static inline Error_t setDevice(int device)
		        {
		            return ::hipSetDevice(device);
		        }

		        static inline Error_t streamCreate(Stream_t* pStream)
		        {
		            return ::hipStreamCreate(pStream);
		        }

		        static inline Error_t streamCreateWithFlags(Stream_t* pStream, Flag_t flags)
		        {
		            return ::hipStreamCreateWithFlags(pStream, flags);
		        }

		        static inline Error_t streamDestroy(Stream_t stream)
		        {
		            return ::hipStreamDestroy(stream);
		        }

		        static inline Error_t streamQuery(Stream_t stream)
		        {
		            return ::hipStreamQuery(stream);
		        }

		        static inline Error_t streamSynchronize(Stream_t stream)
		        {
		            return ::hipStreamSynchronize(stream);
		        }

		        static inline Error_t streamWaitEvent(Stream_t stream, Event_t event, Flag_t flags)
		        {
		            return ::hipStreamWaitEvent(stream, event, flags);
		        }

		        static inline PitchedPtr_t makePitchedPtr(void* d, size_t p, size_t xsz, size_t ysz)
		        {
		            return ::make_hipPitchedPtr(d, p, xsz, ysz);
		        }

		        static inline Pos_t makePos(size_t x, size_t y, size_t z)
		        {
		            return ::make_hipPos(x, y, z);
		        }

		        static inline Extent_t makeExtent(size_t w, size_t h, size_t d)
		        {
		            return ::make_hipExtent(w, h, d);
		        }
		    };

		} // namespace alpaka

		#endif // ALPAKA_ACC_GPU_HIP_ENABLED
		// ==
		// == ./include/alpaka/core/ApiHipRt.hpp ==
		// ============================================================================


	#ifdef ALPAKA_ACC_GPU_HIP_ENABLED

	namespace alpaka
	{
	    template<typename TDim, typename TIdx>
	    using AccGpuHipRt = AccGpuUniformCudaHipRt<ApiHipRt, TDim, TIdx>;

	    namespace trait
	    {
	        template<typename TDim, typename TIdx>
	        struct AccToTag<alpaka::AccGpuHipRt<TDim, TIdx>>
	        {
	            using type = alpaka::TagGpuHipRt;
	        };

	        template<typename TDim, typename TIdx>
	        struct TagToAcc<alpaka::TagGpuHipRt, TDim, TIdx>
	        {
	            using type = alpaka::AccGpuHipRt<TDim, TIdx>;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_HIP_ENABLED
	// ==
	// == ./include/alpaka/acc/AccGpuHipRt.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/acc/AccGpuSyclIntel.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/acc/AccGenericSycl.hpp"    // amalgamate: file already expanded
	// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/dev/DevGpuSyclIntel.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/dev/DevGenericSycl.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/pltf/PltfGpuSyclIntel.hpp ==
			// ==
			/* Copyright 2023 Jan Stephan, Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/dev/DevGenericSycl.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/pltf/PltfGenericSycl.hpp"    // amalgamate: file already expanded

			// #include <string>    // amalgamate: file already included

			#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_GPU)

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka
			{
			    namespace detail
			    {
			        // Prevent clang from annoying us with warnings about emitting too many vtables. These are discarded by the
			        // linker anyway.
			#    if BOOST_COMP_CLANG
			#        pragma clang diagnostic push
			#        pragma clang diagnostic ignored "-Wweak-vtables"
			#    endif
			        struct IntelGpuSelector final
			        {
			            auto operator()(sycl::device const& dev) const -> int
			            {
			                auto const& vendor = dev.get_info<sycl::info::device::vendor>();
			                auto const is_intel_gpu = dev.is_gpu() && (vendor.find("Intel(R) Corporation") != std::string::npos);

			                return is_intel_gpu ? 1 : -1;
			            }
			        };
			#    if BOOST_COMP_CLANG
			#        pragma clang diagnostic pop
			#    endif
			    } // namespace detail

			    //! The SYCL device manager.
			    using PltfGpuSyclIntel = PltfGenericSycl<detail::IntelGpuSelector>;
			} // namespace alpaka

			namespace alpaka::trait
			{
			    //! The SYCL device manager device type trait specialization.
			    template<>
			    struct DevType<PltfGpuSyclIntel>
			    {
			        using type = DevGenericSycl<PltfGpuSyclIntel>; // = DevGpuSyclIntel
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/pltf/PltfGpuSyclIntel.hpp ==
			// ============================================================================


		#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_GPU)

		namespace alpaka
		{
		    using DevGpuSyclIntel = DevGenericSycl<PltfGpuSyclIntel>;
		}

		#endif
		// ==
		// == ./include/alpaka/dev/DevGpuSyclIntel.hpp ==
		// ============================================================================

	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/kernel/TaskKernelGpuSyclIntel.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan
		 * SPDX-License-Identifier: MPL-2.0
		 */


		// #pragma once
		// #include "alpaka/kernel/TaskKernelGenericSycl.hpp"    // amalgamate: file already expanded

		#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_GPU)

		namespace alpaka
		{
		    template<typename TDim, typename TIdx>
		    class AccGpuSyclIntel;

		    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		    using TaskKernelGpuSyclIntel
		        = TaskKernelGenericSycl<AccGpuSyclIntel<TDim, TIdx>, TDim, TIdx, TKernelFnObj, TArgs...>;
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/kernel/TaskKernelGpuSyclIntel.hpp ==
		// ============================================================================

	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/PltfGpuSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

	// #include <string>    // amalgamate: file already included
	// #include <utility>    // amalgamate: file already included

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_GPU)

	namespace alpaka
	{
	    //! The Intel GPU SYCL accelerator.
	    //!
	    //! This accelerator allows parallel kernel execution on a oneAPI-capable Intel GPU target device.
	    template<typename TDim, typename TIdx>
	    class AccGpuSyclIntel final
	        : public AccGenericSycl<TDim, TIdx>
	        , public concepts::Implements<ConceptAcc, AccGpuSyclIntel<TDim, TIdx>>
	    {
	    public:
	        using AccGenericSycl<TDim, TIdx>::AccGenericSycl;
	    };
	} // namespace alpaka

	namespace alpaka::trait
	{
	    //! The Intel GPU SYCL accelerator name trait specialization.
	    template<typename TDim, typename TIdx>
	    struct GetAccName<AccGpuSyclIntel<TDim, TIdx>>
	    {
	        static auto getAccName() -> std::string
	        {
	            return "AccGpuSyclIntel<" + std::to_string(TDim::value) + "," + core::demangled<TIdx> + ">";
	        }
	    };

	    //! The Intel GPU SYCL accelerator device type trait specialization.
	    template<typename TDim, typename TIdx>
	    struct DevType<AccGpuSyclIntel<TDim, TIdx>>
	    {
	        using type = DevGpuSyclIntel;
	    };

	    //! The Intel GPU SYCL accelerator execution task type trait specialization.
	    template<typename TDim, typename TIdx, typename TWorkDiv, typename TKernelFnObj, typename... TArgs>
	    struct CreateTaskKernel<AccGpuSyclIntel<TDim, TIdx>, TWorkDiv, TKernelFnObj, TArgs...>
	    {
	        static auto createTaskKernel(TWorkDiv const& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
	        {
	            return TaskKernelGpuSyclIntel<TDim, TIdx, TKernelFnObj, TArgs...>{
	                workDiv,
	                kernelFnObj,
	                std::forward<TArgs>(args)...};
	        }
	    };

	    //! The Intel GPU SYCL execution task platform type trait specialization.
	    template<typename TDim, typename TIdx>
	    struct PltfType<AccGpuSyclIntel<TDim, TIdx>>
	    {
	        using type = PltfGpuSyclIntel;
	    };

	    template<typename TDim, typename TIdx>
	    struct AccToTag<alpaka::AccGpuSyclIntel<TDim, TIdx>>
	    {
	        using type = alpaka::TagGpuSyclIntel;
	    };

	    template<typename TDim, typename TIdx>
	    struct TagToAcc<alpaka::TagGpuSyclIntel, TDim, TIdx>
	    {
	        using type = alpaka::AccGpuSyclIntel<TDim, TIdx>;
	    };
	} // namespace alpaka::trait

	#endif
	// ==
	// == ./include/alpaka/acc/AccGpuSyclIntel.hpp ==
	// ============================================================================

// #include "alpaka/acc/Tag.hpp"    // amalgamate: file already expanded
// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
// atomic
// #include "alpaka/atomic/AtomicCpu.hpp"    // amalgamate: file already expanded
// #include "alpaka/atomic/AtomicGenericSycl.hpp"    // amalgamate: file already expanded
// #include "alpaka/atomic/AtomicNoOp.hpp"    // amalgamate: file already expanded
// #include "alpaka/atomic/AtomicOmpBuiltIn.hpp"    // amalgamate: file already expanded
// #include "alpaka/atomic/AtomicUniformCudaHipBuiltIn.hpp"    // amalgamate: file already expanded
// #include "alpaka/atomic/Op.hpp"    // amalgamate: file already expanded
// #include "alpaka/atomic/Traits.hpp"    // amalgamate: file already expanded
// block
// shared
// dynamic
// #include "alpaka/block/shared/dyn/BlockSharedMemDynGenericSycl.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/shared/dyn/BlockSharedMemDynMember.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/shared/dyn/BlockSharedMemDynUniformCudaHipBuiltIn.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/shared/dyn/Traits.hpp"    // amalgamate: file already expanded
// static
// #include "alpaka/block/shared/st/BlockSharedMemStGenericSycl.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/shared/st/BlockSharedMemStMember.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/shared/st/BlockSharedMemStMemberMasterSync.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/shared/st/BlockSharedMemStUniformCudaHipBuiltIn.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/shared/st/Traits.hpp"    // amalgamate: file already expanded
// sync
// #include "alpaka/block/sync/BlockSyncBarrierOmp.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/sync/BlockSyncBarrierThread.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/sync/BlockSyncGenericSycl.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/sync/BlockSyncNoOp.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/sync/BlockSyncUniformCudaHipBuiltIn.hpp"    // amalgamate: file already expanded
// #include "alpaka/block/sync/Traits.hpp"    // amalgamate: file already expanded
// core
// #include "alpaka/core/Align.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/AlignedAlloc.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/BarrierThread.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/ClipCast.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/Debug.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/OmpSchedule.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/core/RemoveRestrict.hpp ==
	// ==
	/* Copyright 2021 Rene Widera
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

	namespace alpaka
	{
	    //! Removes __restrict__ from a type
	    template<typename T>
	    struct remove_restrict
	    {
	        using type = T;
	    };

	#if BOOST_COMP_MSVC
	    template<typename T>
	    struct remove_restrict<T* __restrict>
	    {
	        using type = T*;
	    };
	#else
	    template<typename T>
	    struct remove_restrict<T* __restrict__>
	    {
	        using type = T*;
	    };
	#endif

	    //! Helper to remove __restrict__ from a type
	    template<typename T>
	    using remove_restrict_t = typename remove_restrict<T>::type;
	} // namespace alpaka
	// ==
	// == ./include/alpaka/core/RemoveRestrict.hpp ==
	// ============================================================================

// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/core/ThreadPool.hpp ==
	// ==
	/* Copyright 2023 Benjamin Worpitz, René Widera, Jan Stephan, Bernhard Manfred Gruber, Jeffrey Kelling
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

	// #include <atomic>    // amalgamate: file already included
	// #include <future>    // amalgamate: file already included
	// #include <mutex>    // amalgamate: file already included
	// #include <optional>    // amalgamate: file already included
	// #include <queue>    // amalgamate: file already included
	// #include <vector>    // amalgamate: file already included

	namespace alpaka::core::detail
	{
	    //! A thread pool yielding when there is not enough work to be done.
	    struct ThreadPool final
	    {
	        using Task = std::packaged_task<void()>;

	        //! Creates a thread pool with a given thread count
	        explicit ThreadPool(std::size_t threadCount)
	        {
	            if(threadCount < 1)
	                throw std::invalid_argument("The argument 'threadCount' has to be greate or equal to one!");
	            m_threads.reserve(threadCount);
	            for(std::size_t i = 0; i < threadCount; ++i)
	                m_threads.emplace_back([this] { threadFunc(); });
	        }

	        //! Destroys the thread pool, blocking until all enqueued work is done.
	        ~ThreadPool()
	        {
	            m_stop = true; // Signal that concurrent executors should not perform any new work
	            for(auto& t : m_threads)
	            {
	                if(std::this_thread::get_id() == t.get_id())
	                {
	                    std::cerr << "ERROR in ThreadPool joins itself" << std::endl;
	                    std::abort();
	                }
	                t.join();
	            }
	        }

	        //! Runs the given function on one of the pool in First In First Out (FIFO) order.
	        //!
	        //! \param task Function object to be called on the pool. Takes an arbitrary number of arguments. Must return
	        //!             void.
	        //! \param args Arguments for task, cannot be moved. If such parameters must be used, use a lambda and capture
	        //!             via move then move the lambda.
	        //! \return     A future to the created task.
	        template<typename TFnObj, typename... TArgs>
	        auto enqueueTask(TFnObj&& task, TArgs&&... args)
	        {
	            auto ptask = Task{[=, t = std::forward<TFnObj>(task)] { t(args...); }};
	            auto future = ptask.get_future();
	            {
	                std::lock_guard<std::mutex> lock{m_mutex};
	                m_tasks.push(std::move(ptask));
	            }
	            return future;
	        }

	    private:
	        void threadFunc()
	        {
	            while(!m_stop.load(std::memory_order_relaxed))
	            {
	                std::optional<Task> task;
	                {
	                    std::lock_guard<std::mutex> lock{m_mutex};
	                    if(!m_tasks.empty())
	                    {
	                        task = std::move(m_tasks.front());
	                        m_tasks.pop();
	                    }
	                }
	                if(task)
	                    (*task)();
	                else
	                    std::this_thread::yield();
	            }
	        }

	        std::vector<std::thread> m_threads;
	        std::queue<Task> m_tasks; // TODO(bgruber): we could consider a lock-free queue here
	        std::mutex m_mutex;
	        std::atomic<bool> m_stop = false;
	    };
	} // namespace alpaka::core::detail
	// ==
	// == ./include/alpaka/core/ThreadPool.hpp ==
	// ============================================================================

// #include "alpaka/core/Unreachable.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/core/Unroll.hpp ==
	// ==
	/* Copyright 2021 Benjamin Worpitz, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

	//! Suggests unrolling of the directly following loop to the compiler.
	//!
	//! Usage:
	//!  `ALPAKA_UNROLL
	//!  for(...){...}`
	// \TODO: Implement for other compilers.
	#if BOOST_ARCH_PTX
	#    define ALPAKA_UNROLL_STRINGIFY(x) #    x
	#    define ALPAKA_UNROLL(...) _Pragma(ALPAKA_UNROLL_STRINGIFY(unroll __VA_ARGS__))
	#elif BOOST_COMP_IBM || BOOST_COMP_SUNPRO || BOOST_COMP_HPACC
	#    define ALPAKA_UNROLL_STRINGIFY(x) #    x
	#    define ALPAKA_UNROLL(...) _Pragma(ALPAKA_UNROLL_STRINGIFY(unroll(__VA_ARGS__)))
	#elif BOOST_COMP_PGI
	#    define ALPAKA_UNROLL(...) _Pragma("unroll")
	#else
	#    define ALPAKA_UNROLL(...)
	#endif
	// ==
	// == ./include/alpaka/core/Unroll.hpp ==
	// ============================================================================

// #include "alpaka/core/Utility.hpp"    // amalgamate: file already expanded
// #include "alpaka/core/Vectorize.hpp"    // amalgamate: file already expanded
// dev
// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
// #include "alpaka/dev/DevCpuSyclIntel.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/dev/DevCudaRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiCudaRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

	namespace alpaka
	{
	    //! The CUDA RT device handle.
	    using DevCudaRt = DevUniformCudaHipRt<ApiCudaRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_CUDA_ENABLED
	// ==
	// == ./include/alpaka/dev/DevCudaRt.hpp ==
	// ============================================================================

// #include "alpaka/dev/DevFpgaSyclIntel.hpp"    // amalgamate: file already expanded
// #include "alpaka/dev/DevGenericSycl.hpp"    // amalgamate: file already expanded
// #include "alpaka/dev/DevGpuSyclIntel.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/dev/DevHipRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_HIP_ENABLED

	namespace alpaka
	{
	    //! The HIP RT device handle.
	    using DevHipRt = DevUniformCudaHipRt<ApiHipRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_HIP_ENABLED
	// ==
	// == ./include/alpaka/dev/DevHipRt.hpp ==
	// ============================================================================

// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/dev/cpu/Wait.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Rene Widera, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/event/EventCpu.hpp ==
		// ==
		/* Copyright 2020 Jeffrey Kelling, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
		// #include "alpaka/event/EventGenericThreads.hpp"    // amalgamate: file already expanded

		namespace alpaka
		{
		    using EventCpu = EventGenericThreads<DevCpu>;
		}
		// ==
		// == ./include/alpaka/event/EventCpu.hpp ==
		// ============================================================================

	// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

	namespace alpaka::trait
	{
	    //! The CPU device thread wait specialization.
	    //!
	    //! Blocks until the device has completed all preceding requested tasks.
	    //! Tasks that are enqueued or queues that are created after this call is made are not waited for.
	    template<>
	    struct CurrentThreadWaitFor<DevCpu>
	    {
	        ALPAKA_FN_HOST static auto currentThreadWaitFor(DevCpu const& dev) -> void
	        {
	            ALPAKA_DEBUG_FULL_LOG_SCOPE;

	            generic::currentThreadWaitForDevice(dev);
	        }
	    };
	} // namespace alpaka::trait
	// ==
	// == ./include/alpaka/dev/cpu/Wait.hpp ==
	// ============================================================================

// dim
	// ============================================================================
	// == ./include/alpaka/dim/DimArithmetic.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded

	// #include <type_traits>    // amalgamate: file already included

	namespace alpaka::trait
	{
	    //! The arithmetic type dimension getter trait specialization.
	    template<typename T>
	    struct DimType<T, std::enable_if_t<std::is_arithmetic_v<T>>>
	    {
	        using type = DimInt<1u>;
	    };
	} // namespace alpaka::trait
	// ==
	// == ./include/alpaka/dim/DimArithmetic.hpp ==
	// ============================================================================

// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
// event
// #include "alpaka/event/EventCpu.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/event/EventCpuSyclIntel.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevCpuSyclIntel.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/event/EventGenericSycl.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan, Antonio Di Pilato
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/event/Traits.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/queue/QueueGenericSyclBlocking.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/queue/sycl/QueueGenericSyclBase.hpp"    // amalgamate: file already expanded

			// #include <memory>    // amalgamate: file already included
			// #include <utility>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			namespace alpaka
			{
			    template<typename TDev>
			    using QueueGenericSyclBlocking = detail::QueueGenericSyclBase<TDev, true>;
			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/queue/QueueGenericSyclBlocking.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/queue/QueueGenericSyclNonBlocking.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/queue/sycl/QueueGenericSyclBase.hpp"    // amalgamate: file already expanded

			// #include <memory>    // amalgamate: file already included
			// #include <utility>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			namespace alpaka
			{
			    template<typename TDev>
			    using QueueGenericSyclNonBlocking = detail::QueueGenericSyclBase<TDev, false>;
			}

			#endif
			// ==
			// == ./include/alpaka/queue/QueueGenericSyclNonBlocking.hpp ==
			// ============================================================================

		// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

		// #include <functional>    // amalgamate: file already included
		// #include <memory>    // amalgamate: file already included
		// #include <stdexcept>    // amalgamate: file already included

		#ifdef ALPAKA_ACC_SYCL_ENABLED

		// #    include <CL/sycl.hpp>    // amalgamate: file already included

		namespace alpaka
		{
		    //! The SYCL device event.
		    template<typename TDev>
		    class EventGenericSycl final
		    {
		    public:
		        explicit EventGenericSycl(TDev const& dev) : m_dev{dev}
		        {
		        }

		        friend auto operator==(EventGenericSycl const& lhs, EventGenericSycl const& rhs) -> bool
		        {
		            return (lhs.m_event == rhs.m_event);
		        }

		        friend auto operator!=(EventGenericSycl const& lhs, EventGenericSycl const& rhs) -> bool
		        {
		            return !(lhs == rhs);
		        }

		        [[nodiscard]] auto getNativeHandle() const
		        {
		            return m_event;
		        }

		        void setEvent(sycl::event const& event)
		        {
		            m_event = event;
		        }

		        TDev m_dev;

		    private:
		        sycl::event m_event{};
		    };
		} // namespace alpaka

		namespace alpaka::trait
		{
		    //! The SYCL device event device get trait specialization.
		    template<typename TDev>
		    struct GetDev<EventGenericSycl<TDev>>
		    {
		        static auto getDev(EventGenericSycl<TDev> const& event) -> TDev
		        {
		            return event.m_dev;
		        }
		    };

		    //! The SYCL device event test trait specialization.
		    template<typename TDev>
		    struct IsComplete<EventGenericSycl<TDev>>
		    {
		        static auto isComplete(EventGenericSycl<TDev> const& event)
		        {
		            auto const status
		                = event.getNativeHandle().template get_info<sycl::info::event::command_execution_status>();
		            return (status == sycl::info::event_command_status::complete);
		        }
		    };

		    //! The SYCL queue enqueue trait specialization.
		    template<typename TDev>
		    struct Enqueue<QueueGenericSyclNonBlocking<TDev>, EventGenericSycl<TDev>>
		    {
		        static auto enqueue(QueueGenericSyclNonBlocking<TDev>& queue, EventGenericSycl<TDev>& event)
		        {
		            event.setEvent(queue.m_impl->get_last_event());
		        }
		    };

		    //! The SYCL queue enqueue trait specialization.
		    template<typename TDev>
		    struct Enqueue<QueueGenericSyclBlocking<TDev>, EventGenericSycl<TDev>>
		    {
		        static auto enqueue(QueueGenericSyclBlocking<TDev>& queue, EventGenericSycl<TDev>& event)
		        {
		            event.setEvent(queue.m_impl->get_last_event());
		        }
		    };

		    //! The SYCL device event thread wait trait specialization.
		    //!
		    //! Waits until the event itself and therefore all tasks preceding it in the queue it is enqueued to have been
		    //! completed. If the event is not enqueued to a queue the method returns immediately.
		    template<typename TDev>
		    struct CurrentThreadWaitFor<EventGenericSycl<TDev>>
		    {
		        static auto currentThreadWaitFor(EventGenericSycl<TDev> const& event)
		        {
		            event.getNativeHandle().wait_and_throw();
		        }
		    };

		    //! The SYCL queue event wait trait specialization.
		    template<typename TDev>
		    struct WaiterWaitFor<QueueGenericSyclNonBlocking<TDev>, EventGenericSycl<TDev>>
		    {
		        static auto waiterWaitFor(QueueGenericSyclNonBlocking<TDev>& queue, EventGenericSycl<TDev> const& event)
		        {
		            queue.m_impl->register_dependency(event.getNativeHandle());
		        }
		    };

		    //! The SYCL queue event wait trait specialization.
		    template<typename TDev>
		    struct WaiterWaitFor<QueueGenericSyclBlocking<TDev>, EventGenericSycl<TDev>>
		    {
		        static auto waiterWaitFor(QueueGenericSyclBlocking<TDev>& queue, EventGenericSycl<TDev> const& event)
		        {
		            queue.m_impl->register_dependency(event.getNativeHandle());
		        }
		    };

		    //! The SYCL device event wait trait specialization.
		    //!
		    //! Any future work submitted in any queue of this device will wait for event to complete before beginning
		    //! execution.
		    template<typename TDev>
		    struct WaiterWaitFor<TDev, EventGenericSycl<TDev>>
		    {
		        static auto waiterWaitFor(TDev& dev, EventGenericSycl<TDev> const& event)
		        {
		            dev.m_impl->register_dependency(event.getNativeHandle());
		        }
		    };

		    //! The SYCL device event native handle trait specialization.
		    template<typename TDev>
		    struct NativeHandle<EventGenericSycl<TDev>>
		    {
		        [[nodiscard]] static auto getNativeHandle(EventGenericSycl<TDev> const& event)
		        {
		            return event.getNativeHandle();
		        }
		    };
		} // namespace alpaka::trait

		#endif
		// ==
		// == ./include/alpaka/event/EventGenericSycl.hpp ==
		// ============================================================================


	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_CPU)

	namespace alpaka
	{
	    using EventCpuSyclIntel = EventGenericSycl<DevCpuSyclIntel>;
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/event/EventCpuSyclIntel.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/event/EventCudaRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiCudaRt.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/event/EventUniformCudaHipRt.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Andrea Bocci, Bernhard Manfred Gruber, Antonio Di Pilato
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/event/Traits.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/queue/QueueUniformCudaHipRtBlocking.hpp ==
			// ==
			/* Copyright 2022 Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
				// ============================================================================
				// == ./include/alpaka/queue/cuda_hip/QueueUniformCudaHipRt.hpp ==
				// ==
				/* Copyright 2022 Benjamin Worpitz, Matthias Werner, René Widera, Andrea Bocci, Bernhard Manfred Gruber,
				 * Antonio Di Pilato
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/CallbackThread.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
				// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
				// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded
				// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/event/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/meta/DependentFalseType.hpp"    // amalgamate: file already expanded
				// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

				// #include <condition_variable>    // amalgamate: file already included
				// #include <functional>    // amalgamate: file already included
				// #include <future>    // amalgamate: file already included
				// #include <memory>    // amalgamate: file already included
				// #include <mutex>    // amalgamate: file already included
				// #include <thread>    // amalgamate: file already included

				#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

				namespace alpaka
				{
				    template<typename TApi>
				    class EventUniformCudaHipRt;

				    namespace uniform_cuda_hip::detail
				    {
				        //! The CUDA/HIP RT queue implementation.
				        template<typename TApi>
				        class QueueUniformCudaHipRtImpl final
				        {
				        public:
				            ALPAKA_FN_HOST QueueUniformCudaHipRtImpl(DevUniformCudaHipRt<TApi> const& dev)
				                : m_dev(dev)
				                , m_UniformCudaHipQueue()
				            {
				                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

				                // Set the current device.
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(m_dev.getNativeHandle()));

				                // - [cuda/hip]StreamDefault: Default queue creation flag.
				                // - [cuda/hip]StreamNonBlocking: Specifies that work running in the created queue may run
				                // concurrently with work in queue 0 (the NULL queue),
				                //   and that the created queue should perform no implicit synchronization with queue 0.
				                // Create the queue on the current device.
				                // NOTE: [cuda/hip]StreamNonBlocking is required to match the semantic implemented in the alpaka
				                // CPU queue. It would be too much work to implement implicit default queue synchronization on CPU.

				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(
				                    TApi::streamCreateWithFlags(&m_UniformCudaHipQueue, TApi::streamNonBlocking));
				            }
				            QueueUniformCudaHipRtImpl(QueueUniformCudaHipRtImpl&&) = default;
				            auto operator=(QueueUniformCudaHipRtImpl&&) -> QueueUniformCudaHipRtImpl& = delete;
				            ALPAKA_FN_HOST ~QueueUniformCudaHipRtImpl()
				            {
				                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

				                // Make sure all pending async work is finished before destroying the stream to guarantee determinism.
				                // This would not be necessary for plain CUDA/HIP operations, but we can have host functions in the
				                // stream, which reference this queue instance and its CallbackThread. Make sure they are done.
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_NOEXCEPT(TApi::streamSynchronize(m_UniformCudaHipQueue));
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_NOEXCEPT(TApi::streamDestroy(m_UniformCudaHipQueue));
				            }

				            [[nodiscard]] auto getNativeHandle() const noexcept
				            {
				                return m_UniformCudaHipQueue;
				            }

				        public:
				            DevUniformCudaHipRt<TApi> const m_dev; //!< The device this queue is bound to.
				            core::CallbackThread m_callbackThread;

				        private:
				            typename TApi::Stream_t m_UniformCudaHipQueue;
				        };

				        //! The CUDA/HIP RT queue.
				        template<typename TApi, bool TBlocking>
				        class QueueUniformCudaHipRt
				            : public concepts::Implements<ConceptCurrentThreadWaitFor, QueueUniformCudaHipRt<TApi, TBlocking>>
				            , public concepts::Implements<ConceptQueue, QueueUniformCudaHipRt<TApi, TBlocking>>
				            , public concepts::Implements<ConceptGetDev, QueueUniformCudaHipRt<TApi, TBlocking>>
				        {
				        public:
				            ALPAKA_FN_HOST QueueUniformCudaHipRt(DevUniformCudaHipRt<TApi> const& dev)
				                : m_spQueueImpl(std::make_shared<QueueUniformCudaHipRtImpl<TApi>>(dev))
				            {
				            }
				            ALPAKA_FN_HOST auto operator==(QueueUniformCudaHipRt const& rhs) const -> bool
				            {
				                return (m_spQueueImpl == rhs.m_spQueueImpl);
				            }
				            ALPAKA_FN_HOST auto operator!=(QueueUniformCudaHipRt const& rhs) const -> bool
				            {
				                return !((*this) == rhs);
				            }

				            [[nodiscard]] auto getNativeHandle() const noexcept
				            {
				                return m_spQueueImpl->getNativeHandle();
				            }
				            auto getCallbackThread() -> core::CallbackThread&
				            {
				                return m_spQueueImpl->m_callbackThread;
				            }

				        public:
				            std::shared_ptr<QueueUniformCudaHipRtImpl<TApi>> m_spQueueImpl;
				        };
				    } // namespace uniform_cuda_hip::detail

				    namespace trait
				    {
				        //! The CUDA/HIP RT queue device get trait specialization.
				        template<typename TApi, bool TBlocking>
				        struct GetDev<uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking>>
				        {
				            ALPAKA_FN_HOST static auto getDev(
				                uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking> const& queue)
				                -> DevUniformCudaHipRt<TApi>
				            {
				                return queue.m_spQueueImpl->m_dev;
				            }
				        };

				        //! The CUDA/HIP RT queue test trait specialization.
				        template<typename TApi, bool TBlocking>
				        struct Empty<uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking>>
				        {
				            ALPAKA_FN_HOST static auto empty(
				                uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking> const& queue) -> bool
				            {
				                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

				                // Query is allowed even for queues on non current device.
				                typename TApi::Error_t ret = TApi::success;
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_IGNORE(
				                    ret = TApi::streamQuery(queue.getNativeHandle()),
				                    TApi::errorNotReady);
				                return (ret == TApi::success);
				            }
				        };

				        //! The CUDA/HIP RT queue thread wait trait specialization.
				        //!
				        //! Blocks execution of the calling thread until the queue has finished processing all previously requested
				        //! tasks (kernels, data copies, ...)
				        template<typename TApi, bool TBlocking>
				        struct CurrentThreadWaitFor<uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking>>
				        {
				            ALPAKA_FN_HOST static auto currentThreadWaitFor(
				                uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking> const& queue) -> void
				            {
				                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

				                // Sync is allowed even for queues on non current device.
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::streamSynchronize(queue.getNativeHandle()));
				            }
				        };

				        //! The CUDA/HIP RT blocking queue device type trait specialization.
				        template<typename TApi, bool TBlocking>
				        struct DevType<uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking>>
				        {
				            using type = DevUniformCudaHipRt<TApi>;
				        };

				        //! The CUDA/HIP RT blocking queue event type trait specialization.
				        template<typename TApi, bool TBlocking>
				        struct EventType<uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking>>
				        {
				            using type = EventUniformCudaHipRt<TApi>;
				        };

				        //! The CUDA/HIP RT blocking queue enqueue trait specialization.
				        template<typename TApi, bool TBlocking, typename TTask>
				        struct Enqueue<uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking>, TTask>
				        {
				            using QueueImpl = uniform_cuda_hip::detail::QueueUniformCudaHipRtImpl<TApi>;

				            struct HostFuncData
				            {
				                // We don't need to keep the queue alive, because in it's dtor it will synchronize with the CUDA/HIP
				                // stream and wait until all host functions and the CallbackThread are done. It's actually an error to
				                // copy the queue into the host function. Destroying it here would call CUDA/HIP APIs from the host
				                // function. Passing it further to the Callback thread, would make the Callback thread hold a task
				                // containing the queue with the CallbackThread itself. Destroying the task if no other queue instance
				                // exists will make the CallbackThread join itself and crash.
				                QueueImpl& q;
				                TTask t;
				            };

				            ALPAKA_FN_HOST static void uniformCudaHipRtHostFunc(void* arg)
				            {
				                auto data = std::unique_ptr<HostFuncData>(reinterpret_cast<HostFuncData*>(arg));
				                auto& queue = data->q;
				                auto f = queue.m_callbackThread.submit(
				                    [data = std::move(data)]() mutable
				                    {
				                        data->t();
				                        data.reset(); // destroy the task
				                    });
				                f.wait();
				            } // destroys the future `f`, destroying the packaged task and the above lambda

				            ALPAKA_FN_HOST static auto enqueue(
				                uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking>& queue,
				                TTask const& task) -> void
				            {
				                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::launchHostFunc(
				                    queue.getNativeHandle(),
				                    uniformCudaHipRtHostFunc,
				                    new HostFuncData{*queue.m_spQueueImpl, task}));
				                if constexpr(TBlocking)
				                    ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::streamSynchronize(queue.getNativeHandle()));
				            }
				        };

				        //! The CUDA/HIP RT blocking queue native handle trait specialization.
				        template<typename TApi, bool TBlocking>
				        struct NativeHandle<uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking>>
				        {
				            [[nodiscard]] static auto getNativeHandle(
				                uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, TBlocking> const& queue)
				            {
				                return queue.getNativeHandle();
				            }
				        };
				    } // namespace trait
				} // namespace alpaka

				#endif
				// ==
				// == ./include/alpaka/queue/cuda_hip/QueueUniformCudaHipRt.hpp ==
				// ============================================================================


			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    //! The CUDA/HIP RT blocking queue.
			    template<typename TApi>
			    using QueueUniformCudaHipRtBlocking = uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, true>;

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/queue/QueueUniformCudaHipRtBlocking.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/queue/QueueUniformCudaHipRtNonBlocking.hpp ==
			// ==
			/* Copyright 2022 Andrea Bocci
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/queue/cuda_hip/QueueUniformCudaHipRt.hpp"    // amalgamate: file already expanded

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    //! The CUDA/HIP RT non-blocking queue.
			    template<typename TApi>
			    using QueueUniformCudaHipRtNonBlocking = uniform_cuda_hip::detail::QueueUniformCudaHipRt<TApi, false>;

			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/queue/QueueUniformCudaHipRtNonBlocking.hpp ==
			// ============================================================================

		// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

		// #include <functional>    // amalgamate: file already included
		// #include <memory>    // amalgamate: file already included
		// #include <stdexcept>    // amalgamate: file already included

		#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

		namespace alpaka
		{
		    namespace uniform_cuda_hip::detail
		    {
		        //! The CUDA/HIP RT device event implementation.
		        template<typename TApi>
		        class EventUniformCudaHipImpl final
		        {
		        public:
		            ALPAKA_FN_HOST EventUniformCudaHipImpl(DevUniformCudaHipRt<TApi> const& dev, bool bBusyWait)
		                : m_dev(dev)
		                , m_UniformCudaHipEvent()
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                // Set the current device.
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(m_dev.getNativeHandle()));

		                // Create the event on the current device with the specified flags. Valid flags include:
		                // - cuda/hip-EventDefault: Default event creation flag.
		                // - cuda/hip-EventBlockingSync : Specifies that event should use blocking synchronization.
		                //   A host thread that uses cuda/hip-EventSynchronize() to wait on an event created with this flag
		                //   will block until the event actually completes.
		                // - cuda/hip-EventDisableTiming : Specifies that the created event does not need to record timing
		                // data.
		                //   Events created with this flag specified and the cuda/hip-EventBlockingSync flag not specified
		                //   will provide the best performance when used with cudaStreamWaitEvent() and cudaEventQuery().
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::eventCreateWithFlags(
		                    &m_UniformCudaHipEvent,
		                    (bBusyWait ? TApi::eventDefault : TApi::eventBlockingSync) | TApi::eventDisableTiming));
		            }
		            EventUniformCudaHipImpl(EventUniformCudaHipImpl const&) = delete;
		            auto operator=(EventUniformCudaHipImpl const&) -> EventUniformCudaHipImpl& = delete;
		            ALPAKA_FN_HOST ~EventUniformCudaHipImpl()
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                // In case event has been recorded but has not yet been completed when cuda/hip-EventDestroy() is
		                // called, the function will return immediately and the resources associated with event will be
		                // released automatically once the device has completed event.
		                // -> No need to synchronize here.
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_NOEXCEPT(TApi::eventDestroy(m_UniformCudaHipEvent));
		            }

		            [[nodiscard]] auto getNativeHandle() const noexcept
		            {
		                return m_UniformCudaHipEvent;
		            }

		        public:
		            DevUniformCudaHipRt<TApi> const m_dev; //!< The device this event is bound to.

		        private:
		            typename TApi::Event_t m_UniformCudaHipEvent;
		        };
		    } // namespace uniform_cuda_hip::detail

		    //! The CUDA/HIP RT device event.
		    template<typename TApi>
		    class EventUniformCudaHipRt final
		        : public concepts::Implements<ConceptCurrentThreadWaitFor, EventUniformCudaHipRt<TApi>>
		        , public concepts::Implements<ConceptGetDev, EventUniformCudaHipRt<TApi>>
		    {
		    public:
		        ALPAKA_FN_HOST EventUniformCudaHipRt(DevUniformCudaHipRt<TApi> const& dev, bool bBusyWait = true)
		            : m_spEventImpl(std::make_shared<uniform_cuda_hip::detail::EventUniformCudaHipImpl<TApi>>(dev, bBusyWait))
		        {
		            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;
		        }
		        ALPAKA_FN_HOST auto operator==(EventUniformCudaHipRt<TApi> const& rhs) const -> bool
		        {
		            return (m_spEventImpl == rhs.m_spEventImpl);
		        }
		        ALPAKA_FN_HOST auto operator!=(EventUniformCudaHipRt<TApi> const& rhs) const -> bool
		        {
		            return !((*this) == rhs);
		        }

		        [[nodiscard]] auto getNativeHandle() const noexcept
		        {
		            return m_spEventImpl->getNativeHandle();
		        }

		    public:
		        std::shared_ptr<uniform_cuda_hip::detail::EventUniformCudaHipImpl<TApi>> m_spEventImpl;
		    };
		    namespace trait
		    {
		        //! The CUDA/HIP RT device event device type trait specialization.
		        template<typename TApi>
		        struct DevType<EventUniformCudaHipRt<TApi>>
		        {
		            using type = DevUniformCudaHipRt<TApi>;
		        };
		        //! The CUDA/HIP RT device event device get trait specialization.
		        template<typename TApi>
		        struct GetDev<EventUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto getDev(EventUniformCudaHipRt<TApi> const& event) -> DevUniformCudaHipRt<TApi>
		            {
		                return event.m_spEventImpl->m_dev;
		            }
		        };

		        //! The CUDA/HIP RT device event test trait specialization.
		        template<typename TApi>
		        struct IsComplete<EventUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto isComplete(EventUniformCudaHipRt<TApi> const& event) -> bool
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                // Query is allowed even for events on non current device.
		                typename TApi::Error_t ret = TApi::success;
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_IGNORE(
		                    ret = TApi::eventQuery(event.getNativeHandle()),
		                    TApi::errorNotReady);
		                return (ret == TApi::success);
		            }
		        };

		        //! The CUDA/HIP RT queue enqueue trait specialization.
		        template<typename TApi>
		        struct Enqueue<QueueUniformCudaHipRtNonBlocking<TApi>, EventUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto enqueue(
		                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
		                EventUniformCudaHipRt<TApi>& event) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::eventRecord(event.getNativeHandle(), queue.getNativeHandle()));
		            }
		        };
		        //! The CUDA/HIP RT queue enqueue trait specialization.
		        template<typename TApi>
		        struct Enqueue<QueueUniformCudaHipRtBlocking<TApi>, EventUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto enqueue(
		                QueueUniformCudaHipRtBlocking<TApi>& queue,
		                EventUniformCudaHipRt<TApi>& event) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::eventRecord(event.getNativeHandle(), queue.getNativeHandle()));
		            }
		        };

		        //! The CUDA/HIP RT device event thread wait trait specialization.
		        //!
		        //! Waits until the event itself and therefore all tasks preceding it in the queue it is enqueued to have been
		        //! completed. If the event is not enqueued to a queue the method returns immediately.
		        template<typename TApi>
		        struct CurrentThreadWaitFor<EventUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto currentThreadWaitFor(EventUniformCudaHipRt<TApi> const& event) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                // Sync is allowed even for events on non current device.
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::eventSynchronize(event.getNativeHandle()));
		            }
		        };
		        //! The CUDA/HIP RT queue event wait trait specialization.
		        template<typename TApi>
		        struct WaiterWaitFor<QueueUniformCudaHipRtNonBlocking<TApi>, EventUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto waiterWaitFor(
		                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
		                EventUniformCudaHipRt<TApi> const& event) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(
		                    TApi::streamWaitEvent(queue.getNativeHandle(), event.getNativeHandle(), 0));
		            }
		        };
		        //! The CUDA/HIP RT queue event wait trait specialization.
		        template<typename TApi>
		        struct WaiterWaitFor<QueueUniformCudaHipRtBlocking<TApi>, EventUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto waiterWaitFor(
		                QueueUniformCudaHipRtBlocking<TApi>& queue,
		                EventUniformCudaHipRt<TApi> const& event) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(
		                    TApi::streamWaitEvent(queue.getNativeHandle(), event.getNativeHandle(), 0));
		            }
		        };
		        //! The CUDA/HIP RT device event wait trait specialization.
		        //!
		        //! Any future work submitted in any queue of this device will wait for event to complete before beginning
		        //! execution.
		        template<typename TApi>
		        struct WaiterWaitFor<DevUniformCudaHipRt<TApi>, EventUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto waiterWaitFor(
		                DevUniformCudaHipRt<TApi>& dev,
		                EventUniformCudaHipRt<TApi> const& event) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                // Set the current device.
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(dev.getNativeHandle()));

		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::streamWaitEvent(nullptr, event.getNativeHandle(), 0));
		            }
		        };
		        //! The CUDA/HIP RT event native handle trait specialization.
		        template<typename TApi>
		        struct NativeHandle<EventUniformCudaHipRt<TApi>>
		        {
		            [[nodiscard]] static auto getNativeHandle(EventUniformCudaHipRt<TApi> const& event)
		            {
		                return event.getNativeHandle();
		            }
		        };
		    } // namespace trait
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/event/EventUniformCudaHipRt.hpp ==
		// ============================================================================


	#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

	namespace alpaka
	{
	    //! The CUDA RT device event.
	    using EventCudaRt = EventUniformCudaHipRt<ApiCudaRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_CUDA_ENABLED
	// ==
	// == ./include/alpaka/event/EventCudaRt.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/event/EventFpgaSyclIntel.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevFpgaSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/event/EventGenericSycl.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_FPGA)

	namespace alpaka
	{
	    using EventFpgaSyclIntel = EventGenericSycl<DevFpgaSyclIntel>;
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/event/EventFpgaSyclIntel.hpp ==
	// ============================================================================

// #include "alpaka/event/EventGenericSycl.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/event/EventGpuSyclIntel.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevGpuSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/event/EventGenericSycl.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_GPU)

	namespace alpaka
	{
	    using EventGpuSyclIntel = EventGenericSycl<DevGpuSyclIntel>;
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/event/EventGpuSyclIntel.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/event/EventHipRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/event/EventUniformCudaHipRt.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_HIP_ENABLED

	namespace alpaka
	{
	    //! The HIP RT device event.
	    using EventHipRt = EventUniformCudaHipRt<ApiHipRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_HIP_ENABLED
	// ==
	// == ./include/alpaka/event/EventHipRt.hpp ==
	// ============================================================================

// #include "alpaka/event/Traits.hpp"    // amalgamate: file already expanded
// extent
// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
// idx
	// ============================================================================
	// == ./include/alpaka/idx/Accessors.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Positioning.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded
	// #include "alpaka/workdiv/Traits.hpp"    // amalgamate: file already expanded

	// #include <utility>    // amalgamate: file already included

	namespace alpaka
	{
	    //! Get the indices requested.
	    ALPAKA_NO_HOST_ACC_WARNING
	    template<typename TOrigin, typename TUnit, typename TIdx, typename TWorkDiv>
	    ALPAKA_FN_HOST_ACC auto getIdx(TIdx const& idx, TWorkDiv const& workDiv) -> Vec<Dim<TWorkDiv>, Idx<TIdx>>
	    {
	        return trait::GetIdx<TIdx, TOrigin, TUnit>::getIdx(idx, workDiv);
	    }
	    //! Get the indices requested.
	    ALPAKA_NO_HOST_ACC_WARNING
	    template<typename TOrigin, typename TUnit, typename TIdxWorkDiv>
	    ALPAKA_FN_HOST_ACC auto getIdx(TIdxWorkDiv const& idxWorkDiv) -> Vec<Dim<TIdxWorkDiv>, Idx<TIdxWorkDiv>>
	    {
	        return trait::GetIdx<TIdxWorkDiv, TOrigin, TUnit>::getIdx(idxWorkDiv, idxWorkDiv);
	    }

	    namespace trait
	    {
	        //! The grid block index get trait specialization for classes with IdxGbBase member type.
	        template<typename TIdxGb>
	        struct GetIdx<TIdxGb, origin::Grid, unit::Blocks>
	        {
	            using ImplementationBase = concepts::ImplementationBase<ConceptIdxGb, TIdxGb>;
	            //! \return The index of the current thread in the grid.
	            ALPAKA_NO_HOST_ACC_WARNING
	            template<typename TWorkDiv>
	            ALPAKA_FN_HOST_ACC static auto getIdx(TIdxGb const& idx, TWorkDiv const& workDiv)
	                -> Vec<Dim<ImplementationBase>, Idx<ImplementationBase>>
	            {
	                return trait::GetIdx<ImplementationBase, origin::Grid, unit::Blocks>::getIdx(idx, workDiv);
	            }
	        };

	        //! The block thread index get trait specialization for classes with IdxBtBase member type.
	        template<typename TIdxBt>
	        struct GetIdx<TIdxBt, origin::Block, unit::Threads>
	        {
	            using ImplementationBase = concepts::ImplementationBase<ConceptIdxBt, TIdxBt>;
	            //! \return The index of the current thread in the grid.
	            ALPAKA_NO_HOST_ACC_WARNING
	            template<typename TWorkDiv>
	            ALPAKA_FN_HOST_ACC static auto getIdx(TIdxBt const& idx, TWorkDiv const& workDiv)
	                -> Vec<Dim<ImplementationBase>, Idx<ImplementationBase>>
	            {
	                return trait::GetIdx<ImplementationBase, origin::Block, unit::Threads>::getIdx(idx, workDiv);
	            }
	        };

	        //! The grid thread index get trait specialization.
	        template<typename TIdx>
	        struct GetIdx<TIdx, origin::Grid, unit::Threads>
	        {
	            //! \return The index of the current thread in the grid.
	            ALPAKA_NO_HOST_ACC_WARNING
	            template<typename TWorkDiv>
	            ALPAKA_FN_HOST_ACC static auto getIdx(TIdx const& idx, TWorkDiv const& workDiv)
	            {
	                return alpaka::getIdx<origin::Grid, unit::Blocks>(idx, workDiv)
	                           * getWorkDiv<origin::Block, unit::Threads>(workDiv)
	                       + alpaka::getIdx<origin::Block, unit::Threads>(idx, workDiv);
	            }
	        };
	    } // namespace trait
	    //! Get the index of the first element this thread computes.
	    ALPAKA_NO_HOST_ACC_WARNING
	    template<typename TIdxWorkDiv, typename TGridThreadIdx, typename TThreadElemExtent>
	    ALPAKA_FN_HOST_ACC auto getIdxThreadFirstElem(
	        [[maybe_unused]] TIdxWorkDiv const& idxWorkDiv,
	        TGridThreadIdx const& gridThreadIdx,
	        TThreadElemExtent const& threadElemExtent) -> Vec<Dim<TIdxWorkDiv>, Idx<TIdxWorkDiv>>
	    {
	        return gridThreadIdx * threadElemExtent;
	    }
	    //! Get the index of the first element this thread computes.
	    ALPAKA_NO_HOST_ACC_WARNING
	    template<typename TIdxWorkDiv, typename TGridThreadIdx>
	    ALPAKA_FN_HOST_ACC auto getIdxThreadFirstElem(TIdxWorkDiv const& idxWorkDiv, TGridThreadIdx const& gridThreadIdx)
	        -> Vec<Dim<TIdxWorkDiv>, Idx<TIdxWorkDiv>>
	    {
	        auto const threadElemExtent(alpaka::getWorkDiv<alpaka::Thread, alpaka::Elems>(idxWorkDiv));
	        return getIdxThreadFirstElem(idxWorkDiv, gridThreadIdx, threadElemExtent);
	    }
	    //! Get the index of the first element this thread computes.
	    ALPAKA_NO_HOST_ACC_WARNING
	    template<typename TIdxWorkDiv>
	    ALPAKA_FN_HOST_ACC auto getIdxThreadFirstElem(TIdxWorkDiv const& idxWorkDiv)
	        -> Vec<Dim<TIdxWorkDiv>, Idx<TIdxWorkDiv>>
	    {
	        auto const gridThreadIdx(alpaka::getIdx<alpaka::Grid, alpaka::Threads>(idxWorkDiv));
	        return getIdxThreadFirstElem(idxWorkDiv, gridThreadIdx);
	    }
	} // namespace alpaka
	// ==
	// == ./include/alpaka/idx/Accessors.hpp ==
	// ============================================================================

// #include "alpaka/idx/MapIdx.hpp"    // amalgamate: file already expanded
// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
// #include "alpaka/idx/bt/IdxBtGenericSycl.hpp"    // amalgamate: file already expanded
// #include "alpaka/idx/bt/IdxBtOmp.hpp"    // amalgamate: file already expanded
// #include "alpaka/idx/bt/IdxBtRefThreadIdMap.hpp"    // amalgamate: file already expanded
// #include "alpaka/idx/bt/IdxBtUniformCudaHipBuiltIn.hpp"    // amalgamate: file already expanded
// #include "alpaka/idx/bt/IdxBtZero.hpp"    // amalgamate: file already expanded
// #include "alpaka/idx/gb/IdxGbGenericSycl.hpp"    // amalgamate: file already expanded
// #include "alpaka/idx/gb/IdxGbRef.hpp"    // amalgamate: file already expanded
// #include "alpaka/idx/gb/IdxGbUniformCudaHipBuiltIn.hpp"    // amalgamate: file already expanded
// kernel
	// ============================================================================
	// == ./include/alpaka/kernel/TaskKernelCpuOmp2Blocks.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Bert Wesarg, René Widera, Sergei Bastrakov, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Specialized traits.
	// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
	// #include "alpaka/acc/AccCpuOmp2Blocks.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Decay.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/OmpSchedule.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/MapIdx.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

	// #include <functional>    // amalgamate: file already included
	// #include <stdexcept>    // amalgamate: file already included
	// #include <tuple>    // amalgamate: file already included
	// #include <type_traits>    // amalgamate: file already included
	// #include <utility>    // amalgamate: file already included
	#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
	// #    include <iostream>    // amalgamate: file already included
	#endif

	#ifdef ALPAKA_ACC_CPU_B_OMP2_T_SEQ_ENABLED

	#    if _OPENMP < 200203
	#        error If ALPAKA_ACC_CPU_B_OMP2_T_SEQ_ENABLED is set, the compiler has to support OpenMP 2.0 or higher!
	#    endif

	// #    include <omp.h>    // amalgamate: file already included

	namespace alpaka
	{
	    namespace detail
	    {
	        //! Executor of parallel OpenMP loop with the given schedule
	        //!
	        //! Is explicitly specialized for all supported schedule kinds to help code optimization by compilers.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        //! \tparam TScheduleKind The schedule kind value.
	        template<typename TKernel, typename TSchedule, omp::Schedule::Kind TScheduleKind>
	        struct ParallelForImpl;

	        //! Executor of parallel OpenMP loop with no schedule set
	        //!
	        //! Does not use chunk size.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelForImpl<TKernel, TSchedule, omp::Schedule::NoSchedule>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const&,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const&)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        /* Implementations for Static, Dynamic and Guided follow the same pattern.
	         * There are two specializations of ParallelForImpl for compile-time dispatch depending on whether the
	         * OmpSchedule trait is specialized.
	         * The no trait case is further compile-time dispatched with a helper ParallelForStaticImpl.
	         * It is based on whether ompScheduleChunkSize member is available.
	         */

	        //! Executor of parallel OpenMP loop with the static schedule
	        //!
	        //! Specialization for kernels specializing the OmpSchedule trait.
	        //!
	        //! \tparam TKernel The kernel type.
	        template<typename TKernel>
	        struct ParallelForImpl<TKernel, omp::Schedule, omp::Schedule::Static>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            //! \param schedule The schedule object.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const&,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                omp::Schedule const& schedule)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(static, schedule.chunkSize)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(static, schedule.chunkSize)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Helper executor of parallel OpenMP loop with the static schedule
	        //!
	        //! Generel implementation is for TKernel types without member ompScheduleChunkSize.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule, typename TSfinae = void>
	        struct ParallelForStaticImpl
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const&,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const&)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(static)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(static)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Helper type to check if TKernel has member ompScheduleChunkSize
	        //!
	        //! Is void for those types, ill-formed otherwise.
	        //!
	        //! \tparam TKernel The kernel type.
	        template<typename TKernel>
	        using HasScheduleChunkSize = std::void_t<decltype(TKernel::ompScheduleChunkSize)>;

	        //! Helper executor of parallel OpenMP loop with the static schedule
	        //!
	        //! Specialization for kernels with ompScheduleChunkSize member.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelForStaticImpl<TKernel, TSchedule, HasScheduleChunkSize<TKernel>>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param kernel The kernel instance reference
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const& kernel,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const&)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(static, kernel.ompScheduleChunkSize)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(static, kernel.ompScheduleChunkSize)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Executor of parallel OpenMP loop with the static schedule
	        //!
	        //! Specialization for kernels not specializing the OmpSchedule trait.
	        //! Falls back to ParallelForStaticImpl for further dispatch.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelForImpl<TKernel, TSchedule, omp::Schedule::Static> : ParallelForStaticImpl<TKernel, TSchedule>
	        {
	        };

	        //! Executor of parallel OpenMP loop with the dynamic schedule
	        //!
	        //! Specialization for kernels specializing the OmpSchedule trait.
	        //!
	        //! \tparam TKernel The kernel type.
	        template<typename TKernel>
	        struct ParallelForImpl<TKernel, omp::Schedule, omp::Schedule::Dynamic>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            //! \param schedule The schedule object.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const&,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                omp::Schedule const& schedule)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(dynamic, schedule.chunkSize)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(dynamic, schedule.chunkSize)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Helper executor of parallel OpenMP loop with the dynamic schedule
	        //!
	        //! Generel implementation is for TKernel types without member ompScheduleChunkSize.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule, typename TSfinae = void>
	        struct ParallelForDynamicImpl
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const&,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const&)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(dynamic)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(dynamic)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Helper executor of parallel OpenMP loop with the dynamic schedule
	        //!
	        //! Specialization for kernels with ompScheduleChunkSize member.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelForDynamicImpl<TKernel, TSchedule, HasScheduleChunkSize<TKernel>>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param kernel The kernel instance reference
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const& kernel,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const&)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(dynamic, kernel.ompScheduleChunkSize)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(dynamic, kernel.ompScheduleChunkSize)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Executor of parallel OpenMP loop with the dynamic schedule
	        //!
	        //! Specialization for kernels not specializing the OmpSchedule trait.
	        //! Falls back to ParallelForDynamicImpl for further dispatch.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelForImpl<TKernel, TSchedule, omp::Schedule::Dynamic> : ParallelForDynamicImpl<TKernel, TSchedule>
	        {
	        };

	        //! Executor of parallel OpenMP loop with the guided schedule
	        //!
	        //! Specialization for kernels specializing the OmpSchedule trait.
	        //!
	        //! \tparam TKernel The kernel type.
	        template<typename TKernel>
	        struct ParallelForImpl<TKernel, omp::Schedule, omp::Schedule::Guided>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            //! \param schedule The schedule object.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const&,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                omp::Schedule const& schedule)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(guided, schedule.chunkSize)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(guided, schedule.chunkSize)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Helper executor of parallel OpenMP loop with the guided schedule
	        //!
	        //! Generel implementation is for TKernel types without member ompScheduleChunkSize.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule, typename TSfinae = void>
	        struct ParallelForGuidedImpl
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const&,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const&)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(guided)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(guided)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Helper executor of parallel OpenMP loop with the guided schedule
	        //!
	        //! Specialization for kernels with ompScheduleChunkSize member.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelForGuidedImpl<TKernel, TSchedule, HasScheduleChunkSize<TKernel>>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param kernel The kernel instance reference
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const& kernel,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const&)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(guided, kernel.ompScheduleChunkSize)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(guided, kernel.ompScheduleChunkSize)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Executor of parallel OpenMP loop with the guided schedule
	        //!
	        //! Specialization for kernels not specializing the OmpSchedule trait.
	        //! Falls back to ParallelForGuidedImpl for further dispatch.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelForImpl<TKernel, TSchedule, omp::Schedule::Guided> : ParallelForGuidedImpl<TKernel, TSchedule>
	        {
	        };

	#    if _OPENMP >= 200805
	        //! Executor of parallel OpenMP loop with auto schedule set
	        //!
	        //! Does not use chunk size.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelForImpl<TKernel, TSchedule, omp::Schedule::Auto>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const&,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const&)
	            {
	#        pragma omp for nowait schedule(auto)
	                for(TIdx i = 0; i < numIterations; ++i)
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };
	#    endif

	        //! Executor of parallel OpenMP loop with runtime schedule set
	        //!
	        //! Does not use chunk size.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelForImpl<TKernel, TSchedule, omp::Schedule::Runtime>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const&,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const&)
	            {
	#    if _OPENMP < 200805 // For OpenMP < 3.0 you have to declare the loop index (a signed integer) outside of the loop
	                         // header.
	                std::intmax_t iNumBlocksInGrid(static_cast<std::intmax_t>(numIterations));
	                std::intmax_t i;
	#        pragma omp for nowait schedule(runtime)
	                for(i = 0; i < iNumBlocksInGrid; ++i)
	#    else
	#        pragma omp for nowait schedule(runtime)
	                for(TIdx i = 0; i < numIterations; ++i)
	#    endif
	                {
	                    // Make another lambda to work around #1288
	                    auto wrappedLoopBody = [&loopBody](auto idx) { loopBody(idx); };
	                    wrappedLoopBody(i);
	                }
	            }
	        };

	        //! Executor of parallel OpenMP loop
	        //!
	        //! Performs dispatch based on schedule kind and forwards to the corresponding ParallelForImpl.
	        //! The default implementation is for the kernels that do not set schedule in any way, compile-time dispatch.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule, typename TSfinae = void>
	        struct ParallelFor
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param kernel The kernel instance reference
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            //! \param schedule The schedule object.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const& kernel,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const& schedule)
	            {
	                // Forward to ParallelForImpl that performs dispatch by by chunk size
	                ParallelForImpl<TKernel, TSchedule, omp::Schedule::NoSchedule>{}(
	                    kernel,
	                    std::forward<TLoopBody>(loopBody),
	                    numIterations,
	                    schedule);
	            }
	        };

	        //! Executor of parallel OpenMP loop
	        //!
	        //! Performs dispatch based on schedule kind and forwards to the corresponding ParallelForImpl.
	        //! Specialization for kernels specializing the OmpSchedule trait, run-time dispatch.
	        //!
	        //! \tparam TKernel The kernel type.
	        template<typename TKernel>
	        struct ParallelFor<TKernel, omp::Schedule>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param kernel The kernel instance reference
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            //! \param schedule The schedule object.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const& kernel,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                omp::Schedule const& schedule)
	            {
	                // Forward to ParallelForImpl that performs dispatch by by chunk size
	                switch(schedule.kind)
	                {
	                case omp::Schedule::NoSchedule:
	                    ParallelForImpl<TKernel, omp::Schedule, omp::Schedule::NoSchedule>{}(
	                        kernel,
	                        std::forward<TLoopBody>(loopBody),
	                        numIterations,
	                        schedule);
	                    break;
	                case omp::Schedule::Static:
	                    ParallelForImpl<TKernel, omp::Schedule, omp::Schedule::Static>{}(
	                        kernel,
	                        std::forward<TLoopBody>(loopBody),
	                        numIterations,
	                        schedule);
	                    break;
	                case omp::Schedule::Dynamic:
	                    ParallelForImpl<TKernel, omp::Schedule, omp::Schedule::Dynamic>{}(
	                        kernel,
	                        std::forward<TLoopBody>(loopBody),
	                        numIterations,
	                        schedule);
	                    break;
	                case omp::Schedule::Guided:
	                    ParallelForImpl<TKernel, omp::Schedule, omp::Schedule::Guided>{}(
	                        kernel,
	                        std::forward<TLoopBody>(loopBody),
	                        numIterations,
	                        schedule);
	                    break;
	#    if _OPENMP >= 200805
	                case omp::Schedule::Auto:
	                    ParallelForImpl<TKernel, omp::Schedule, omp::Schedule::Auto>{}(
	                        kernel,
	                        std::forward<TLoopBody>(loopBody),
	                        numIterations,
	                        schedule);
	                    break;
	#    endif
	                case omp::Schedule::Runtime:
	                    ParallelForImpl<TKernel, omp::Schedule, omp::Schedule::Runtime>{}(
	                        kernel,
	                        std::forward<TLoopBody>(loopBody),
	                        numIterations,
	                        schedule);
	                    break;
	                }
	            }
	        };

	        //! Helper type to check if TSchedule is a type originating from OmpSchedule trait definition
	        //!
	        //! \tparam TSchedule The schedule type.
	        template<typename TSchedule>
	        using IsOmpScheduleTraitSpecialized
	            = std::integral_constant<bool, std::is_same<TSchedule, omp::Schedule>::value>;

	        //! Helper type to check if member ompScheduleKind of TKernel should be used
	        //!
	        //! For that it has to be present, and no OmpSchedule trait specialized.
	        //! Is void for those types, ill-formed otherwise.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type.
	        template<typename TKernel, typename TSchedule>
	        using UseScheduleKind
	            = std::enable_if_t<sizeof(TKernel::ompScheduleKind) && !IsOmpScheduleTraitSpecialized<TSchedule>::value>;

	        //! Executor of parallel OpenMP loop
	        //!
	        //! Performs dispatch based on schedule kind and forwards to the corresponding ParallelForImpl.
	        //! Specialization for kernels with ompScheduleKind member, compile-time dispatch.
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        template<typename TKernel, typename TSchedule>
	        struct ParallelFor<TKernel, TSchedule, UseScheduleKind<TKernel, TSchedule>>
	        {
	            //! Run parallel OpenMP loop
	            //!
	            //! \tparam TLoopBody The loop body functor type.
	            //! \tparam TIdx The index type.
	            //!
	            //! \param kernel The kernel instance reference
	            //! \param loopBody The loop body functor instance, takes iteration index as input.
	            //! \param numIterations The number of loop iterations.
	            //! \param schedule The schedule object.
	            template<typename TLoopBody, typename TIdx>
	            ALPAKA_FN_HOST void operator()(
	                TKernel const& kernel,
	                TLoopBody&& loopBody,
	                TIdx const numIterations,
	                TSchedule const& schedule)
	            {
	                // Forward to ParallelForImpl that performs dispatch by by chunk size
	                ParallelForImpl<TKernel, TSchedule, TKernel::ompScheduleKind>{}(
	                    kernel,
	                    std::forward<TLoopBody>(loopBody),
	                    numIterations,
	                    schedule);
	            }
	        };

	        //! Run parallel OpenMP loop
	        //!
	        //! \tparam TKernel The kernel type.
	        //! \tparam TLoopBody The loop body functor type.
	        //! \tparam TIdx The index type.
	        //! \tparam TSchedule The schedule type (not necessarily omp::Schedule).
	        //!
	        //! \param kernel The kernel instance reference,
	        //!        not perfect=forwarded to shorten SFINAE internally.
	        //! \param loopBody The loop body functor instance, takes iteration index as input.
	        //! \param numIterations The number of loop iterations.
	        //! \param schedule The schedule object.
	        template<typename TKernel, typename TLoopBody, typename TIdx, typename TSchedule>
	        ALPAKA_FN_HOST ALPAKA_FN_INLINE void parallelFor(
	            TKernel const& kernel,
	            TLoopBody&& loopBody,
	            TIdx const numIterations,
	            TSchedule const& schedule)
	        {
	            // Forward to ParallelFor that performs first a dispatch by schedule kind, and then by chunk size
	            ParallelFor<TKernel, TSchedule>{}(kernel, std::forward<TLoopBody>(loopBody), numIterations, schedule);
	        }

	    } // namespace detail

	    //! The CPU OpenMP 2.0 block accelerator execution task.
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuOmp2Blocks final : public WorkDivMembers<TDim, TIdx>
	    {
	    public:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST TaskKernelCpuOmp2Blocks(TWorkDiv&& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
	            : WorkDivMembers<TDim, TIdx>(std::forward<TWorkDiv>(workDiv))
	            , m_kernelFnObj(kernelFnObj)
	            , m_args(std::forward<TArgs>(args)...)
	        {
	            static_assert(
	                Dim<std::decay_t<TWorkDiv>>::value == TDim::value,
	                "The work division and the execution task have to be of the same dimensionality!");
	        }

	        //! Executes the kernel function object.
	        ALPAKA_FN_HOST auto operator()() const -> void
	        {
	            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

	            auto const gridBlockExtent = getWorkDiv<Grid, Blocks>(*this);
	            auto const blockThreadExtent = getWorkDiv<Block, Threads>(*this);
	            auto const threadElemExtent = getWorkDiv<Thread, Elems>(*this);

	            // Get the size of the block shared dynamic memory.
	            auto const blockSharedMemDynSizeBytes = std::apply(
	                [&](ALPAKA_DECAY_T(TArgs) const&... args)
	                {
	                    return getBlockSharedMemDynSizeBytes<AccCpuOmp2Blocks<TDim, TIdx>>(
	                        m_kernelFnObj,
	                        blockThreadExtent,
	                        threadElemExtent,
	                        args...);
	                },
	                m_args);

	#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
	            std::cout << __func__ << " blockSharedMemDynSizeBytes: " << blockSharedMemDynSizeBytes << " B"
	                      << std::endl;
	#    endif

	            // The number of blocks in the grid.
	            TIdx const numBlocksInGrid(gridBlockExtent.prod());
	            if(blockThreadExtent.prod() != static_cast<TIdx>(1u))
	            {
	                throw std::runtime_error("Only one thread per block allowed in the OpenMP 2.0 block accelerator!");
	            }

	            // Get the OpenMP schedule information for the given kernel and parameter types
	            auto const schedule = std::apply(
	                [&](ALPAKA_DECAY_T(TArgs) const&... args) {
	                    return getOmpSchedule<AccCpuOmp2Blocks<TDim, TIdx>>(
	                        m_kernelFnObj,
	                        blockThreadExtent,
	                        threadElemExtent,
	                        args...);
	                },
	                m_args);

	            if(::omp_in_parallel() != 0)
	            {
	#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
	                std::cout << __func__ << " already within a parallel region." << std::endl;
	#    endif
	                parallelFn(blockSharedMemDynSizeBytes, numBlocksInGrid, gridBlockExtent, schedule);
	            }
	            else
	            {
	#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
	                std::cout << __func__ << " opening new parallel region." << std::endl;
	#    endif
	#    pragma omp parallel
	                parallelFn(blockSharedMemDynSizeBytes, numBlocksInGrid, gridBlockExtent, schedule);
	            }
	        }

	    private:
	        template<typename TSchedule>
	        ALPAKA_FN_HOST auto parallelFn(
	            std::size_t const& blockSharedMemDynSizeBytes,
	            TIdx const& numBlocksInGrid,
	            Vec<TDim, TIdx> const& gridBlockExtent,
	            TSchedule const& schedule) const -> void
	        {
	#    pragma omp single nowait
	            {
	                // The OpenMP runtime does not create a parallel region when either:
	                // * only one thread is required in the num_threads clause
	                // * or only one thread is available
	                // In all other cases we expect to be in a parallel region now.
	                if((numBlocksInGrid > 1) && (::omp_get_max_threads() > 1) && (::omp_in_parallel() == 0))
	                {
	                    throw std::runtime_error("The OpenMP 2.0 runtime did not create a parallel region!");
	                }

	#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
	                std::cout << __func__ << " omp_get_num_threads: " << ::omp_get_num_threads() << std::endl;
	#    endif
	            }

	            AccCpuOmp2Blocks<TDim, TIdx> acc(
	                *static_cast<WorkDivMembers<TDim, TIdx> const*>(this),
	                blockSharedMemDynSizeBytes);

	            // Body of the OpenMP parallel loop to be executed.
	            // Index type is auto since we have a difference for OpenMP 2.0 and later ones
	            auto loopBody = [&](auto currentIndex)
	            {
	#    if _OPENMP < 200805
	                auto const i_tidx = static_cast<TIdx>(currentIndex); // for issue #840
	                auto const index = Vec<DimInt<1u>, TIdx>(i_tidx); // for issue #840
	#    else
	                auto const index = Vec<DimInt<1u>, TIdx>(currentIndex); // for issue #840
	#    endif
	                acc.m_gridBlockIdx = mapIdx<TDim::value>(index, gridBlockExtent);

	                std::apply(m_kernelFnObj, std::tuple_cat(std::tie(acc), m_args));

	                // After a block has been processed, the shared memory has to be deleted.
	                freeSharedVars(acc);
	            };

	            detail::parallelFor(m_kernelFnObj, loopBody, numBlocksInGrid, schedule);
	        }

	        TKernelFnObj m_kernelFnObj;
	        std::tuple<std::decay_t<TArgs>...> m_args;
	    };

	    namespace trait
	    {
	        //! The CPU OpenMP 2.0 grid block execution task accelerator type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct AccType<TaskKernelCpuOmp2Blocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = AccCpuOmp2Blocks<TDim, TIdx>;
	        };

	        //! The CPU OpenMP 2.0 grid block execution task device type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DevType<TaskKernelCpuOmp2Blocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU OpenMP 2.0 grid block execution task dimension getter trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DimType<TaskKernelCpuOmp2Blocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TDim;
	        };

	        //! The CPU OpenMP 2.0 grid block execution task platform type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct PltfType<TaskKernelCpuOmp2Blocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU OpenMP 2.0 block execution task idx type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct IdxType<TaskKernelCpuOmp2Blocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TIdx;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/kernel/TaskKernelCpuOmp2Blocks.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/kernel/TaskKernelCpuOmp2Threads.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Bert Wesarg, René Widera, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Specialized traits.
	// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
	// #include "alpaka/acc/AccCpuOmp2Threads.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Decay.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/meta/NdLoop.hpp ==
		// ==
		/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

		// #include <utility>    // amalgamate: file already included

		namespace alpaka::meta
		{
		    namespace detail
		    {
		        ALPAKA_NO_HOST_ACC_WARNING
		        template<typename TIndex, typename TExtentVec, typename TFnObj>
		        ALPAKA_FN_HOST_ACC constexpr void ndLoopImpl(
		            std::index_sequence<>,
		            TIndex& idx,
		            TExtentVec const&,
		            TFnObj const& f)
		        {
		            f(idx);
		        }

		        ALPAKA_NO_HOST_ACC_WARNING
		        template<std::size_t Tdim0, std::size_t... Tdims, typename TIndex, typename TExtentVec, typename TFnObj>
		        ALPAKA_FN_HOST_ACC constexpr void ndLoopImpl(
		            std::index_sequence<Tdim0, Tdims...>,
		            TIndex& idx,
		            TExtentVec const& extent,
		            TFnObj const& f)
		        {
		            static_assert(Dim<TIndex>::value > 0u, "The dimension given to ndLoop has to be larger than zero!");
		            static_assert(
		                Dim<TIndex>::value == Dim<TExtentVec>::value,
		                "The dimensions of the iteration vector and the extent vector have to be identical!");
		            static_assert(Dim<TIndex>::value > Tdim0, "The current dimension has to be in the range [0,dim-1]!");

		            for(idx[Tdim0] = 0u; idx[Tdim0] < extent[Tdim0]; ++idx[Tdim0])
		            {
		                ndLoopImpl(std::index_sequence<Tdims...>{}, idx, extent, f);
		            }
		        }
		    } // namespace detail

		    //! Loops over an n-dimensional iteration index variable calling f(idx, args...) for each iteration.
		    //! The loops are nested in the order given by the index_sequence with the first element being the outermost
		    //! and the last index the innermost loop.
		    //!
		    //! \param indexSequence A sequence of indices being a permutation of the values [0, dim-1].
		    //! \param extent N-dimensional loop extent.
		    //! \param f The function called at each iteration.
		    ALPAKA_NO_HOST_ACC_WARNING
		    template<typename TExtentVec, typename TFnObj, std::size_t... Tdims>
		    ALPAKA_FN_HOST_ACC auto ndLoop(
		        [[maybe_unused]] std::index_sequence<Tdims...> indexSequence,
		        TExtentVec const& extent,
		        TFnObj const& f) -> void
		    {
		        static_assert(
		            IntegerSequenceValuesInRange<std::index_sequence<Tdims...>, std::size_t, 0, Dim<TExtentVec>::value>::value,
		            "The values in the index_sequence have to be in the range [0,dim-1]!");
		        static_assert(
		            IntegerSequenceValuesUnique<std::index_sequence<Tdims...>>::value,
		            "The values in the index_sequence have to be unique!");

		        auto idx = Vec<Dim<TExtentVec>, Idx<TExtentVec>>::zeros();
		        detail::ndLoopImpl(std::index_sequence<Tdims...>{}, idx, extent, f);
		    }

		    //! Loops over an n-dimensional iteration index variable calling f(idx, args...) for each iteration.
		    //! The loops are nested from index zero outmost to index (dim-1) innermost.
		    //!
		    //! \param extent N-dimensional loop extent.
		    //! \param f The function called at each iteration.
		    ALPAKA_NO_HOST_ACC_WARNING
		    template<typename TExtentVec, typename TFnObj>
		    ALPAKA_FN_HOST_ACC auto ndLoopIncIdx(TExtentVec const& extent, TFnObj const& f) -> void
		    {
		        ndLoop(std::make_index_sequence<Dim<TExtentVec>::value>(), extent, f);
		    }
		} // namespace alpaka::meta
		// ==
		// == ./include/alpaka/meta/NdLoop.hpp ==
		// ============================================================================

	// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

	// #include <functional>    // amalgamate: file already included
	// #include <stdexcept>    // amalgamate: file already included
	// #include <tuple>    // amalgamate: file already included
	// #include <type_traits>    // amalgamate: file already included
	#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
	// #    include <iostream>    // amalgamate: file already included
	#endif

	#ifdef ALPAKA_ACC_CPU_B_SEQ_T_OMP2_ENABLED

	#    if _OPENMP < 200203
	#        error If ALPAKA_ACC_CPU_B_SEQ_T_OMP2_ENABLED is set, the compiler has to support OpenMP 2.0 or higher!
	#    endif

	// #    include <omp.h>    // amalgamate: file already included

	namespace alpaka
	{
	    //! The CPU OpenMP 2.0 thread accelerator execution task.
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuOmp2Threads final : public WorkDivMembers<TDim, TIdx>
	    {
	    public:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST TaskKernelCpuOmp2Threads(TWorkDiv&& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
	            : WorkDivMembers<TDim, TIdx>(std::forward<TWorkDiv>(workDiv))
	            , m_kernelFnObj(kernelFnObj)
	            , m_args(std::forward<TArgs>(args)...)
	        {
	            static_assert(
	                Dim<std::decay_t<TWorkDiv>>::value == TDim::value,
	                "The work division and the execution task have to be of the same dimensionality!");
	        }

	        //! Executes the kernel function object.
	        ALPAKA_FN_HOST auto operator()() const -> void
	        {
	            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

	            auto const gridBlockExtent = getWorkDiv<Grid, Blocks>(*this);
	            auto const blockThreadExtent = getWorkDiv<Block, Threads>(*this);
	            auto const threadElemExtent = getWorkDiv<Thread, Elems>(*this);

	            // Get the size of the block shared dynamic memory.
	            auto const blockSharedMemDynSizeBytes = std::apply(
	                [&](ALPAKA_DECAY_T(TArgs) const&... args)
	                {
	                    return getBlockSharedMemDynSizeBytes<AccCpuOmp2Threads<TDim, TIdx>>(
	                        m_kernelFnObj,
	                        blockThreadExtent,
	                        threadElemExtent,
	                        args...);
	                },
	                m_args);

	#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
	            std::cout << __func__ << " blockSharedMemDynSizeBytes: " << blockSharedMemDynSizeBytes << " B"
	                      << std::endl;
	#    endif

	            AccCpuOmp2Threads<TDim, TIdx> acc(
	                *static_cast<WorkDivMembers<TDim, TIdx> const*>(this),
	                blockSharedMemDynSizeBytes);

	            // The number of threads in this block.
	            TIdx const blockThreadCount(blockThreadExtent.prod());
	            [[maybe_unused]] int const iBlockThreadCount(static_cast<int>(blockThreadCount));

	            if(::omp_in_parallel() != 0)
	            {
	                throw std::runtime_error(
	                    "The OpenMP 2.0 thread backend can not be used within an existing parallel region!");
	            }

	            // Force the environment to use the given number of threads.
	            int const ompIsDynamic(::omp_get_dynamic());
	            ::omp_set_dynamic(0);

	            // Execute the blocks serially.
	            meta::ndLoopIncIdx(
	                gridBlockExtent,
	                [&](Vec<TDim, TIdx> const& gridBlockIdx)
	                {
	                    acc.m_gridBlockIdx = gridBlockIdx;

	// Execute the threads in parallel.

	// Parallel execution of the threads in a block is required because when syncBlockThreads is called all of them have to
	// be done with their work up to this line. So we have to spawn one OS thread per thread in a block. 'omp for' is not
	// useful because it is meant for cases where multiple iterations are executed by one thread but in our case a 1:1
	// mapping is required. Therefore we use 'omp parallel' with the specified number of threads in a block.
	#    pragma omp parallel num_threads(iBlockThreadCount)
	                    {
	                        // The guard is for gcc internal compiler error, as discussed in #735
	                        if constexpr((!BOOST_COMP_GNUC) || (BOOST_COMP_GNUC >= BOOST_VERSION_NUMBER(8, 1, 0)))
	                        {
	#    pragma omp single nowait
	                            {
	                                // The OpenMP runtime does not create a parallel region when only one thread is
	                                // required in the num_threads clause. In all other cases we expect to be in a parallel
	                                // region now.
	                                if((iBlockThreadCount > 1) && (::omp_in_parallel() == 0))
	                                {
	                                    throw std::runtime_error(
	                                        "The OpenMP 2.0 runtime did not create a parallel region!");
	                                }

	                                int const numThreads = ::omp_get_num_threads();
	                                if(numThreads != iBlockThreadCount)
	                                {
	                                    throw std::runtime_error(
	                                        "The OpenMP 2.0 runtime did not use the number of threads "
	                                        "that had been required!");
	                                }
	                            }
	                        }

	                        std::apply(m_kernelFnObj, std::tuple_cat(std::tie(acc), m_args));

	                        // Wait for all threads to finish before deleting the shared memory.
	                        // This is done by default if the omp 'nowait' clause is missing on the omp parallel directive
	                        // syncBlockThreads(acc);
	                    }

	                    // After a block has been processed, the shared memory has to be deleted.
	                    freeSharedVars(acc);
	                });

	            // Reset the dynamic thread number setting.
	            ::omp_set_dynamic(ompIsDynamic);
	        }

	    private:
	        TKernelFnObj m_kernelFnObj;
	        std::tuple<std::decay_t<TArgs>...> m_args;
	    };

	    namespace trait
	    {
	        //! The CPU OpenMP 2.0 block thread execution task accelerator type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct AccType<TaskKernelCpuOmp2Threads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = AccCpuOmp2Threads<TDim, TIdx>;
	        };

	        //! The CPU OpenMP 2.0 block thread execution task device type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DevType<TaskKernelCpuOmp2Threads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU OpenMP 2.0 block thread execution task dimension getter trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DimType<TaskKernelCpuOmp2Threads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TDim;
	        };

	        //! The CPU OpenMP 2.0 block thread execution task platform type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct PltfType<TaskKernelCpuOmp2Threads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU OpenMP 2.0 block thread execution task idx type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct IdxType<TaskKernelCpuOmp2Threads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TIdx;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/kernel/TaskKernelCpuOmp2Threads.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/kernel/TaskKernelCpuSerial.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, René Widera, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Specialized traits.
	// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
	// #include "alpaka/acc/AccCpuSerial.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Decay.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/meta/NdLoop.hpp"    // amalgamate: file already expanded
	// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

	// #include <functional>    // amalgamate: file already included
	// #include <tuple>    // amalgamate: file already included
	// #include <type_traits>    // amalgamate: file already included
	// #include <utility>    // amalgamate: file already included
	#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
	// #    include <iostream>    // amalgamate: file already included
	#endif

	#ifdef ALPAKA_ACC_CPU_B_SEQ_T_SEQ_ENABLED

	namespace alpaka
	{
	    //! The CPU serial execution task implementation.
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuSerial final : public WorkDivMembers<TDim, TIdx>
	    {
	    public:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST TaskKernelCpuSerial(TWorkDiv&& workDiv, TKernelFnObj kernelFnObj, TArgs&&... args)
	            : WorkDivMembers<TDim, TIdx>(std::forward<TWorkDiv>(workDiv))
	            , m_kernelFnObj(std::move(kernelFnObj))
	            , m_args(std::forward<TArgs>(args)...)
	        {
	            static_assert(
	                Dim<std::decay_t<TWorkDiv>>::value == TDim::value,
	                "The work division and the execution task have to be of the same dimensionality!");
	        }

	        //! Executes the kernel function object.
	        ALPAKA_FN_HOST auto operator()() const -> void
	        {
	            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

	            auto const gridBlockExtent = getWorkDiv<Grid, Blocks>(*this);
	            auto const blockThreadExtent = getWorkDiv<Block, Threads>(*this);
	            auto const threadElemExtent = getWorkDiv<Thread, Elems>(*this);

	            // Get the size of the block shared dynamic memory.
	            auto const blockSharedMemDynSizeBytes = std::apply(
	                [&](ALPAKA_DECAY_T(TArgs) const&... args)
	                {
	                    return getBlockSharedMemDynSizeBytes<AccCpuSerial<TDim, TIdx>>(
	                        m_kernelFnObj,
	                        blockThreadExtent,
	                        threadElemExtent,
	                        args...);
	                },
	                m_args);

	#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
	            std::cout << __func__ << " blockSharedMemDynSizeBytes: " << blockSharedMemDynSizeBytes << " B"
	                      << std::endl;
	#    endif

	            AccCpuSerial<TDim, TIdx> acc(
	                *static_cast<WorkDivMembers<TDim, TIdx> const*>(this),
	                blockSharedMemDynSizeBytes);

	            if(blockThreadExtent.prod() != static_cast<TIdx>(1u))
	            {
	                throw std::runtime_error("A block for the serial accelerator can only ever have one single thread!");
	            }

	            // Execute the blocks serially.
	            meta::ndLoopIncIdx(
	                gridBlockExtent,
	                [&](Vec<TDim, TIdx> const& blockThreadIdx)
	                {
	                    acc.m_gridBlockIdx = blockThreadIdx;

	                    std::apply(m_kernelFnObj, std::tuple_cat(std::tie(acc), m_args));

	                    // After a block has been processed, the shared memory has to be deleted.
	                    freeSharedVars(acc);
	                });
	        }

	    private:
	        TKernelFnObj m_kernelFnObj;
	        std::tuple<std::decay_t<TArgs>...> m_args;
	    };

	    namespace trait
	    {
	        //! The CPU serial execution task accelerator type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct AccType<TaskKernelCpuSerial<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = AccCpuSerial<TDim, TIdx>;
	        };

	        //! The CPU serial execution task device type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DevType<TaskKernelCpuSerial<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU serial execution task dimension getter trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DimType<TaskKernelCpuSerial<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TDim;
	        };

	        //! The CPU serial execution task platform type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct PltfType<TaskKernelCpuSerial<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU serial execution task idx type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct IdxType<TaskKernelCpuSerial<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TIdx;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/kernel/TaskKernelCpuSerial.hpp ==
	// ============================================================================

// #include "alpaka/kernel/TaskKernelCpuSyclIntel.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/kernel/TaskKernelCpuTbbBlocks.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Erik Zenker, René Widera, Felice Pantaleo, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Specialized traits.
	// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
	// #include "alpaka/acc/AccCpuTbbBlocks.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Decay.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/MapIdx.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/meta/NdLoop.hpp"    // amalgamate: file already expanded
	// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

	// #include <functional>    // amalgamate: file already included
	// #include <stdexcept>    // amalgamate: file already included
	// #include <tuple>    // amalgamate: file already included
	// #include <type_traits>    // amalgamate: file already included
	#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
	// #    include <iostream>    // amalgamate: file already included
	#endif

	#ifdef ALPAKA_ACC_CPU_B_TBB_T_SEQ_ENABLED

	#    include <tbb/blocked_range.h>
	#    include <tbb/parallel_for.h>
	#    include <tbb/task_group.h>

	namespace alpaka
	{
	    //! The CPU TBB block accelerator execution task.
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuTbbBlocks final : public WorkDivMembers<TDim, TIdx>
	    {
	    public:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST TaskKernelCpuTbbBlocks(TWorkDiv&& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
	            : WorkDivMembers<TDim, TIdx>(std::forward<TWorkDiv>(workDiv))
	            , m_kernelFnObj(kernelFnObj)
	            , m_args(std::forward<TArgs>(args)...)
	        {
	            static_assert(
	                Dim<std::decay_t<TWorkDiv>>::value == TDim::value,
	                "The work division and the execution task have to be of the same dimensionality!");
	        }

	        //! Executes the kernel function object.
	        ALPAKA_FN_HOST auto operator()() const -> void
	        {
	            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

	            auto const gridBlockExtent = getWorkDiv<Grid, Blocks>(*this);
	            auto const blockThreadExtent = getWorkDiv<Block, Threads>(*this);
	            auto const threadElemExtent = getWorkDiv<Thread, Elems>(*this);

	            // Get the size of the block shared dynamic memory.
	            auto const blockSharedMemDynSizeBytes = std::apply(
	                [&](ALPAKA_DECAY_T(TArgs) const&... args)
	                {
	                    return getBlockSharedMemDynSizeBytes<AccCpuTbbBlocks<TDim, TIdx>>(
	                        m_kernelFnObj,
	                        blockThreadExtent,
	                        threadElemExtent,
	                        args...);
	                },
	                m_args);

	#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
	            std::cout << __func__ << " blockSharedMemDynSizeBytes: " << blockSharedMemDynSizeBytes << " B"
	                      << std::endl;
	#    endif

	            // The number of blocks in the grid.
	            TIdx const numBlocksInGrid = gridBlockExtent.prod();

	            if(blockThreadExtent.prod() != static_cast<TIdx>(1u))
	            {
	                throw std::runtime_error("A block for the TBB accelerator can only ever have one single thread!");
	            }

	            tbb::this_task_arena::isolate(
	                [&]
	                {
	                    tbb::parallel_for(
	                        static_cast<TIdx>(0),
	                        static_cast<TIdx>(numBlocksInGrid),
	                        [&](TIdx i)
	                        {
	                            AccCpuTbbBlocks<TDim, TIdx> acc(
	                                *static_cast<WorkDivMembers<TDim, TIdx> const*>(this),
	                                blockSharedMemDynSizeBytes);

	                            acc.m_gridBlockIdx
	                                = mapIdx<TDim::value>(Vec<DimInt<1u>, TIdx>(static_cast<TIdx>(i)), gridBlockExtent);

	                            std::apply(m_kernelFnObj, std::tuple_cat(std::tie(acc), m_args));

	                            freeSharedVars(acc);
	                        });
	                });
	        }

	    private:
	        TKernelFnObj m_kernelFnObj;
	        std::tuple<std::decay_t<TArgs>...> m_args;
	    };

	    namespace trait
	    {
	        //! The CPU TBB block execution task accelerator type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct AccType<TaskKernelCpuTbbBlocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = AccCpuTbbBlocks<TDim, TIdx>;
	        };

	        //! The CPU TBB block execution task device type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DevType<TaskKernelCpuTbbBlocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU TBB block execution task dimension getter trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DimType<TaskKernelCpuTbbBlocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TDim;
	        };

	        //! The CPU TBB block execution task platform type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct PltfType<TaskKernelCpuTbbBlocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU TBB block execution task idx type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct IdxType<TaskKernelCpuTbbBlocks<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TIdx;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/kernel/TaskKernelCpuTbbBlocks.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/kernel/TaskKernelCpuThreads.hpp ==
	// ==
	/* Copyright 2023 Benjamin Worpitz, René Widera, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// Specialized traits.
	// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

	// Implementation details.
	// #include "alpaka/acc/AccCpuThreads.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Decay.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/ThreadPool.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/meta/NdLoop.hpp"    // amalgamate: file already expanded
	// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

	// #include <algorithm>    // amalgamate: file already included
	// #include <functional>    // amalgamate: file already included
	// #include <future>    // amalgamate: file already included
	// #include <thread>    // amalgamate: file already included
	// #include <tuple>    // amalgamate: file already included
	// #include <type_traits>    // amalgamate: file already included
	// #include <vector>    // amalgamate: file already included
	#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
	// #    include <iostream>    // amalgamate: file already included
	#endif

	#ifdef ALPAKA_ACC_CPU_B_SEQ_T_THREADS_ENABLED

	namespace alpaka
	{
	    //! The CPU threads execution task.
	    template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    class TaskKernelCpuThreads final : public WorkDivMembers<TDim, TIdx>
	    {
	    private:
	        // When using the thread pool the threads are yielding because this is faster.
	        // Using condition variables and going to sleep is very costly for real threads.
	        // Especially when the time to wait is really short (syncBlockThreads) yielding is much faster.
	        using ThreadPool = alpaka::core::detail::ThreadPool;

	    public:
	        template<typename TWorkDiv>
	        ALPAKA_FN_HOST TaskKernelCpuThreads(TWorkDiv&& workDiv, TKernelFnObj const& kernelFnObj, TArgs&&... args)
	            : WorkDivMembers<TDim, TIdx>(std::forward<TWorkDiv>(workDiv))
	            , m_kernelFnObj(kernelFnObj)
	            , m_args(std::forward<TArgs>(
	                  args)...) // FIXME(bgruber): this does not forward, since TArgs is not a deduced template parameter
	        {
	            static_assert(
	                Dim<std::decay_t<TWorkDiv>>::value == TDim::value,
	                "The work division and the execution task have to be of the same dimensionality!");
	        }

	        //! Executes the kernel function object.
	        ALPAKA_FN_HOST auto operator()() const -> void
	        {
	            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

	            std::apply([&](auto const&... args) { runWithArgs(args...); }, m_args);
	        }

	    private:
	        ALPAKA_FN_HOST auto runWithArgs(std::decay_t<TArgs> const&... args) const -> void
	        {
	            auto const gridBlockExtent = getWorkDiv<Grid, Blocks>(*this);
	            auto const blockThreadExtent = getWorkDiv<Block, Threads>(*this);
	            auto const threadElemExtent = getWorkDiv<Thread, Elems>(*this);

	            // Get the size of the block shared dynamic memory.
	            auto const smBytes = getBlockSharedMemDynSizeBytes<AccCpuThreads<TDim, TIdx>>(
	                m_kernelFnObj,
	                blockThreadExtent,
	                threadElemExtent,
	                args...);
	#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
	            std::cout << __func__ << " smBytes: " << smBytes << " B" << std::endl;
	#    endif
	            AccCpuThreads<TDim, TIdx> acc(*static_cast<WorkDivMembers<TDim, TIdx> const*>(this), smBytes);

	            auto const threadsPerBlock = blockThreadExtent.prod();
	            ThreadPool threadPool(static_cast<std::size_t>(threadsPerBlock));

	            // Execute the blocks serially.
	            meta::ndLoopIncIdx(
	                gridBlockExtent,
	                [&](Vec<TDim, TIdx> const& gridBlockIdx)
	                { runBlock(acc, gridBlockIdx, blockThreadExtent, threadPool, m_kernelFnObj, args...); });
	        }

	        //! The function executed for each grid block.
	        ALPAKA_FN_HOST static auto runBlock(
	            AccCpuThreads<TDim, TIdx>& acc,
	            Vec<TDim, TIdx> const& gridBlockIdx,
	            Vec<TDim, TIdx> const& blockThreadExtent,
	            ThreadPool& threadPool,
	            TKernelFnObj const& kernelFnObj,
	            std::decay_t<TArgs> const&... args) -> void
	        {
	            std::vector<std::future<void>> futuresInBlock;
	            acc.m_gridBlockIdx = gridBlockIdx;

	            // Execute the threads of this block in parallel.
	            meta::ndLoopIncIdx(
	                blockThreadExtent,
	                [&](Vec<TDim, TIdx> const& blockThreadIdx)
	                {
	                    // copy blockThreadIdx because it will get changed for the next iteration/thread.
	                    futuresInBlock.emplace_back(threadPool.enqueueTask(
	                        [&, blockThreadIdx] { runThread(acc, blockThreadIdx, kernelFnObj, args...); }));
	                });

	            // Wait for the completion of the block thread kernels.
	            for(auto& t : futuresInBlock)
	                t.wait();

	            // Clean up.
	            futuresInBlock.clear();
	            acc.m_threadToIndexMap.clear();
	            freeSharedVars(acc); // After a block has been processed, the shared memory has to be deleted.
	        }

	        //! The thread entry point on the accelerator.
	        ALPAKA_FN_HOST static auto runThread(
	            AccCpuThreads<TDim, TIdx>& acc,
	            Vec<TDim, TIdx> const& blockThreadIdx,
	            TKernelFnObj const& kernelFnObj,
	            std::decay_t<TArgs> const&... args) -> void
	        {
	            // We have to store the thread data before the kernel is calling any of the methods of this class depending
	            // on them.
	            auto const threadId = std::this_thread::get_id();

	            if(blockThreadIdx.sum() == 0)
	            {
	                acc.m_idMasterThread = threadId;
	            }

	            {
	                // Save the thread id, and index.
	                std::lock_guard<std::mutex> lock(acc.m_mtxMapInsert);
	                acc.m_threadToIndexMap.emplace(threadId, blockThreadIdx);
	            }

	            // Sync all threads so that the maps with thread id's are complete and not changed after here.
	            syncBlockThreads(acc);

	            // Execute the kernel itself.
	            kernelFnObj(std::as_const(acc), args...);

	            // We have to sync all threads here because if a thread would finish before all threads have been started,
	            // a new thread could get the recycled (then duplicate) thread id!
	            syncBlockThreads(acc);
	        }

	        TKernelFnObj m_kernelFnObj;
	        std::tuple<std::decay_t<TArgs>...> m_args;
	    };

	    namespace trait
	    {
	        //! The CPU threads execution task accelerator type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct AccType<TaskKernelCpuThreads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = AccCpuThreads<TDim, TIdx>;
	        };

	        //! The CPU threads execution task device type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DevType<TaskKernelCpuThreads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = DevCpu;
	        };

	        //! The CPU threads execution task dimension getter trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct DimType<TaskKernelCpuThreads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TDim;
	        };

	        //! The CPU threads execution task platform type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct PltfType<TaskKernelCpuThreads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = PltfCpu;
	        };

	        //! The CPU threads execution task idx type trait specialization.
	        template<typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	        struct IdxType<TaskKernelCpuThreads<TDim, TIdx, TKernelFnObj, TArgs...>>
	        {
	            using type = TIdx;
	        };
	    } // namespace trait
	} // namespace alpaka

	#endif
	// ==
	// == ./include/alpaka/kernel/TaskKernelCpuThreads.hpp ==
	// ============================================================================

// #include "alpaka/kernel/TaskKernelFpgaSyclIntel.hpp"    // amalgamate: file already expanded
// #include "alpaka/kernel/TaskKernelGenericSycl.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/kernel/TaskKernelGpuCudaRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiCudaRt.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/kernel/TaskKernelGpuUniformCudaHipRt.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Erik Zenker, Matthias Werner, René Widera, Jan Stephan, Andrea Bocci, Bernhard
		 * Manfred Gruber, Antonio Di Pilato
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/acc/AccGpuUniformCudaHipRt.hpp"    // amalgamate: file already expanded
		// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Decay.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/DemangleTypeNames.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/RemoveRestrict.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/queue/QueueUniformCudaHipRtBlocking.hpp"    // amalgamate: file already expanded
		// #include "alpaka/queue/QueueUniformCudaHipRtNonBlocking.hpp"    // amalgamate: file already expanded
		// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/workdiv/WorkDivHelpers.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Matthias Werner, Jan Stephan, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/acc/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Utility.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded
			// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

			// #include <algorithm>    // amalgamate: file already included
			// #include <array>    // amalgamate: file already included
			// #include <cmath>    // amalgamate: file already included
			// #include <functional>    // amalgamate: file already included
			#include <set>
			// #include <type_traits>    // amalgamate: file already included

			//! The alpaka library.
			namespace alpaka
			{
			    //! The grid block extent subdivision restrictions.
			    enum class GridBlockExtentSubDivRestrictions
			    {
			        EqualExtent, //!< The block thread extent will be equal in all dimensions.
			        CloseToEqualExtent, //!< The block thread extent will be as close to equal as possible in all dimensions.
			        Unrestricted, //!< The block thread extent will not have any restrictions.
			    };

			    namespace detail
			    {
			        //! Finds the largest divisor where divident % divisor == 0
			        //! \param dividend The dividend.
			        //! \param maxDivisor The maximum divisor.
			        //! \return The biggest number that satisfies the following conditions:
			        //!     1) dividend%ret==0
			        //!     2) ret<=maxDivisor
			        template<typename T, typename = std::enable_if_t<std::is_integral_v<T>>>
			        ALPAKA_FN_HOST auto nextDivisorLowerOrEqual(T const& dividend, T const& maxDivisor) -> T
			        {
			            core::assertValueUnsigned(dividend);
			            core::assertValueUnsigned(maxDivisor);
			            ALPAKA_ASSERT(dividend >= maxDivisor);

			            T divisor = maxDivisor;
			            while(dividend % divisor != 0)
			                --divisor;
			            return divisor;
			        }
			        //! \param val The value to find divisors of.
			        //! \param maxDivisor The maximum.
			        //! \return A list of all divisors less then or equal to the given maximum.
			        template<typename T, typename = std::enable_if_t<std::is_integral_v<T>>>
			        ALPAKA_FN_HOST auto allDivisorsLessOrEqual(T const& val, T const& maxDivisor) -> std::set<T>
			        {
			            std::set<T> divisorSet;

			            core::assertValueUnsigned(val);
			            core::assertValueUnsigned(maxDivisor);
			            ALPAKA_ASSERT(maxDivisor <= val);

			            for(T i(1); i <= std::min(val, maxDivisor); ++i)
			            {
			                if(val % i == 0)
			                {
			                    divisorSet.insert(static_cast<T>(val / i));
			                }
			            }

			            return divisorSet;
			        }
			    } // namespace detail

			    //! \tparam TDim The dimensionality of the accelerator device properties.
			    //! \tparam TIdx The idx type of the accelerator device properties.
			    //! \param accDevProps The maxima for the work division.
			    //! \return If the accelerator device properties are valid.
			    template<typename TDim, typename TIdx>
			    ALPAKA_FN_HOST auto isValidAccDevProps(AccDevProps<TDim, TIdx> const& accDevProps) -> bool
			    {
			        // Check that the maximum counts are greater or equal 1.
			        if((accDevProps.m_gridBlockCountMax < 1) || (accDevProps.m_blockThreadCountMax < 1)
			           || (accDevProps.m_threadElemCountMax < 1))
			        {
			            return false;
			        }

			        // Store the maxima allowed for extents of grid, blocks and threads.
			        auto const gridBlockExtentMax = subVecEnd<TDim>(accDevProps.m_gridBlockExtentMax);
			        auto const blockThreadExtentMax = subVecEnd<TDim>(accDevProps.m_blockThreadExtentMax);
			        auto const threadElemExtentMax = subVecEnd<TDim>(accDevProps.m_threadElemExtentMax);

			        // Check that the extents for all dimensions are correct.
			        for(typename TDim::value_type i(0); i < TDim::value; ++i)
			        {
			            // Check that the maximum extents are greater or equal 1.
			            if((gridBlockExtentMax[i] < 1) || (blockThreadExtentMax[i] < 1) || (threadElemExtentMax[i] < 1))
			            {
			                return false;
			            }
			        }

			        return true;
			    }

			    //! Subdivides the given grid thread extent into blocks restricted by the maxima allowed.
			    //! 1. The the maxima block, thread and element extent and counts
			    //! 2. The requirement of the block thread extent to divide the grid thread extent without remainder
			    //! 3. The requirement of the block extent.
			    //!
			    //! \param gridElemExtent
			    //!     The full extent of elements in the grid.
			    //! \param threadElemExtent
			    //!     the number of elements computed per thread.
			    //! \param accDevProps
			    //!     The maxima for the work division.
			    //! \param blockThreadMustDivideGridThreadExtent
			    //!     If this is true, the grid thread extent will be multiples of the corresponding block thread extent.
			    //!     NOTE: If this is true and gridThreadExtent is prime (or otherwise bad chosen) in a dimension, the block
			    //!     thread extent will be one in this dimension.
			    //! \param gridBlockExtentSubDivRestrictions
			    //!     The grid block extent subdivision restrictions.
			    template<typename TDim, typename TIdx>
			    ALPAKA_FN_HOST auto subDivideGridElems(
			        Vec<TDim, TIdx> const& gridElemExtent,
			        Vec<TDim, TIdx> const& threadElemExtent,
			        AccDevProps<TDim, TIdx> const& accDevProps,
			        bool blockThreadMustDivideGridThreadExtent = true,
			        GridBlockExtentSubDivRestrictions gridBlockExtentSubDivRestrictions
			        = GridBlockExtentSubDivRestrictions::Unrestricted) -> WorkDivMembers<TDim, TIdx>
			    {
			        using Vec = Vec<TDim, TIdx>;
			        using DimLoopInd = typename TDim::value_type;

			        for(DimLoopInd i(0); i < TDim::value; ++i)
			        {
			            ALPAKA_ASSERT(gridElemExtent[i] >= 1);
			            ALPAKA_ASSERT(threadElemExtent[i] >= 1);
			            ALPAKA_ASSERT(threadElemExtent[i] <= accDevProps.m_threadElemExtentMax[i]);
			        }
			        ALPAKA_ASSERT(threadElemExtent.prod() <= accDevProps.m_threadElemCountMax);
			        ALPAKA_ASSERT(isValidAccDevProps(accDevProps));

			        // Handle threadElemExtent and compute gridThreadExtent. Afterwards, only the blockThreadExtent has to be
			        // optimized.
			        auto const clippedThreadElemExtent = elementwise_min(threadElemExtent, gridElemExtent);
			        auto const gridThreadExtent = [&]
			        {
			            Vec r;
			            for(DimLoopInd i(0u); i < TDim::value; ++i)
			                r[i] = core::divCeil(gridElemExtent[i], clippedThreadElemExtent[i]);
			            return r;
			        }();

			        ///////////////////////////////////////////////////////////////////
			        // Try to calculate an optimal blockThreadExtent.

			        // Restrict the max block thread extent from the maximum possible to the grid thread extent.
			        // This removes dimensions not required in the grid thread extent.
			        // This has to be done before the blockThreadCountMax clipping to get the maximum correctly.
			        auto blockThreadExtent = elementwise_min(accDevProps.m_blockThreadExtentMax, gridThreadExtent);

			        // For equal block thread extent, restrict it to its minimum component.
			        // For example (512, 256, 1024) will get (256, 256, 256).
			        if(gridBlockExtentSubDivRestrictions == GridBlockExtentSubDivRestrictions::EqualExtent)
			            blockThreadExtent = Vec::all(blockThreadExtent.min());

			        // Make the blockThreadExtent product smaller or equal to the accelerator's limit.
			        auto const& blockThreadCountMax = accDevProps.m_blockThreadCountMax;
			        if(blockThreadExtent.prod() > blockThreadCountMax)
			        {
			            switch(gridBlockExtentSubDivRestrictions)
			            {
			            case GridBlockExtentSubDivRestrictions::EqualExtent:
			                blockThreadExtent = Vec::all(core::nthRootFloor(blockThreadCountMax, TIdx{TDim::value}));
			                break;
			            case GridBlockExtentSubDivRestrictions::CloseToEqualExtent:
			                // Very primitive clipping. Just halve the largest value until it fits.
			                while(blockThreadExtent.prod() > blockThreadCountMax)
			                    blockThreadExtent[blockThreadExtent.maxElem()] /= TIdx{2};
			                break;
			            case GridBlockExtentSubDivRestrictions::Unrestricted:
			                // Very primitive clipping. Just halve the smallest value (which is not 1) until it fits.
			                while(blockThreadExtent.prod() > blockThreadCountMax)
			                {
			                    auto const it = std::min_element(
			                        blockThreadExtent.begin(),
			                        blockThreadExtent.end() - 1, //! \todo why omit the last element?
			                        [](TIdx const& a, TIdx const& b)
			                        {
			                            if(a == TIdx{1})
			                                return false;
			                            if(b == TIdx{1})
			                                return true;
			                            return a < b;
			                        });
			                    *it /= TIdx{2};
			                }
			                break;
			            }
			        }

			        // Make the block thread extent divide the grid thread extent.
			        if(blockThreadMustDivideGridThreadExtent)
			        {
			            switch(gridBlockExtentSubDivRestrictions)
			            {
			            case GridBlockExtentSubDivRestrictions::EqualExtent:
			                {
			                    // For equal size block extent we have to compute the gcd of all grid thread extent that is less
			                    // then the current maximal block thread extent. For this we compute the divisors of all grid
			                    // thread extent less then the current maximal block thread extent.
			                    std::array<std::set<TIdx>, TDim::value> gridThreadExtentDivisors;
			                    for(DimLoopInd i(0u); i < TDim::value; ++i)
			                    {
			                        gridThreadExtentDivisors[i]
			                            = detail::allDivisorsLessOrEqual(gridThreadExtent[i], blockThreadExtent[i]);
			                    }
			                    // The maximal common divisor of all block thread extent is the optimal solution.
			                    std::set<TIdx> intersects[2u];
			                    for(DimLoopInd i(1u); i < TDim::value; ++i)
			                    {
			                        intersects[(i - 1u) % 2u] = gridThreadExtentDivisors[0];
			                        intersects[(i) % 2u].clear();
			                        set_intersection(
			                            std::begin(intersects[(i - 1u) % 2u]),
			                            std::end(intersects[(i - 1u) % 2u]),
			                            std::begin(gridThreadExtentDivisors[i]),
			                            std::end(gridThreadExtentDivisors[i]),
			                            std::inserter(intersects[i % 2], std::begin(intersects[i % 2u])));
			                    }
			                    TIdx const maxCommonDivisor = *(--std::end(intersects[(TDim::value - 1) % 2u]));
			                    blockThreadExtent = Vec::all(maxCommonDivisor);
			                    break;
			                }
			            case GridBlockExtentSubDivRestrictions::CloseToEqualExtent:
			                [[fallthrough]];
			            case GridBlockExtentSubDivRestrictions::Unrestricted:
			                for(DimLoopInd i(0u); i < TDim::value; ++i)
			                    blockThreadExtent[i] = detail::nextDivisorLowerOrEqual(gridThreadExtent[i], blockThreadExtent[i]);
			                break;
			            }
			        }

			        // grid blocks extent = grid thread / block thread extent. quotient is rounded up.
			        auto const gridBlockExtent = [&]
			        {
			            Vec r;
			            for(DimLoopInd i = 0; i < TDim::value; ++i)
			                r[i] = core::divCeil(gridThreadExtent[i], blockThreadExtent[i]);
			            return r;
			        }();

			        return WorkDivMembers<TDim, TIdx>(gridBlockExtent, blockThreadExtent, clippedThreadElemExtent);
			    }

			    //! \tparam TAcc The accelerator for which this work division has to be valid.
			    //! \tparam TGridElemExtent The type of the grid element extent.
			    //! \tparam TThreadElemExtent The type of the thread element extent.
			    //! \tparam TDev The type of the device.
			    //! \param dev
			    //!     The device the work division should be valid for.
			    //! \param gridElemExtent
			    //!     The full extent of elements in the grid.
			    //! \param threadElemExtents
			    //!     the number of elements computed per thread.
			    //! \param blockThreadMustDivideGridThreadExtent
			    //!     If this is true, the grid thread extent will be multiples of the corresponding block thread extent.
			    //!     NOTE: If this is true and gridThreadExtent is prime (or otherwise bad chosen) in a dimension, the block
			    //!     thread extent will be one in this dimension.
			    //! \param gridBlockExtentSubDivRestrictions
			    //!     The grid block extent subdivision restrictions.
			    //! \return The work division.
			    template<
			        typename TAcc,
			        typename TDev,
			        typename TGridElemExtent = Vec<Dim<TAcc>, Idx<TAcc>>,
			        typename TThreadElemExtent = Vec<Dim<TAcc>, Idx<TAcc>>>
			    ALPAKA_FN_HOST auto getValidWorkDiv(
			        [[maybe_unused]] TDev const& dev,
			        [[maybe_unused]] TGridElemExtent const& gridElemExtent = Vec<Dim<TAcc>, Idx<TAcc>>::ones(),
			        [[maybe_unused]] TThreadElemExtent const& threadElemExtents = Vec<Dim<TAcc>, Idx<TAcc>>::ones(),
			        [[maybe_unused]] bool blockThreadMustDivideGridThreadExtent = true,
			        [[maybe_unused]] GridBlockExtentSubDivRestrictions gridBlockExtentSubDivRestrictions
			        = GridBlockExtentSubDivRestrictions::Unrestricted)
			        -> WorkDivMembers<Dim<TGridElemExtent>, Idx<TGridElemExtent>>
			    {
			        static_assert(
			            Dim<TGridElemExtent>::value == Dim<TAcc>::value,
			            "The dimension of TAcc and the dimension of TGridElemExtent have to be identical!");
			        static_assert(
			            Dim<TThreadElemExtent>::value == Dim<TAcc>::value,
			            "The dimension of TAcc and the dimension of TThreadElemExtent have to be identical!");
			        static_assert(
			            std::is_same_v<Idx<TGridElemExtent>, Idx<TAcc>>,
			            "The idx type of TAcc and the idx type of TGridElemExtent have to be identical!");
			        static_assert(
			            std::is_same_v<Idx<TThreadElemExtent>, Idx<TAcc>>,
			            "The idx type of TAcc and the idx type of TThreadElemExtent have to be identical!");

			        if constexpr(Dim<TGridElemExtent>::value == 0)
			        {
			            auto const zero = Vec<DimInt<0>, Idx<TAcc>>{};
			            ALPAKA_ASSERT(gridElemExtent == zero);
			            ALPAKA_ASSERT(threadElemExtents == zero);
			            return WorkDivMembers<DimInt<0>, Idx<TAcc>>{zero, zero, zero};
			        }
			        else
			            return subDivideGridElems(
			                getExtentVec(gridElemExtent),
			                getExtentVec(threadElemExtents),
			                getAccDevProps<TAcc>(dev),
			                blockThreadMustDivideGridThreadExtent,
			                gridBlockExtentSubDivRestrictions);
			        using V [[maybe_unused]] = Vec<Dim<TGridElemExtent>, Idx<TGridElemExtent>>;
			        ALPAKA_UNREACHABLE(WorkDivMembers<Dim<TGridElemExtent>, Idx<TGridElemExtent>>{V{}, V{}, V{}});
			    }

			    //! \tparam TDim The dimensionality of the accelerator device properties.
			    //! \tparam TIdx The idx type of the accelerator device properties.
			    //! \tparam TWorkDiv The type of the work division.
			    //! \param accDevProps The maxima for the work division.
			    //! \param workDiv The work division to test for validity.
			    //! \return If the work division is valid for the given accelerator device properties.
			    template<typename TDim, typename TIdx, typename TWorkDiv>
			    ALPAKA_FN_HOST auto isValidWorkDiv(AccDevProps<TDim, TIdx> const& accDevProps, TWorkDiv const& workDiv) -> bool
			    {
			        // Get the extents of grid, blocks and threads of the work division to check.
			        auto const gridBlockExtent = getWorkDiv<Grid, Blocks>(workDiv);
			        auto const blockThreadExtent = getWorkDiv<Block, Threads>(workDiv);
			        auto const threadElemExtent = getWorkDiv<Block, Threads>(workDiv);

			        // Check that the maximal counts are satisfied.
			        if(accDevProps.m_gridBlockCountMax < gridBlockExtent.prod())
			        {
			            return false;
			        }
			        if(accDevProps.m_blockThreadCountMax < blockThreadExtent.prod())
			        {
			            return false;
			        }
			        if(accDevProps.m_threadElemCountMax < threadElemExtent.prod())
			        {
			            return false;
			        }

			        // Check that the extents for all dimensions are correct.
			        if constexpr(Dim<TWorkDiv>::value > 0)
			        {
			            // Store the maxima allowed for extents of grid, blocks and threads.
			            auto const gridBlockExtentMax = subVecEnd<Dim<TWorkDiv>>(accDevProps.m_gridBlockExtentMax);
			            auto const blockThreadExtentMax = subVecEnd<Dim<TWorkDiv>>(accDevProps.m_blockThreadExtentMax);
			            auto const threadElemExtentMax = subVecEnd<Dim<TWorkDiv>>(accDevProps.m_threadElemExtentMax);

			            for(typename Dim<TWorkDiv>::value_type i(0); i < Dim<TWorkDiv>::value; ++i)
			            {
			                // No extent is allowed to be zero or greater then the allowed maximum.
			                if((gridBlockExtent[i] < 1) || (blockThreadExtent[i] < 1) || (threadElemExtent[i] < 1)
			                   || (gridBlockExtentMax[i] < gridBlockExtent[i]) || (blockThreadExtentMax[i] < blockThreadExtent[i])
			                   || (threadElemExtentMax[i] < threadElemExtent[i]))
			                {
			                    return false;
			                }
			            }
			        }

			        return true;
			    }
			    //! \tparam TAcc The accelerator to test the validity on.
			    //! \param dev The device to test the work division for validity on.
			    //! \param workDiv The work division to test for validity.
			    //! \return If the work division is valid on this accelerator.
			    template<typename TAcc, typename TDev, typename TWorkDiv>
			    ALPAKA_FN_HOST auto isValidWorkDiv(TDev const& dev, TWorkDiv const& workDiv) -> bool
			    {
			        return isValidWorkDiv(getAccDevProps<TAcc>(dev), workDiv);
			    }
			} // namespace alpaka
			// ==
			// == ./include/alpaka/workdiv/WorkDivHelpers.hpp ==
			// ============================================================================

		// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded

		// #include <stdexcept>    // amalgamate: file already included
		// #include <tuple>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included
		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
		// #    include <iostream>    // amalgamate: file already included
		#endif

		#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

		#    if !defined(ALPAKA_HOST_ONLY)

		// #        include "alpaka/core/BoostPredef.hpp"    // amalgamate: file already expanded

		#        if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) && !BOOST_LANG_CUDA
		#            error If ALPAKA_ACC_GPU_CUDA_ENABLED is set, the compiler has to support CUDA!
		#        endif

		#        if defined(ALPAKA_ACC_GPU_HIP_ENABLED) && !BOOST_LANG_HIP
		#            error If ALPAKA_ACC_GPU_HIP_ENABLED is set, the compiler has to support HIP!
		#        endif

		namespace alpaka
		{
		    namespace detail
		    {
		        //! The GPU CUDA/HIP kernel entry point.
		        // \NOTE: 'A __global__ function or function template cannot have a trailing return type.'
		        // We have put the function into a shallow namespace and gave it a short name, so the mangled name in the
		        // profiler (e.g. ncu) is as shorter as possible.
		        template<typename TKernelFnObj, typename TApi, typename TAcc, typename TDim, typename TIdx, typename... TArgs>
		        __global__ void gpuKernel(
		            Vec<TDim, TIdx> const threadElemExtent,
		            TKernelFnObj const kernelFnObj,
		            TArgs... args)
		        {
		#        if BOOST_ARCH_PTX && (BOOST_ARCH_PTX < BOOST_VERSION_NUMBER(2, 0, 0))
		#            error "Device capability >= 2.0 is required!"
		#        endif

		            const TAcc acc(threadElemExtent);

		// with clang it is not possible to query std::result_of for a pure device lambda created on the host side
		#        if !(BOOST_COMP_CLANG_CUDA && BOOST_COMP_CLANG)
		            static_assert(
		                std::is_same_v<decltype(kernelFnObj(const_cast<TAcc const&>(acc), args...)), void>,
		                "The TKernelFnObj is required to return void!");
		#        endif
		            kernelFnObj(const_cast<TAcc const&>(acc), args...);
		        }
		    } // namespace detail

		    namespace uniform_cuda_hip
		    {
		        namespace detail
		        {
		            template<typename TDim, typename TIdx>
		            ALPAKA_FN_HOST auto checkVecOnly3Dim(Vec<TDim, TIdx> const& vec) -> void
		            {
		                if constexpr(TDim::value > 0)
		                {
		                    for(auto i = std::min(typename TDim::value_type{3}, TDim::value); i < TDim::value; ++i)
		                    {
		                        if(vec[TDim::value - 1u - i] != 1)
		                        {
		                            throw std::runtime_error(
		                                "The CUDA/HIP accelerator supports a maximum of 3 dimensions. All "
		                                "work division extents of the dimensions higher 3 have to be 1!");
		                        }
		                    }
		                }
		            }

		            template<typename TDim, typename TIdx>
		            ALPAKA_FN_HOST auto convertVecToUniformCudaHipDim(Vec<TDim, TIdx> const& vec) -> dim3
		            {
		                dim3 dim(1, 1, 1);
		                if constexpr(TDim::value >= 1)
		                    dim.x = static_cast<unsigned>(vec[TDim::value - 1u]);
		                if constexpr(TDim::value >= 2)
		                    dim.y = static_cast<unsigned>(vec[TDim::value - 2u]);
		                if constexpr(TDim::value >= 3)
		                    dim.z = static_cast<unsigned>(vec[TDim::value - 3u]);
		                checkVecOnly3Dim(vec);
		                return dim;
		            }
		        } // namespace detail
		    } // namespace uniform_cuda_hip

		    //! The GPU CUDA/HIP accelerator execution task.
		    template<typename TApi, typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		    class TaskKernelGpuUniformCudaHipRt final : public WorkDivMembers<TDim, TIdx>
		    {
		    public:
		        template<typename TWorkDiv>
		        ALPAKA_FN_HOST TaskKernelGpuUniformCudaHipRt(
		            TWorkDiv&& workDiv,
		            TKernelFnObj const& kernelFnObj,
		            TArgs&&... args)
		            : WorkDivMembers<TDim, TIdx>(std::forward<TWorkDiv>(workDiv))
		            , m_kernelFnObj(kernelFnObj)
		            , m_args(std::forward<TArgs>(args)...)
		        {
		            static_assert(
		                Dim<std::decay_t<TWorkDiv>>::value == TDim::value,
		                "The work division and the execution task have to be of the same dimensionality!");
		        }

		        TKernelFnObj m_kernelFnObj;
		        std::tuple<remove_restrict_t<std::decay_t<TArgs>>...> m_args;
		    };

		    namespace trait
		    {
		        //! The GPU CUDA/HIP execution task accelerator type trait specialization.
		        template<typename TApi, typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		        struct AccType<TaskKernelGpuUniformCudaHipRt<TApi, TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
		        {
		            using type = AccGpuUniformCudaHipRt<TApi, TDim, TIdx>;
		        };

		        //! The GPU CUDA/HIP execution task device type trait specialization.
		        template<typename TApi, typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		        struct DevType<TaskKernelGpuUniformCudaHipRt<TApi, TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
		        {
		            using type = DevUniformCudaHipRt<TApi>;
		        };

		        //! The GPU CUDA/HIP execution task dimension getter trait specialization.
		        template<typename TApi, typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		        struct DimType<TaskKernelGpuUniformCudaHipRt<TApi, TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
		        {
		            using type = TDim;
		        };

		        //! The CPU CUDA/HIP execution task platform type trait specialization.
		        template<typename TApi, typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		        struct PltfType<TaskKernelGpuUniformCudaHipRt<TApi, TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
		        {
		            using type = PltfUniformCudaHipRt<TApi>;
		        };

		        //! The GPU CUDA/HIP execution task idx type trait specialization.
		        template<typename TApi, typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		        struct IdxType<TaskKernelGpuUniformCudaHipRt<TApi, TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
		        {
		            using type = TIdx;
		        };

		        //! The CUDA/HIP non-blocking kernel enqueue trait specialization.
		        template<typename TApi, typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		        struct Enqueue<
		            QueueUniformCudaHipRtNonBlocking<TApi>,
		            TaskKernelGpuUniformCudaHipRt<TApi, TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
		        {
		            ALPAKA_FN_HOST static auto enqueue(
		                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
		                TaskKernelGpuUniformCudaHipRt<TApi, TAcc, TDim, TIdx, TKernelFnObj, TArgs...> const& task) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;
		                // TODO: Check that (sizeof(TKernelFnObj) * m_3uiBlockThreadExtent.prod()) < available memory idx

		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                // std::size_t printfFifoSize;
		                // TApi::deviceGetLimit(&printfFifoSize, TApi::limitPrintfFifoSize);
		                // std::cout << __func__ << "INFO: printfFifoSize: " << printfFifoSize << std::endl;
		                // TApi::deviceSetLimit(TApi::limitPrintfFifoSize, printfFifoSize*10);
		                // TApi::deviceGetLimit(&printfFifoSize, TApi::limitPrintfFifoSize);
		                // std::cout << __func__ << "INFO: printfFifoSize: " << printfFifoSize << std::endl;
		#        endif
		                auto const gridBlockExtent = getWorkDiv<Grid, Blocks>(task);
		                auto const blockThreadExtent = getWorkDiv<Block, Threads>(task);
		                auto const threadElemExtent = getWorkDiv<Thread, Elems>(task);

		                dim3 const gridDim = uniform_cuda_hip::detail::convertVecToUniformCudaHipDim(gridBlockExtent);
		                dim3 const blockDim = uniform_cuda_hip::detail::convertVecToUniformCudaHipDim(blockThreadExtent);
		                uniform_cuda_hip::detail::checkVecOnly3Dim(threadElemExtent);

		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                std::cout << __func__ << " gridDim: " << gridDim.z << " " << gridDim.y << " " << gridDim.x
		                          << " blockDim: " << blockDim.z << " " << blockDim.y << " " << blockDim.x << std::endl;
		#        endif

		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
		                // This checks for a valid work division that is also compliant with the maxima of the accelerator.
		                if(!isValidWorkDiv<TAcc>(getDev(queue), task))
		                {
		                    throw std::runtime_error(
		                        "The given work division is not valid or not supported by the device of type "
		                        + getAccName<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>() + "!");
		                }
		#        endif

		                // Get the size of the block shared dynamic memory.
		                auto const blockSharedMemDynSizeBytes = std::apply(
		                    [&](remove_restrict_t<ALPAKA_DECAY_T(TArgs)> const&... args) {
		                        return getBlockSharedMemDynSizeBytes<TAcc>(
		                            task.m_kernelFnObj,
		                            blockThreadExtent,
		                            threadElemExtent,
		                            args...);
		                    },
		                    task.m_args);

		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                // Log the block shared memory idx.
		                std::cout << __func__ << " BlockSharedMemDynSizeBytes: " << blockSharedMemDynSizeBytes << " B"
		                          << std::endl;
		#        endif
		                auto kernelName = alpaka::detail::
		                    gpuKernel<TKernelFnObj, TApi, TAcc, TDim, TIdx, remove_restrict_t<std::decay_t<TArgs>>...>;

		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                // Log the function attributes.
		                typename TApi::FuncAttributes_t funcAttrs;
		                TApi::funcGetAttributes(&funcAttrs, kernelName);
		                std::cout << __func__ << " binaryVersion: " << funcAttrs.binaryVersion
		                          << " constSizeBytes: " << funcAttrs.constSizeBytes << " B"
		                          << " localSizeBytes: " << funcAttrs.localSizeBytes << " B"
		                          << " maxThreadsPerBlock: " << funcAttrs.maxThreadsPerBlock
		                          << " numRegs: " << funcAttrs.numRegs << " ptxVersion: " << funcAttrs.ptxVersion
		                          << " sharedSizeBytes: " << funcAttrs.sharedSizeBytes << " B" << std::endl;
		#        endif

		                // Set the current device.
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(queue.m_spQueueImpl->m_dev.getNativeHandle()));
		                // Enqueue the kernel execution.
		                // \NOTE: No const reference (const &) is allowed as the parameter type because the kernel launch
		                // language extension expects the arguments by value. This forces the type of a float argument given
		                // with std::forward to this function to be of type float instead of e.g. "float const & __ptr64"
		                // (MSVC). If not given by value, the kernel launch code does not copy the value but the pointer to the
		                // value location.
		                std::apply(
		                    [&](remove_restrict_t<ALPAKA_DECAY_T(TArgs)> const&... args)
		                    {
		                        kernelName<<<
		                            gridDim,
		                            blockDim,
		                            static_cast<std::size_t>(blockSharedMemDynSizeBytes),
		                            queue.getNativeHandle()>>>(threadElemExtent, task.m_kernelFnObj, args...);
		                    },
		                    task.m_args);

		                if constexpr(ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL)
		                {
		                    // Wait for the kernel execution to finish but do not check error return of this call.
		                    // Do not use the alpaka::wait method because it checks the error itself but we want to give a
		                    // custom error message.
		                    std::ignore = TApi::streamSynchronize(queue.getNativeHandle());
		                    auto const msg = std::string{
		                        "'execution of kernel: '" + std::string{core::demangled<TKernelFnObj>} + "' failed with"};
		                    ::alpaka::uniform_cuda_hip::detail::rtCheckLastError<TApi, true>(msg.c_str(), __FILE__, __LINE__);
		                }
		            }
		        };

		        //! The CUDA/HIP synchronous kernel enqueue trait specialization.
		        template<typename TApi, typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
		        struct Enqueue<
		            QueueUniformCudaHipRtBlocking<TApi>,
		            TaskKernelGpuUniformCudaHipRt<TApi, TAcc, TDim, TIdx, TKernelFnObj, TArgs...>>
		        {
		            ALPAKA_FN_HOST static auto enqueue(
		                QueueUniformCudaHipRtBlocking<TApi>& queue,
		                TaskKernelGpuUniformCudaHipRt<TApi, TAcc, TDim, TIdx, TKernelFnObj, TArgs...> const& task) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;
		                // TODO: Check that (sizeof(TKernelFnObj) * m_3uiBlockThreadExtent.prod()) < available memory idx

		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                // std::size_t printfFifoSize;
		                // TApi::deviceGetLimit(&printfFifoSize, TApi::limitPrintfFifoSize);
		                // std::cout << __func__ << "INFO: printfFifoSize: " << printfFifoSize << std::endl;
		                // TApi::deviceSetLimit(TApi::limitPrintfFifoSize, printfFifoSize*10);
		                // TApi::deviceGetLimit(&printfFifoSize, TApi::limitPrintfFifoSize);
		                // std::cout << __func__ << "INFO: printfFifoSize: " << printfFifoSize << std::endl;
		#        endif
		                auto const gridBlockExtent = getWorkDiv<Grid, Blocks>(task);
		                auto const blockThreadExtent = getWorkDiv<Block, Threads>(task);
		                auto const threadElemExtent = getWorkDiv<Thread, Elems>(task);

		                dim3 const gridDim = uniform_cuda_hip::detail::convertVecToUniformCudaHipDim(gridBlockExtent);
		                dim3 const blockDim = uniform_cuda_hip::detail::convertVecToUniformCudaHipDim(blockThreadExtent);
		                uniform_cuda_hip::detail::checkVecOnly3Dim(threadElemExtent);

		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                std::cout << __func__ << "gridDim: " << gridDim.z << " " << gridDim.y << " " << gridDim.x << std::endl;
		                std::cout << __func__ << "blockDim: " << blockDim.z << " " << blockDim.y << " " << blockDim.x
		                          << std::endl;
		#        endif

		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
		                // This checks for a valid work division that is also compliant with the maxima of the accelerator.
		                if(!isValidWorkDiv<TAcc>(getDev(queue), task))
		                {
		                    throw std::runtime_error(
		                        "The given work division is not valid or not supported by the device of type "
		                        + getAccName<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>>() + "!");
		                }
		#        endif

		                // Get the size of the block shared dynamic memory.
		                auto const blockSharedMemDynSizeBytes = std::apply(
		                    [&](remove_restrict_t<ALPAKA_DECAY_T(TArgs)> const&... args) {
		                        return getBlockSharedMemDynSizeBytes<TAcc>(
		                            task.m_kernelFnObj,
		                            blockThreadExtent,
		                            threadElemExtent,
		                            args...);
		                    },
		                    task.m_args);

		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                // Log the block shared memory idx.
		                std::cout << __func__ << " BlockSharedMemDynSizeBytes: " << blockSharedMemDynSizeBytes << " B"
		                          << std::endl;
		#        endif

		                auto kernelName = alpaka::detail::
		                    gpuKernel<TKernelFnObj, TApi, TAcc, TDim, TIdx, remove_restrict_t<std::decay_t<TArgs>>...>;
		#        if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                // Log the function attributes.
		                typename TApi::FuncAttributes_t funcAttrs;
		                TApi::funcGetAttributes(&funcAttrs, kernelName);
		                std::cout << __func__ << " binaryVersion: " << funcAttrs.binaryVersion
		                          << " constSizeBytes: " << funcAttrs.constSizeBytes << " B"
		                          << " localSizeBytes: " << funcAttrs.localSizeBytes << " B"
		                          << " maxThreadsPerBlock: " << funcAttrs.maxThreadsPerBlock
		                          << " numRegs: " << funcAttrs.numRegs << " ptxVersion: " << funcAttrs.ptxVersion
		                          << " sharedSizeBytes: " << funcAttrs.sharedSizeBytes << " B" << std::endl;
		#        endif

		                // Set the current device.
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(queue.m_spQueueImpl->m_dev.getNativeHandle()));

		                // Enqueue the kernel execution.
		                std::apply(
		                    [&](remove_restrict_t<ALPAKA_DECAY_T(TArgs)> const&... args)
		                    {
		                        kernelName<<<
		                            gridDim,
		                            blockDim,
		                            static_cast<std::size_t>(blockSharedMemDynSizeBytes),
		                            queue.getNativeHandle()>>>(threadElemExtent, task.m_kernelFnObj, args...);
		                    },
		                    task.m_args);

		                // Wait for the kernel execution to finish but do not check error return of this call.
		                // Do not use the alpaka::wait method because it checks the error itself but we want to give a custom
		                // error message.
		                std::ignore = TApi::streamSynchronize(queue.getNativeHandle());
		                if constexpr(ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL)
		                {
		                    auto const msg
		                        = std::string{"'execution of kernel: '" + core::demangled<TKernelFnObj> + "' failed with"};
		                    ::alpaka::uniform_cuda_hip::detail::rtCheckLastError<TApi, true>(msg.c_str(), __FILE__, __LINE__);
		                }
		            }
		        };
		    } // namespace trait
		} // namespace alpaka

		#    endif

		#endif
		// ==
		// == ./include/alpaka/kernel/TaskKernelGpuUniformCudaHipRt.hpp ==
		// ============================================================================


	#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

	namespace alpaka
	{
	    template<typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    using TaskKernelGpuCudaRt = TaskKernelGpuUniformCudaHipRt<ApiCudaRt, TAcc, TDim, TIdx, TKernelFnObj, TArgs...>;
	}

	#endif // ALPAKA_ACC_GPU_CUDA_ENABLED
	// ==
	// == ./include/alpaka/kernel/TaskKernelGpuCudaRt.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/kernel/TaskKernelGpuHipRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/kernel/TaskKernelGpuUniformCudaHipRt.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_HIP_ENABLED

	namespace alpaka
	{
	    template<typename TAcc, typename TDim, typename TIdx, typename TKernelFnObj, typename... TArgs>
	    using TaskKernelGpuHipRt = TaskKernelGpuUniformCudaHipRt<ApiHipRt, TAcc, TDim, TIdx, TKernelFnObj, TArgs...>;
	}

	#endif // ALPAKA_ACC_GPU_HIP_ENABLED
	// ==
	// == ./include/alpaka/kernel/TaskKernelGpuHipRt.hpp ==
	// ============================================================================

// #include "alpaka/kernel/TaskKernelGpuSyclIntel.hpp"    // amalgamate: file already expanded
// #include "alpaka/kernel/Traits.hpp"    // amalgamate: file already expanded
// math
// #include "alpaka/math/Complex.hpp"    // amalgamate: file already expanded
// #include "alpaka/math/MathGenericSycl.hpp"    // amalgamate: file already expanded
// #include "alpaka/math/MathStdLib.hpp"    // amalgamate: file already expanded
// #include "alpaka/math/MathUniformCudaHipBuiltIn.hpp"    // amalgamate: file already expanded
// mem
	// ============================================================================
	// == ./include/alpaka/mem/alloc/AllocCpuAligned.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jan Stephan, Andrea Bocci, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/AlignedAlloc.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/cpu/SysInfo.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/mem/alloc/Traits.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Matthias Werner, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded

		namespace alpaka
		{
		    struct ConceptMemAlloc
		    {
		    };

		    //! The allocator traits.
		    namespace trait
		    {
		        //! The memory allocation trait.
		        template<typename T, typename TAlloc, typename TSfinae = void>
		        struct Malloc;

		        //! The memory free trait.
		        template<typename T, typename TAlloc, typename TSfinae = void>
		        struct Free;
		    } // namespace trait

		    //! \return The pointer to the allocated memory.
		    template<typename T, typename TAlloc>
		    ALPAKA_FN_HOST auto malloc(TAlloc const& alloc, std::size_t const& sizeElems) -> T*
		    {
		        using ImplementationBase = concepts::ImplementationBase<ConceptMemAlloc, TAlloc>;
		        return trait::Malloc<T, ImplementationBase>::malloc(alloc, sizeElems);
		    }

		    //! Frees the memory identified by the given pointer.
		    template<typename TAlloc, typename T>
		    ALPAKA_FN_HOST auto free(TAlloc const& alloc, T const* const ptr) -> void
		    {
		        using ImplementationBase = concepts::ImplementationBase<ConceptMemAlloc, TAlloc>;
		        trait::Free<T, ImplementationBase>::free(alloc, ptr);
		    }
		} // namespace alpaka
		// ==
		// == ./include/alpaka/mem/alloc/Traits.hpp ==
		// ============================================================================


	// #include <algorithm>    // amalgamate: file already included

	namespace alpaka
	{
	    //! The CPU boost aligned allocator.
	    //!
	    //! \tparam TAlignment An integral constant containing the alignment.
	    template<typename TAlignment>
	    class AllocCpuAligned : public concepts::Implements<ConceptMemAlloc, AllocCpuAligned<TAlignment>>
	    {
	    };

	    namespace trait
	    {
	        //! The CPU boost aligned allocator memory allocation trait specialization.
	        template<typename T, typename TAlignment>
	        struct Malloc<T, AllocCpuAligned<TAlignment>>
	        {
	            ALPAKA_FN_HOST static auto malloc(
	                AllocCpuAligned<TAlignment> const& /* alloc */,
	                std::size_t const& sizeElems) -> T*
	            {
	#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)
	                // For CUDA/HIP host memory must be aligned to 4 kib to pin it with `cudaHostRegister`,
	                // this was described in older programming guides but was removed later.
	                // From testing with PIConGPU and cuda-memcheck we found out that the alignment is still required.
	                //
	                // For HIP the required alignment is the size of a cache line.
	                // https://rocm-developer-tools.github.io/HIP/group__Memory.html#gab8258f051e1a1f7385f794a15300e674
	                // On most x86 systems the page size is 4KiB and on OpenPower 64KiB.
	                // Page size can be tested on the terminal with: `getconf PAGE_SIZE`
	                size_t minAlignement = std::max<size_t>(TAlignment::value, cpu::detail::getPageSize());
	#else
	                constexpr size_t minAlignement = TAlignment::value;
	#endif
	                return reinterpret_cast<T*>(core::alignedAlloc(minAlignement, sizeElems * sizeof(T)));
	            }
	        };

	        //! The CPU boost aligned allocator memory free trait specialization.
	        template<typename T, typename TAlignment>
	        struct Free<T, AllocCpuAligned<TAlignment>>
	        {
	            ALPAKA_FN_HOST static auto free(AllocCpuAligned<TAlignment> const& /* alloc */, T const* const ptr) -> void
	            {
	#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)
	                size_t minAlignement = std::max<size_t>(TAlignment::value, cpu::detail::getPageSize());
	#else
	                constexpr size_t minAlignement = TAlignment::value;
	#endif
	                core::alignedFree(minAlignement, const_cast<void*>(reinterpret_cast<void const*>(ptr)));
	            }
	        };
	    } // namespace trait
	} // namespace alpaka
	// ==
	// == ./include/alpaka/mem/alloc/AllocCpuAligned.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/mem/alloc/AllocCpuNew.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/alloc/Traits.hpp"    // amalgamate: file already expanded

	namespace alpaka
	{
	    //! The CPU new allocator.
	    class AllocCpuNew : public concepts::Implements<ConceptMemAlloc, AllocCpuNew>
	    {
	    };

	    namespace trait
	    {
	        //! The CPU new allocator memory allocation trait specialization.
	        template<typename T>
	        struct Malloc<T, AllocCpuNew>
	        {
	            ALPAKA_FN_HOST static auto malloc(AllocCpuNew const& /* alloc */, std::size_t const& sizeElems) -> T*
	            {
	                return new T[sizeElems];
	            }
	        };

	        //! The CPU new allocator memory free trait specialization.
	        template<typename T>
	        struct Free<T, AllocCpuNew>
	        {
	            ALPAKA_FN_HOST static auto free(AllocCpuNew const& /* alloc */, T const* const ptr) -> void
	            {
	                return delete[] ptr;
	            }
	        };
	    } // namespace trait
	} // namespace alpaka
	// ==
	// == ./include/alpaka/mem/alloc/AllocCpuNew.hpp ==
	// ============================================================================

// #include "alpaka/mem/alloc/Traits.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/mem/buf/BufCpu.hpp ==
	// ==
	/* Copyright 2022 Alexander Matthes, Axel Huebl, Benjamin Worpitz, Andrea Bocci, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiCudaRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/ApiHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Vectorize.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/alloc/AllocCpuAligned.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/buf/Traits.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/mem/view/ViewAccessOps.hpp ==
		// ==
		/* Copyright 2022 Andrea Bocci
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/view/ViewAccessor.hpp"    // amalgamate: file already expanded

		// #include <sstream>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included

		namespace alpaka::internal
		{
		    template<typename TView>
		    struct ViewAccessOps
		    {
		    private:
		        using value_type = Elem<TView>;
		        using pointer = value_type*;
		        using const_pointer = value_type const*;
		        using reference = value_type&;
		        using const_reference = value_type const&;

		    public:
		        ViewAccessOps()
		        {
		            static_assert(experimental::trait::internal::IsView<TView>::value);
		        }

		        ALPAKA_FN_HOST auto data() -> pointer
		        {
		            return getPtrNative(*static_cast<TView*>(this));
		        }

		        [[nodiscard]] ALPAKA_FN_HOST auto data() const -> const_pointer
		        {
		            return getPtrNative(*static_cast<TView const*>(this));
		        }

		        ALPAKA_FN_HOST auto operator*() -> reference
		        {
		            static_assert(Dim<TView>::value == 0, "operator* is only valid for Buffers and Views of dimension 0");
		            return *data();
		        }

		        ALPAKA_FN_HOST auto operator*() const -> const_reference
		        {
		            static_assert(Dim<TView>::value == 0, "operator* is only valid for Buffers and Views of dimension 0");
		            return *data();
		        }

		        ALPAKA_FN_HOST auto operator->() -> pointer
		        {
		            static_assert(Dim<TView>::value == 0, "operator-> is only valid for Buffers and Views of dimension 0");
		            return data();
		        }

		        ALPAKA_FN_HOST auto operator->() const -> const_pointer
		        {
		            static_assert(Dim<TView>::value == 0, "operator-> is only valid for Buffers and Views of dimension 0");
		            return data();
		        }

		        ALPAKA_FN_HOST auto operator[](std::size_t i) -> reference
		        {
		            static_assert(Dim<TView>::value == 1, "operator[i] is only valid for Buffers and Views of dimension 1");
		            return data()[i];
		        }

		        ALPAKA_FN_HOST auto operator[](std::size_t i) const -> const_reference
		        {
		            static_assert(Dim<TView>::value == 1, "operator[i] is only valid for Buffers and Views of dimension 1");
		            return data()[i];
		        }

		    private:
		        template<std::size_t TDim, typename TIdx>
		        [[nodiscard]] ALPAKA_FN_HOST auto ptr_at([[maybe_unused]] Vec<DimInt<TDim>, TIdx> index) const -> const_pointer
		        {
		            using Idx = alpaka::Idx<TView>;
		            static_assert(
		                Dim<TView>::value == TDim,
		                "the index type must have the same dimensionality as the Buffer or View");
		            static_assert(
		                std::is_convertible_v<TIdx, Idx>,
		                "the index type must be convertible to the index of the Buffer or View");

		            auto ptr = reinterpret_cast<uintptr_t>(data());
		            if constexpr(TDim > 0)
		            {
		                auto const pitchesInBytes = getPitchBytesVec(*static_cast<TView const*>(this));
		                for(std::size_t i = 0u; i < TDim; i++)
		                {
		                    const Idx pitch = i + 1 < TDim ? pitchesInBytes[i + 1] : static_cast<Idx>(sizeof(value_type));
		                    ptr += static_cast<uintptr_t>(index[i] * pitch);
		                }
		            }
		            return reinterpret_cast<const_pointer>(ptr);
		        }

		    public:
		        template<std::size_t TDim, typename TIdx>
		        ALPAKA_FN_HOST auto operator[](Vec<DimInt<TDim>, TIdx> index) -> reference
		        {
		            return *const_cast<pointer>(ptr_at(index));
		        }

		        template<std::size_t TDim, typename TIdx>
		        ALPAKA_FN_HOST auto operator[](Vec<DimInt<TDim>, TIdx> index) const -> const_reference
		        {
		            return *ptr_at(index);
		        }

		        template<std::size_t TDim, typename TIdx>
		        ALPAKA_FN_HOST auto at(Vec<DimInt<TDim>, TIdx> index) -> reference
		        {
		            auto extent = getExtentVec(*static_cast<TView*>(this));
		            if(!(index < extent).foldrAll(std::logical_and<bool>(), true))
		            {
		                std::stringstream msg;
		                msg << "index " << index << " is outside of the Buffer or View extent " << extent;
		                throw std::out_of_range(msg.str());
		            }
		            return *const_cast<pointer>(ptr_at(index));
		        }

		        template<std::size_t TDim, typename TIdx>
		        [[nodiscard]] ALPAKA_FN_HOST auto at(Vec<DimInt<TDim>, TIdx> index) const -> const_reference
		        {
		            auto extent = getExtentVec(*static_cast<TView const*>(this));
		            if(!(index < extent).foldrAll(std::logical_and<bool>(), true))
		            {
		                std::stringstream msg;
		                msg << "index " << index << " is outside of the Buffer or View extent " << extent;
		                throw std::out_of_range(msg.str());
		            }
		            return *ptr_at(index);
		        }
		    };
		} // namespace alpaka::internal
		// ==
		// == ./include/alpaka/mem/view/ViewAccessOps.hpp ==
		// ============================================================================

	// #include "alpaka/meta/DependentFalseType.hpp"    // amalgamate: file already expanded
	// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

	// #include <functional>    // amalgamate: file already included
	// #include <memory>    // amalgamate: file already included
	// #include <type_traits>    // amalgamate: file already included
	// #include <utility>    // amalgamate: file already included

	namespace alpaka
	{
	    namespace detail
	    {
	        //! The CPU memory buffer.
	        template<typename TElem, typename TDim, typename TIdx>
	        class BufCpuImpl final
	        {
	            static_assert(
	                !std::is_const_v<TElem>,
	                "The elem type of the buffer can not be const because the C++ Standard forbids containers of const "
	                "elements!");
	            static_assert(!std::is_const_v<TIdx>, "The idx type of the buffer can not be const!");

	        public:
	            template<typename TExtent>
	            ALPAKA_FN_HOST BufCpuImpl(
	                DevCpu dev,
	                TElem* pMem,
	                std::function<void(TElem*)> deleter,
	                TExtent const& extent) noexcept
	                : m_dev(std::move(dev))
	                , m_extentElements(getExtentVecEnd<TDim>(extent))
	                , m_pMem(pMem)
	                , m_deleter(std::move(deleter))
	            {
	                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

	                static_assert(
	                    TDim::value == Dim<TExtent>::value,
	                    "The dimensionality of TExtent and the dimensionality of the TDim template parameter have to be "
	                    "identical!");
	                static_assert(
	                    std::is_same_v<TIdx, Idx<TExtent>>,
	                    "The idx type of TExtent and the TIdx template parameter have to be identical!");

	#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
	                std::cout << __func__ << " e: " << m_extentElements << " ptr: " << static_cast<void*>(m_pMem)
	                          << std::endl;
	#endif
	            }
	            BufCpuImpl(BufCpuImpl&&) = delete;
	            auto operator=(BufCpuImpl&&) -> BufCpuImpl& = delete;
	            ALPAKA_FN_HOST ~BufCpuImpl()
	            {
	                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

	                // NOTE: m_pMem is allowed to be a nullptr here.
	                m_deleter(m_pMem);
	            }

	        public:
	            DevCpu const m_dev;
	            Vec<TDim, TIdx> const m_extentElements;
	            TElem* const m_pMem;
	            std::function<void(TElem*)> m_deleter;
	        };
	    } // namespace detail

	    //! The CPU memory buffer.
	    template<typename TElem, typename TDim, typename TIdx>
	    class BufCpu : public internal::ViewAccessOps<BufCpu<TElem, TDim, TIdx>>
	    {
	    public:
	        template<typename TExtent, typename Deleter>
	        ALPAKA_FN_HOST BufCpu(DevCpu const& dev, TElem* pMem, Deleter deleter, TExtent const& extent)
	            : m_spBufCpuImpl{
	                std::make_shared<detail::BufCpuImpl<TElem, TDim, TIdx>>(dev, pMem, std::move(deleter), extent)}
	        {
	        }

	    public:
	        std::shared_ptr<detail::BufCpuImpl<TElem, TDim, TIdx>> m_spBufCpuImpl;
	    };

	    namespace trait
	    {
	        //! The BufCpu device type trait specialization.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct DevType<BufCpu<TElem, TDim, TIdx>>
	        {
	            using type = DevCpu;
	        };
	        //! The BufCpu device get trait specialization.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct GetDev<BufCpu<TElem, TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getDev(BufCpu<TElem, TDim, TIdx> const& buf) -> DevCpu
	            {
	                return buf.m_spBufCpuImpl->m_dev;
	            }
	        };

	        //! The BufCpu dimension getter trait.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct DimType<BufCpu<TElem, TDim, TIdx>>
	        {
	            using type = TDim;
	        };

	        //! The BufCpu memory element type get trait specialization.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct ElemType<BufCpu<TElem, TDim, TIdx>>
	        {
	            using type = TElem;
	        };

	        //! The BufCpu width get trait specialization.
	        template<typename TIdxIntegralConst, typename TElem, typename TDim, typename TIdx>
	        struct GetExtent<
	            TIdxIntegralConst,
	            BufCpu<TElem, TDim, TIdx>,
	            std::enable_if_t<(TDim::value > TIdxIntegralConst::value)>>
	        {
	            ALPAKA_FN_HOST static auto getExtent(BufCpu<TElem, TDim, TIdx> const& extent) -> TIdx
	            {
	                return extent.m_spBufCpuImpl->m_extentElements[TIdxIntegralConst::value];
	            }
	        };

	        //! The BufCpu native pointer get trait specialization.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct GetPtrNative<BufCpu<TElem, TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getPtrNative(BufCpu<TElem, TDim, TIdx> const& buf) -> TElem const*
	            {
	                return buf.m_spBufCpuImpl->m_pMem;
	            }
	            ALPAKA_FN_HOST static auto getPtrNative(BufCpu<TElem, TDim, TIdx>& buf) -> TElem*
	            {
	                return buf.m_spBufCpuImpl->m_pMem;
	            }
	        };
	        //! The BufCpu pointer on device get trait specialization.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct GetPtrDev<BufCpu<TElem, TDim, TIdx>, DevCpu>
	        {
	            ALPAKA_FN_HOST static auto getPtrDev(BufCpu<TElem, TDim, TIdx> const& buf, DevCpu const& dev)
	                -> TElem const*
	            {
	                if(dev == getDev(buf))
	                {
	                    return buf.m_spBufCpuImpl->m_pMem;
	                }
	                else
	                {
	                    throw std::runtime_error("The buffer is not accessible from the given device!");
	                }
	            }
	            ALPAKA_FN_HOST static auto getPtrDev(BufCpu<TElem, TDim, TIdx>& buf, DevCpu const& dev) -> TElem*
	            {
	                if(dev == getDev(buf))
	                {
	                    return buf.m_spBufCpuImpl->m_pMem;
	                }
	                else
	                {
	                    throw std::runtime_error("The buffer is not accessible from the given device!");
	                }
	            }
	        };

	        //! The BufCpu memory allocation trait specialization.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct BufAlloc<TElem, TDim, TIdx, DevCpu>
	        {
	            template<typename TExtent>
	            ALPAKA_FN_HOST static auto allocBuf(DevCpu const& dev, TExtent const& extent) -> BufCpu<TElem, TDim, TIdx>
	            {
	                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

	                // If ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT is defined, positive, and a power of 2, use it as the
	                // default alignment for host memory allocations. Otherwise, the alignment is chosen to enable optimal
	                // performance dependant on the target architecture.
	#if defined(ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT)
	                static_assert(
	                    ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT > 0
	                        && ((ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT & (ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT - 1)) == 0),
	                    "If defined, ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT must be a power of 2.");
	                constexpr std::size_t alignment = static_cast<std::size_t>(ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT);
	#else
	                constexpr std::size_t alignment = core::vectorization::defaultAlignment;
	#endif
	                // alpaka::AllocCpuAligned is stateless
	                using Allocator = AllocCpuAligned<std::integral_constant<std::size_t, alignment>>;
	                static_assert(std::is_empty_v<Allocator>, "AllocCpuAligned is expected to be stateless");
	                auto* memPtr = alpaka::malloc<TElem>(Allocator{}, static_cast<std::size_t>(getExtentProduct(extent)));
	                auto deleter = [](TElem* ptr) { alpaka::free(Allocator{}, ptr); };

	                return BufCpu<TElem, TDim, TIdx>(dev, memPtr, std::move(deleter), extent);
	            }
	        };
	        //! The BufCpu stream-ordered memory allocation trait specialization.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct AsyncBufAlloc<TElem, TDim, TIdx, DevCpu>
	        {
	            template<typename TQueue, typename TExtent>
	            ALPAKA_FN_HOST static auto allocAsyncBuf(TQueue queue, TExtent const& extent) -> BufCpu<TElem, TDim, TIdx>
	            {
	                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

	                static_assert(
	                    std::is_same_v<Dev<TQueue>, DevCpu>,
	                    "The BufCpu buffer can only be used with a queue on a DevCpu device!");
	                DevCpu const& dev = getDev(queue);

	                // If ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT is defined, positive, and a power of 2, use it as the
	                // default alignment for host memory allocations. Otherwise, the alignment is chosen to enable optimal
	                // performance dependant on the target architecture.
	#if defined(ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT)
	                static_assert(
	                    ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT > 0
	                        && ((ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT & (ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT - 1)) == 0),
	                    "If defined, ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT must be a power of 2.");
	                constexpr std::size_t alignment = static_cast<std::size_t>(ALPAKA_DEFAULT_HOST_MEMORY_ALIGNMENT);
	#else
	                constexpr std::size_t alignment = core::vectorization::defaultAlignment;
	#endif
	                // alpaka::AllocCpuAligned is stateless
	                using Allocator = AllocCpuAligned<std::integral_constant<std::size_t, alignment>>;
	                static_assert(std::is_empty_v<Allocator>, "AllocCpuAligned is expected to be stateless");
	                auto* memPtr = alpaka::malloc<TElem>(Allocator{}, static_cast<std::size_t>(getExtentProduct(extent)));
	                auto deleter = [queue = std::move(queue)](TElem* ptr) mutable
	                {
	                    alpaka::enqueue(
	                        queue,
	                        [ptr]()
	                        {
	                            // free the memory
	                            alpaka::free(Allocator{}, ptr);
	                        });
	                };

	                return BufCpu<TElem, TDim, TIdx>(dev, memPtr, std::move(deleter), extent);
	            }
	        };

	        //! The BufCpu stream-ordered memory allocation capability trait specialization.
	        template<typename TDim>
	        struct HasAsyncBufSupport<TDim, DevCpu> : public std::true_type
	        {
	        };

	        //! The pinned/mapped memory allocation trait specialization.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct BufAllocMapped<PltfCpu, TElem, TDim, TIdx>
	        {
	            template<typename TExtent>
	            ALPAKA_FN_HOST static auto allocMappedBuf(DevCpu const& host, TExtent const& extent)
	                -> BufCpu<TElem, TDim, TIdx>
	            {
	                // Allocate standard host memory.
	                return allocBuf<TElem, TIdx>(host, extent);
	            }
	        };

	        //! The pinned/mapped memory allocation capability trait specialization.
	        template<>
	        struct HasMappedBufSupport<PltfCpu> : public std::true_type
	        {
	        };

	        //! The BufCpu offset get trait specialization.
	        template<typename TIdxIntegralConst, typename TElem, typename TDim, typename TIdx>
	        struct GetOffset<TIdxIntegralConst, BufCpu<TElem, TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getOffset(BufCpu<TElem, TDim, TIdx> const&) -> TIdx
	            {
	                return 0u;
	            }
	        };

	        //! The BufCpu idx type trait specialization.
	        template<typename TElem, typename TDim, typename TIdx>
	        struct IdxType<BufCpu<TElem, TDim, TIdx>>
	        {
	            using type = TIdx;
	        };
	    } // namespace trait
	} // namespace alpaka

		// ============================================================================
		// == ./include/alpaka/mem/buf/cpu/Copy.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Erik Zenker, Matthias Werner, René Widera, Andrea Bocci, Jan Stephan, Bernhard
		 * Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
		// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/meta/Integral.hpp"    // amalgamate: file already expanded
		// #include "alpaka/meta/NdLoop.hpp"    // amalgamate: file already expanded

		// #include <cstring>    // amalgamate: file already included

		namespace alpaka
		{
		    class DevCpu;
		}

		namespace alpaka
		{
		    namespace detail
		    {
		        //! The CPU device memory copy task base.
		        //!
		        //! Copies from CPU memory into CPU memory.
		        template<typename TDim, typename TViewDst, typename TViewSrc, typename TExtent>
		        struct TaskCopyCpuBase
		        {
		            using ExtentSize = Idx<TExtent>;
		            using DstSize = Idx<TViewDst>;
		            using SrcSize = Idx<TViewSrc>;
		            using Elem = alpaka::Elem<TViewSrc>;

		            template<typename TViewFwd>
		            TaskCopyCpuBase(TViewFwd&& viewDst, TViewSrc const& viewSrc, TExtent const& extent)
		                : m_extent(getExtentVec(extent))
		                , m_extentWidthBytes(m_extent[TDim::value - 1u] * static_cast<ExtentSize>(sizeof(Elem)))
		#if(!defined(NDEBUG)) || (ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL)
		                , m_dstExtent(getExtentVec(viewDst))
		                , m_srcExtent(getExtentVec(viewSrc))
		#endif
		                , m_dstPitchBytes(getPitchBytesVec(viewDst))
		                , m_srcPitchBytes(getPitchBytesVec(viewSrc))
		                , m_dstMemNative(reinterpret_cast<std::uint8_t*>(getPtrNative(viewDst)))
		                , m_srcMemNative(reinterpret_cast<std::uint8_t const*>(getPtrNative(viewSrc)))
		            {
		                if constexpr(TDim::value > 0)
		                {
		                    ALPAKA_ASSERT((castVec<DstSize>(m_extent) <= m_dstExtent).foldrAll(std::logical_or<bool>()));
		                    ALPAKA_ASSERT((castVec<SrcSize>(m_extent) <= m_srcExtent).foldrAll(std::logical_or<bool>()));
		                    ALPAKA_ASSERT(static_cast<DstSize>(m_extentWidthBytes) <= m_dstPitchBytes[TDim::value - 1u]);
		                    ALPAKA_ASSERT(static_cast<SrcSize>(m_extentWidthBytes) <= m_srcPitchBytes[TDim::value - 1u]);
		                }
		            }

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		            ALPAKA_FN_HOST auto printDebug() const -> void
		            {
		                std::cout << __func__ << " e: " << m_extent << " ewb: " << this->m_extentWidthBytes
		                          << " de: " << m_dstExtent << " dptr: " << reinterpret_cast<void*>(m_dstMemNative)
		                          << " dpitchb: " << m_dstPitchBytes << " se: " << m_srcExtent
		                          << " sptr: " << reinterpret_cast<void const*>(m_srcMemNative)
		                          << " spitchb: " << m_srcPitchBytes << std::endl;
		            }
		#endif

		            Vec<TDim, ExtentSize> const m_extent;
		            ExtentSize const m_extentWidthBytes;
		#if(!defined(NDEBUG)) || (ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL)
		            Vec<TDim, DstSize> const m_dstExtent;
		            Vec<TDim, SrcSize> const m_srcExtent;
		#endif
		            Vec<TDim, DstSize> const m_dstPitchBytes;
		            Vec<TDim, SrcSize> const m_srcPitchBytes;

		            std::uint8_t* const m_dstMemNative;
		            std::uint8_t const* const m_srcMemNative;
		        };

		        //! The CPU device ND memory copy task.
		        template<typename TDim, typename TViewDst, typename TViewSrc, typename TExtent>
		        struct TaskCopyCpu : public TaskCopyCpuBase<TDim, TViewDst, TViewSrc, TExtent>
		        {
		            using DimMin1 = DimInt<TDim::value - 1u>;
		            using typename TaskCopyCpuBase<TDim, TViewDst, TViewSrc, TExtent>::ExtentSize;
		            using typename TaskCopyCpuBase<TDim, TViewDst, TViewSrc, TExtent>::DstSize;
		            using typename TaskCopyCpuBase<TDim, TViewDst, TViewSrc, TExtent>::SrcSize;

		            using TaskCopyCpuBase<TDim, TViewDst, TViewSrc, TExtent>::TaskCopyCpuBase;

		            ALPAKA_FN_HOST auto operator()() const -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                this->printDebug();
		#endif
		                // [z, y, x] -> [z, y] because all elements with the innermost x dimension are handled within one
		                // iteration.
		                Vec<DimMin1, ExtentSize> const extentWithoutInnermost(subVecBegin<DimMin1>(this->m_extent));
		                // [z, y, x] -> [y, x] because the z pitch (the full size of the buffer) is not required.
		                Vec<DimMin1, DstSize> const dstPitchBytesWithoutOutmost(subVecEnd<DimMin1>(this->m_dstPitchBytes));
		                Vec<DimMin1, SrcSize> const srcPitchBytesWithoutOutmost(subVecEnd<DimMin1>(this->m_srcPitchBytes));

		                if(static_cast<std::size_t>(this->m_extent.prod()) != 0u)
		                {
		                    meta::ndLoopIncIdx(
		                        extentWithoutInnermost,
		                        [&](Vec<DimMin1, ExtentSize> const& idx)
		                        {
		                            std::memcpy(
		                                reinterpret_cast<void*>(
		                                    this->m_dstMemNative
		                                    + (castVec<DstSize>(idx) * dstPitchBytesWithoutOutmost)
		                                          .foldrAll(std::plus<DstSize>())),
		                                reinterpret_cast<void const*>(
		                                    this->m_srcMemNative
		                                    + (castVec<SrcSize>(idx) * srcPitchBytesWithoutOutmost)
		                                          .foldrAll(std::plus<SrcSize>())),
		                                static_cast<std::size_t>(this->m_extentWidthBytes));
		                        });
		                }
		            }
		        };

		        //! The CPU device 1D memory copy task.
		        template<typename TViewDst, typename TViewSrc, typename TExtent>
		        struct TaskCopyCpu<DimInt<1u>, TViewDst, TViewSrc, TExtent>
		            : TaskCopyCpuBase<DimInt<1u>, TViewDst, TViewSrc, TExtent>
		        {
		            using TaskCopyCpuBase<DimInt<1u>, TViewDst, TViewSrc, TExtent>::TaskCopyCpuBase;

		            ALPAKA_FN_HOST auto operator()() const -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                this->printDebug();
		#endif
		                if(static_cast<std::size_t>(this->m_extent.prod()) != 0u)
		                {
		                    std::memcpy(
		                        reinterpret_cast<void*>(this->m_dstMemNative),
		                        reinterpret_cast<void const*>(this->m_srcMemNative),
		                        static_cast<std::size_t>(this->m_extentWidthBytes));
		                }
		            }
		        };

		        //! The CPU device scalar memory copy task.
		        //!
		        //! Copies from CPU memory into CPU memory.
		        template<typename TViewDst, typename TViewSrc, typename TExtent>
		        struct TaskCopyCpu<DimInt<0u>, TViewDst, TViewSrc, TExtent>
		        {
		            using Elem = alpaka::Elem<TViewSrc>;

		            template<typename TViewDstFwd>
		            TaskCopyCpu(TViewDstFwd&& viewDst, TViewSrc const& viewSrc, [[maybe_unused]] TExtent const& extent)
		                : m_dstMemNative(reinterpret_cast<std::uint8_t*>(getPtrNative(viewDst)))
		                , m_srcMemNative(reinterpret_cast<std::uint8_t const*>(getPtrNative(viewSrc)))
		            {
		                // all zero-sized extents are equivalent
		                ALPAKA_ASSERT(getExtentVec(extent).prod() == 1u);
		                ALPAKA_ASSERT(getExtentVec(viewDst).prod() == 1u);
		                ALPAKA_ASSERT(getExtentVec(viewSrc).prod() == 1u);
		            }

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		            ALPAKA_FN_HOST auto printDebug() const -> void
		            {
		                using Scalar = Vec<DimInt<0u>, Idx<TExtent>>;
		                std::cout << __func__ << " e: " << Scalar() << " ewb: " << sizeof(Elem) << " de: " << Scalar()
		                          << " dptr: " << reinterpret_cast<void*>(m_dstMemNative) << " dpitchb: " << Scalar()
		                          << " se: " << Scalar() << " sptr: " << reinterpret_cast<void const*>(m_srcMemNative)
		                          << " spitchb: " << Scalar() << std::endl;
		            }
		#endif

		            ALPAKA_FN_HOST auto operator()() const noexcept(ALPAKA_DEBUG < ALPAKA_DEBUG_FULL) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                printDebug();
		#endif
		                std::memcpy(
		                    reinterpret_cast<void*>(m_dstMemNative),
		                    reinterpret_cast<void const*>(m_srcMemNative),
		                    sizeof(Elem));
		            }

		            std::uint8_t* const m_dstMemNative;
		            std::uint8_t const* const m_srcMemNative;
		        };
		    } // namespace detail

		    namespace trait
		    {
		        //! The CPU device memory copy trait specialization.
		        //!
		        //! Copies from CPU memory into CPU memory.
		        template<typename TDim>
		        struct CreateTaskMemcpy<TDim, DevCpu, DevCpu>
		        {
		            template<typename TExtent, typename TViewSrc, typename TViewDstFwd>
		            ALPAKA_FN_HOST static auto createTaskMemcpy(
		                TViewDstFwd&& viewDst,
		                TViewSrc const& viewSrc,
		                TExtent const& extent)
		                -> alpaka::detail::TaskCopyCpu<TDim, std::remove_reference_t<TViewDstFwd>, TViewSrc, TExtent>
		            {
		                return {std::forward<TViewDstFwd>(viewDst), viewSrc, extent};
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/mem/buf/cpu/Copy.hpp ==
		// ============================================================================

		// ============================================================================
		// == ./include/alpaka/mem/buf/cpu/Set.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Erik Zenker, Matthias Werner, Andrea Bocci, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
		// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/meta/Integral.hpp"    // amalgamate: file already expanded
		// #include "alpaka/meta/NdLoop.hpp"    // amalgamate: file already expanded

		// #include <cstring>    // amalgamate: file already included

		namespace alpaka
		{
		    class DevCpu;

		    namespace detail
		    {
		        //! The CPU device ND memory set task base.
		        template<typename TDim, typename TView, typename TExtent>
		        struct TaskSetCpuBase
		        {
		            using ExtentSize = Idx<TExtent>;
		            using DstSize = Idx<TView>;
		            using Elem = alpaka::Elem<TView>;

		            template<typename TViewFwd>
		            TaskSetCpuBase(TViewFwd&& view, std::uint8_t const& byte, TExtent const& extent)
		                : m_byte(byte)
		                , m_extent(getExtentVec(extent))
		                , m_extentWidthBytes(m_extent[TDim::value - 1u] * static_cast<ExtentSize>(sizeof(Elem)))
		#if(!defined(NDEBUG)) || (ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL)
		                , m_dstExtent(getExtentVec(view))
		#endif
		                , m_dstPitchBytes(getPitchBytesVec(view))
		                , m_dstMemNative(reinterpret_cast<std::uint8_t*>(getPtrNative(view)))
		            {
		                ALPAKA_ASSERT((castVec<DstSize>(m_extent) <= m_dstExtent).foldrAll(std::logical_or<bool>()));
		                ALPAKA_ASSERT(m_extentWidthBytes <= m_dstPitchBytes[TDim::value - 1u]);
		            }

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		            ALPAKA_FN_HOST auto printDebug() const -> void
		            {
		                std::cout << __func__ << " e: " << this->m_extent << " ewb: " << this->m_extentWidthBytes
		                          << " de: " << this->m_dstExtent << " dptr: " << reinterpret_cast<void*>(this->m_dstMemNative)
		                          << " dpitchb: " << this->m_dstPitchBytes << std::endl;
		            }
		#endif

		            std::uint8_t const m_byte;
		            Vec<TDim, ExtentSize> const m_extent;
		            ExtentSize const m_extentWidthBytes;
		#if(!defined(NDEBUG)) || (ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL)
		            Vec<TDim, DstSize> const m_dstExtent;
		#endif
		            Vec<TDim, DstSize> const m_dstPitchBytes;
		            std::uint8_t* const m_dstMemNative;
		        };

		        //! The CPU device ND memory set task.
		        template<typename TDim, typename TView, typename TExtent>
		        struct TaskSetCpu : public TaskSetCpuBase<TDim, TView, TExtent>
		        {
		            using DimMin1 = DimInt<TDim::value - 1u>;
		            using typename TaskSetCpuBase<TDim, TView, TExtent>::ExtentSize;
		            using typename TaskSetCpuBase<TDim, TView, TExtent>::DstSize;

		            using TaskSetCpuBase<TDim, TView, TExtent>::TaskSetCpuBase;

		            ALPAKA_FN_HOST auto operator()() const -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                this->printDebug();
		#endif
		                // [z, y, x] -> [z, y] because all elements with the innermost x dimension are handled within one
		                // iteration.
		                Vec<DimMin1, ExtentSize> const extentWithoutInnermost(subVecBegin<DimMin1>(this->m_extent));
		                // [z, y, x] -> [y, x] because the z pitch (the full idx of the buffer) is not required.
		                Vec<DimMin1, DstSize> const dstPitchBytesWithoutOutmost(subVecEnd<DimMin1>(this->m_dstPitchBytes));

		                if(static_cast<std::size_t>(this->m_extent.prod()) != 0u)
		                {
		                    meta::ndLoopIncIdx(
		                        extentWithoutInnermost,
		                        [&](Vec<DimMin1, ExtentSize> const& idx)
		                        {
		                            std::memset(
		                                reinterpret_cast<void*>(
		                                    this->m_dstMemNative
		                                    + (castVec<DstSize>(idx) * dstPitchBytesWithoutOutmost)
		                                          .foldrAll(std::plus<DstSize>())),
		                                this->m_byte,
		                                static_cast<std::size_t>(this->m_extentWidthBytes));
		                        });
		                }
		            }
		        };

		        //! The CPU device 1D memory set task.
		        template<typename TView, typename TExtent>
		        struct TaskSetCpu<DimInt<1u>, TView, TExtent> : public TaskSetCpuBase<DimInt<1u>, TView, TExtent>
		        {
		            using TaskSetCpuBase<DimInt<1u>, TView, TExtent>::TaskSetCpuBase;

		            ALPAKA_FN_HOST auto operator()() const -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                this->printDebug();
		#endif
		                if(static_cast<std::size_t>(this->m_extent.prod()) != 0u)
		                {
		                    std::memset(
		                        reinterpret_cast<void*>(this->m_dstMemNative),
		                        this->m_byte,
		                        static_cast<std::size_t>(this->m_extentWidthBytes));
		                }
		            }
		        };

		        //! The CPU device scalar memory set task.
		        template<typename TView, typename TExtent>
		        struct TaskSetCpu<DimInt<0u>, TView, TExtent>
		        {
		            using ExtentSize = Idx<TExtent>;
		            using Scalar = Vec<DimInt<0u>, ExtentSize>;
		            using DstSize = Idx<TView>;
		            using Elem = alpaka::Elem<TView>;

		            template<typename TViewFwd>
		            TaskSetCpu(TViewFwd&& view, std::uint8_t const& byte, [[maybe_unused]] TExtent const& extent)
		                : m_byte(byte)
		                , m_dstMemNative(reinterpret_cast<std::uint8_t*>(getPtrNative(view)))
		            {
		                // all zero-sized extents are equivalent
		                ALPAKA_ASSERT(getExtentVec(extent).prod() == 1u);
		                ALPAKA_ASSERT(getExtentVec(view).prod() == 1u);
		            }

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		            ALPAKA_FN_HOST auto printDebug() const -> void
		            {
		                std::cout << __func__ << " e: " << Scalar() << " ewb: " << sizeof(Elem) << " de: " << Scalar()
		                          << " dptr: " << reinterpret_cast<void*>(m_dstMemNative) << " dpitchb: " << Scalar()
		                          << std::endl;
		            }
		#endif

		            ALPAKA_FN_HOST auto operator()() const noexcept(ALPAKA_DEBUG < ALPAKA_DEBUG_FULL) -> void
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		#if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                printDebug();
		#endif
		                std::memset(reinterpret_cast<void*>(m_dstMemNative), m_byte, sizeof(Elem));
		            }

		            std::uint8_t const m_byte;
		            std::uint8_t* const m_dstMemNative;
		        };
		    } // namespace detail

		    namespace trait
		    {
		        //! The CPU device memory set trait specialization.
		        template<typename TDim>
		        struct CreateTaskMemset<TDim, DevCpu>
		        {
		            template<typename TExtent, typename TViewFwd>
		            ALPAKA_FN_HOST static auto createTaskMemset(
		                TViewFwd&& view,
		                std::uint8_t const& byte,
		                TExtent const& extent) -> alpaka::detail::TaskSetCpu<TDim, std::remove_reference_t<TViewFwd>, TExtent>
		            {
		                return {std::forward<TViewFwd>(view), byte, extent};
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/mem/buf/cpu/Set.hpp ==
		// ============================================================================

	// ==
	// == ./include/alpaka/mem/buf/BufCpu.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/mem/buf/BufCpuSyclIntel.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevCpuSyclIntel.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/mem/buf/BufGenericSycl.hpp ==
		// ==
		/* Copyright 2022 Jan Stephan
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/DevGenericSycl.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/buf/BufCpu.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/buf/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/view/Accessor.hpp"    // amalgamate: file already expanded
		// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

		// #include <memory>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included

		#ifdef ALPAKA_ACC_SYCL_ENABLED

		// #    include <CL/sycl.hpp>    // amalgamate: file already included

		namespace alpaka
		{
		    //! The SYCL memory buffer.
		    template<typename TElem, typename TDim, typename TIdx, typename TDev>
		    class BufGenericSycl
		    {
		        static_assert(
		            !std::is_const_v<TElem>,
		            "The elem type of the buffer can not be const because the C++ Standard forbids containers of const "
		            "elements!");
		        static_assert(!std::is_const_v<TIdx>, "The idx type of the buffer can not be const!");

		    public:
		        //! Constructor
		        template<typename TExtent>
		        BufGenericSycl(TDev const& dev, sycl::buffer<TElem, TDim::value> buffer, TExtent const& extent)
		            : m_dev{dev}
		            , m_extentElements{getExtentVecEnd<TDim>(extent)}
		            , m_buffer{buffer}
		        {
		            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		            static_assert(
		                TDim::value == Dim<TExtent>::value,
		                "The dimensionality of TExtent and the dimensionality of the TDim template parameter have to be "
		                "identical!");

		            static_assert(
		                std::is_same_v<TIdx, Idx<TExtent>>,
		                "The idx type of TExtent and the TIdx template parameter have to be identical!");
		        }

		        TDev m_dev;
		        Vec<TDim, TIdx> m_extentElements;
		        sycl::buffer<TElem, TDim::value> m_buffer;
		    };
		} // namespace alpaka

		namespace alpaka::trait
		{
		    //! The BufGenericSycl device type trait specialization.
		    template<typename TElem, typename TDim, typename TIdx, typename TDev>
		    struct DevType<BufGenericSycl<TElem, TDim, TIdx, TDev>>
		    {
		        using type = TDev;
		    };

		    //! The BufGenericSycl device get trait specialization.
		    template<typename TElem, typename TDim, typename TIdx, typename TDev>
		    struct GetDev<BufGenericSycl<TElem, TDim, TIdx, TDev>>
		    {
		        static auto getDev(BufGenericSycl<TElem, TDim, TIdx, TDev> const& buf)
		        {
		            return buf.m_dev;
		        }
		    };

		    //! The BufGenericSycl dimension getter trait specialization.
		    template<typename TElem, typename TDim, typename TIdx, typename TDev>
		    struct DimType<BufGenericSycl<TElem, TDim, TIdx, TDev>>
		    {
		        using type = TDim;
		    };

		    //! The BufGenericSycl memory element type get trait specialization.
		    template<typename TElem, typename TDim, typename TIdx, typename TDev>
		    struct ElemType<BufGenericSycl<TElem, TDim, TIdx, TDev>>
		    {
		        using type = TElem;
		    };

		    //! The BufGenericSycl extent get trait specialization.
		    template<typename TIdxIntegralConst, typename TElem, typename TDim, typename TIdx, typename TDev>
		    struct GetExtent<TIdxIntegralConst, BufGenericSycl<TElem, TDim, TIdx, TDev>>
		    {
		        static_assert(TDim::value > TIdxIntegralConst::value, "Requested dimension out of bounds");

		        static auto getExtent(BufGenericSycl<TElem, TDim, TIdx, TDev> const& buf) -> TIdx
		        {
		            return buf.m_extentElements[TIdxIntegralConst::value];
		        }
		    };

		    //! The BufGenericSycl native pointer get trait specialization.
		    template<typename TElem, typename TDim, typename TIdx, typename TDev>
		    struct GetPtrNative<BufGenericSycl<TElem, TDim, TIdx, TDev>>
		    {
		        static_assert(
		            !sizeof(TElem),
		            "Accessing device-side pointers on the host is not supported by the SYCL back-end");

		        static auto getPtrNative(BufGenericSycl<TElem, TDim, TIdx, TDev> const&) -> TElem const*
		        {
		            return nullptr;
		        }

		        static auto getPtrNative(BufGenericSycl<TElem, TDim, TIdx, TDev>&) -> TElem*
		        {
		            return nullptr;
		        }
		    };

		    //! The BufGenericSycl pointer on device get trait specialization.
		    template<typename TElem, typename TDim, typename TIdx, typename TDev>
		    struct GetPtrDev<BufGenericSycl<TElem, TDim, TIdx, TDev>, TDev>
		    {
		        static_assert(
		            !sizeof(TElem),
		            "Accessing device-side pointers on the host is not supported by the SYCL back-end");

		        static auto getPtrDev(BufGenericSycl<TElem, TDim, TIdx, TDev> const&, TDev const&) -> TElem const*
		        {
		            return nullptr;
		        }

		        static auto getPtrDev(BufGenericSycl<TElem, TDim, TIdx, TDev>&, TDev const&) -> TElem*
		        {
		            return nullptr;
		        }
		    };

		    //! The SYCL memory allocation trait specialization.
		    template<typename TElem, typename TDim, typename TIdx, typename TPltf>
		    struct BufAlloc<TElem, TDim, TIdx, DevGenericSycl<TPltf>>
		    {
		        template<typename TExtent>
		        static auto allocBuf(DevGenericSycl<TPltf> const& dev, TExtent const& ext)
		            -> BufGenericSycl<TElem, TDim, TIdx, DevGenericSycl<TPltf>>
		        {
		            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		            if constexpr(TDim::value == 0 || TDim::value == 1)
		            {
		                auto const width = getWidth(ext);

		#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                auto const widthBytes = width * static_cast<TIdx>(sizeof(TElem));
		                std::cout << __func__ << " ew: " << width << " ewb: " << widthBytes << '\n';
		#    endif

		                auto const range = sycl::range<1>{width};
		                return {dev, sycl::buffer<TElem, 1>{range}, ext};
		            }
		            else if constexpr(TDim::value == 2)
		            {
		                auto const width = getWidth(ext);
		                auto const height = getHeight(ext);

		#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                auto const widthBytes = width * static_cast<TIdx>(sizeof(TElem));
		                std::cout << __func__ << " ew: " << width << " eh: " << height << " ewb: " << widthBytes
		                          << " pitch: " << widthBytes << '\n';
		#    endif

		                auto const range = sycl::range<2>{width, height};
		                return {dev, sycl::buffer<TElem, 2>{range}, ext};
		            }
		            else if constexpr(TDim::value == 3)
		            {
		                auto const width = getWidth(ext);
		                auto const height = getHeight(ext);
		                auto const depth = getDepth(ext);

		#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                auto const widthBytes = width * static_cast<TIdx>(sizeof(TElem));
		                std::cout << __func__ << " ew: " << width << " eh: " << height << " ed: " << depth
		                          << " ewb: " << widthBytes << " pitch: " << widthBytes << '\n';
		#    endif

		                auto const range = sycl::range<3>{width, height, depth};
		                return {dev, sycl::buffer<TElem, 3>{range}, ext};
		            }
		        }
		    };

		    //! The BufGenericSycl offset get trait specialization.
		    template<typename TIdxIntegralConst, typename TElem, typename TDim, typename TIdx, typename TDev>
		    struct GetOffset<TIdxIntegralConst, BufGenericSycl<TElem, TDim, TIdx, TDev>>
		    {
		        static auto getOffset(BufGenericSycl<TElem, TDim, TIdx, TDev> const&) -> TIdx
		        {
		            return 0u;
		        }
		    };

		    //! The BufGenericSycl idx type trait specialization.
		    template<typename TElem, typename TDim, typename TIdx, typename TDev>
		    struct IdxType<BufGenericSycl<TElem, TDim, TIdx, TDev>>
		    {
		        using type = TIdx;
		    };

		    //! The BufCpu pointer on SYCL device get trait specialization.
		    template<typename TElem, typename TDim, typename TIdx, typename TPltf>
		    struct GetPtrDev<BufCpu<TElem, TDim, TIdx>, DevGenericSycl<TPltf>>
		    {
		        static_assert(!sizeof(TElem), "Accessing host pointers on the device is not supported by the SYCL back-end");

		        static auto getPtrDev(BufCpu<TElem, TDim, TIdx> const&, DevGenericSycl<TPltf> const&) -> TElem const*
		        {
		            return nullptr;
		        }

		        static auto getPtrDev(BufCpu<TElem, TDim, TIdx>&, DevGenericSycl<TPltf> const&) -> TElem*
		        {
		            return nullptr;
		        }
		    };
		} // namespace alpaka::trait

		// #    include "alpaka/mem/buf/sycl/Accessor.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/mem/buf/sycl/Copy.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */


			// #pragma once
			// #include "alpaka/core/Debug.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/DevGenericSycl.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
			// #include "alpaka/elem/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
				// ============================================================================
				// == ./include/alpaka/mem/buf/sycl/Common.hpp ==
				// ==
				/* Copyright 2022 Jan Stephan
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
				// #include "alpaka/offset/Traits.hpp"    // amalgamate: file already expanded

				// #include <cstddef>    // amalgamate: file already included

				#ifdef ALPAKA_ACC_SYCL_ENABLED

				// #    include <CL/sycl.hpp>    // amalgamate: file already included

				namespace alpaka::detail
				{
				    template<typename TExtent>
				    inline auto make_sycl_range(TExtent const& ext, std::size_t multiplier = 1)
				    {
				        constexpr auto dim = Dim<TExtent>::value;

				        auto const width = getWidth(ext) * multiplier;

				        if constexpr(dim == 1)
				            return sycl::range<1>{width};
				        else if constexpr(dim == 2)
				            return sycl::range<2>{width, getHeight(ext)};
				        else
				            return sycl::range<3>{width, getHeight(ext), getDepth(ext)};
				    }

				    template<typename TView>
				    inline auto make_sycl_offset(TView const& view)
				    {
				        constexpr auto dim = Dim<TView>::value;

				        if constexpr(dim == 1)
				            return sycl::id<1>{getOffsetX(view)};
				        else if constexpr(dim == 2)
				            return sycl::id<2>{getOffsetX(view), getOffsetY(view)};
				        else
				            return sycl::id<3>{getOffsetX(view), getOffsetY(view), getOffsetZ(view)};
				    }
				} // namespace alpaka::detail

				#endif
				// ==
				// == ./include/alpaka/mem/buf/sycl/Common.hpp ==
				// ============================================================================

			// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded

			// #include <memory>    // amalgamate: file already included
			// #include <type_traits>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			// #    include <CL/sycl.hpp>    // amalgamate: file already included

			namespace alpaka::detail
			{
			    template<typename TElem, std::size_t TDim>
			    using SrcAccessor = sycl::
			        accessor<TElem, TDim, sycl::access_mode::read, sycl::target::global_buffer, sycl::access::placeholder::true_t>;

			    template<typename TElem, std::size_t TDim>
			    using DstAccessor = sycl::accessor<
			        TElem,
			        TDim,
			        sycl::access_mode::write,
			        sycl::target::global_buffer,
			        sycl::access::placeholder::true_t>;

			    enum class Direction
			    {
			        h2d,
			        d2h,
			        d2d
			    };

			    template<typename TSrc, typename TDst, Direction TDirection>
			    struct TaskCopySycl
			    {
			        auto operator()(sycl::handler& cgh) const -> void
			        {
			            if constexpr(TDirection == Direction::d2h || TDirection == Direction::d2d)
			                cgh.require(m_src);

			            if constexpr(TDirection == Direction::h2d || TDirection == Direction::d2d)
			                cgh.require(m_dst);

			            cgh.copy(m_src, m_dst);
			        }

			        TSrc m_src;
			        TDst m_dst;
			        static constexpr auto is_sycl_task = true;
			    };
			} // namespace alpaka::detail

			// Trait specializations for CreateTaskMemcpy.
			namespace alpaka::trait
			{
			    //! The SYCL host-to-device memory copy trait specialization.
			    template<typename TDim, typename TPltf>
			    struct CreateTaskMemcpy<TDim, DevGenericSycl<TPltf>, DevCpu>
			    {
			        template<typename TExtent, typename TViewSrc, typename TViewDstFwd>
			        static auto createTaskMemcpy(TViewDstFwd&& viewDst, TViewSrc const& viewSrc, TExtent const& ext)
			        {
			            ALPAKA_DEBUG_FULL_LOG_SCOPE;

			            constexpr auto copy_dim = static_cast<int>(Dim<TExtent>::value);
			            using ElemType = Elem<std::remove_const_t<TViewSrc>>;
			            using SrcType = ElemType const*;
			            using DstType = alpaka::detail::DstAccessor<ElemType, copy_dim>;

			            auto const range = detail::make_sycl_range(ext);
			            auto const offset = detail::make_sycl_offset(viewDst);

			            return detail::TaskCopySycl<SrcType, DstType, detail::Direction::h2d>{
			                getPtrNative(viewSrc),
			                DstType{viewDst.m_buffer, range, offset}};
			        }
			    };

			    //! The SYCL device-to-host memory copy trait specialization.
			    template<typename TDim, typename TPltf>
			    struct CreateTaskMemcpy<TDim, DevCpu, DevGenericSycl<TPltf>>
			    {
			        template<typename TExtent, typename TViewSrc, typename TViewDstFwd>
			        static auto createTaskMemcpy(TViewDstFwd&& viewDst, TViewSrc const& viewSrc, TExtent const& ext)
			        {
			            ALPAKA_DEBUG_FULL_LOG_SCOPE;

			            constexpr auto copy_dim = static_cast<int>(Dim<TExtent>::value);
			            using ElemType = Elem<std::remove_const_t<TViewSrc>>;
			            using SrcType = alpaka::detail::SrcAccessor<ElemType, copy_dim>;
			            using DstType = ElemType*;

			            auto const range = detail::make_sycl_range(ext);
			            auto const offset = detail::make_sycl_offset(viewSrc);

			            auto view_src = const_cast<TViewSrc&>(viewSrc);

			            return detail::TaskCopySycl<SrcType, DstType, detail::Direction::d2h>{
			                SrcType{view_src.m_buffer, range, offset},
			                getPtrNative(viewDst)};
			        }
			    };

			    //! The SYCL device-to-device memory copy trait specialization.
			    template<typename TDim, typename TPltfDst, typename TPltfSrc>
			    struct CreateTaskMemcpy<TDim, DevGenericSycl<TPltfDst>, DevGenericSycl<TPltfSrc>>
			    {
			        template<typename TExtent, typename TViewSrc, typename TViewDstFwd>
			        static auto createTaskMemcpy(TViewDstFwd&& viewDst, TViewSrc const& viewSrc, TExtent const& ext)
			        {
			            ALPAKA_DEBUG_FULL_LOG_SCOPE;

			            constexpr auto copy_dim = static_cast<int>(Dim<TExtent>::value);
			            using ElemType = Elem<std::remove_const_t<TViewSrc>>;
			            using SrcType = alpaka::detail::SrcAccessor<ElemType, copy_dim>;
			            using DstType = alpaka::detail::DstAccessor<ElemType, copy_dim>;

			            auto const range = detail::make_sycl_range(ext);
			            auto const offset_src = detail::make_sycl_offset(viewSrc);
			            auto const offset_dst = detail::make_sycl_offset(viewDst);

			            auto view_src = const_cast<TViewSrc&>(viewSrc);

			            return detail::TaskCopySycl<SrcType, DstType, detail::Direction::d2d>{
			                SrcType{view_src.m_buffer, range, offset_src},
			                DstType{viewDst.m_buffer, range, offset_dst}};
			        }
			    };
			} // namespace alpaka::trait

			#endif
			// ==
			// == ./include/alpaka/mem/buf/sycl/Copy.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/mem/buf/sycl/Set.hpp ==
			// ==
			/* Copyright 2022 Jan Stephan
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Debug.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Sycl.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/DevGenericSycl.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
			// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/mem/buf/sycl/Common.hpp"    // amalgamate: file already expanded
			// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded

			// #include <cstddef>    // amalgamate: file already included
			// #include <cstdint>    // amalgamate: file already included
			// #include <memory>    // amalgamate: file already included

			#ifdef ALPAKA_ACC_SYCL_ENABLED

			namespace alpaka
			{

			    namespace detail
			    {
			        template<std::size_t TDim>
			        using Accessor = sycl::accessor<
			            std::byte,
			            TDim,
			            sycl::access_mode::write,
			            sycl::target::global_buffer,
			            sycl::access::placeholder::true_t>;

			        //! The SYCL memory set trait.
			        template<typename TAccessor>
			        struct TaskSetSycl
			        {
			            auto operator()(sycl::handler& cgh) const -> void
			            {
			                cgh.require(m_accessor);
			                cgh.fill(m_accessor, m_value);
			            }

			            TAccessor m_accessor;
			            std::byte m_value;
			            // Distinguish from non-alpaka types (= host tasks)
			            static constexpr auto is_sycl_task = true;
			        };
			    } // namespace detail


			    namespace trait
			    {
			        //! The SYCL device memory set trait specialization.
			        template<typename TDim, typename TPltf>
			        struct CreateTaskMemset<TDim, DevGenericSycl<TPltf>>
			        {
			            template<typename TExtent, typename TViewFwd>
			            static auto createTaskMemset(TViewFwd&& view, std::uint8_t const& byte, TExtent const& ext)
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                constexpr auto set_dim = static_cast<int>(Dim<TExtent>::value);
			                using TView = std::remove_reference_t<TViewFwd>;
			                using ElemType = Elem<TView>;
			                using DstType = alpaka::detail::Accessor<set_dim>;

			                // Reinterpret as byte buffer
			                auto buf = view.m_buffer.template reinterpret<std::byte>();
			                auto const byte_val = static_cast<std::byte>(byte);

			                auto const range = detail::make_sycl_range(ext, sizeof(ElemType));
			                return detail::TaskSetSycl<DstType>{DstType{buf, range}, byte_val};
			            }
			        };
			    } // namespace trait
			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/mem/buf/sycl/Set.hpp ==
			// ============================================================================


		#endif
		// ==
		// == ./include/alpaka/mem/buf/BufGenericSycl.hpp ==
		// ============================================================================


	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_CPU)

	namespace alpaka
	{
	    template<typename TElem, typename TDim, typename TIdx>
	    using BufCpuSyclIntel = BufGenericSycl<TElem, TDim, TIdx, DevCpuSyclIntel>;
	}

	#endif
	// ==
	// == ./include/alpaka/mem/buf/BufCpuSyclIntel.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/mem/buf/BufCudaRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiCudaRt.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/mem/buf/BufUniformCudaHipRt.hpp ==
		// ==
		/* Copyright 2022 Alexander Matthes, Benjamin Worpitz, Matthias Werner, René Widera, Andrea Bocci, Jan Stephan,
		 * Bernhard Manfred Gruber, Antonio Di Pilato
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/buf/Traits.hpp"    // amalgamate: file already expanded
		// #include "alpaka/mem/view/ViewAccessOps.hpp"    // amalgamate: file already expanded
		// #include "alpaka/meta/DependentFalseType.hpp"    // amalgamate: file already expanded
		// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

		// #include <functional>    // amalgamate: file already included
		// #include <memory>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included

		#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

		namespace alpaka
		{
		    // Forward declarations.
		    struct ApiCudaRt;
		    struct ApiHipRt;

		    template<typename TElem, typename TDim, typename TIdx>
		    class BufCpu;

		    //! The CUDA/HIP memory buffer.
		    template<typename TApi, typename TElem, typename TDim, typename TIdx>
		    class BufUniformCudaHipRt : public internal::ViewAccessOps<BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>>
		    {
		    public:
		        static_assert(
		            !std::is_const_v<TElem>,
		            "The elem type of the buffer can not be const because the C++ Standard forbids containers of const "
		            "elements!");
		        static_assert(!std::is_const_v<TIdx>, "The idx type of the buffer can not be const!");

		        //! Constructor
		        template<typename TExtent, typename Deleter>
		        ALPAKA_FN_HOST BufUniformCudaHipRt(
		            DevUniformCudaHipRt<TApi> const& dev,
		            TElem* const pMem,
		            Deleter deleter,
		            TIdx const& pitchBytes,
		            TExtent const& extent)
		            : m_dev(dev)
		            , m_extentElements(getExtentVecEnd<TDim>(extent))
		            , m_spMem(pMem, std::move(deleter))
		            , m_pitchBytes(pitchBytes)
		        {
		            ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		            static_assert(
		                TDim::value == alpaka::Dim<TExtent>::value,
		                "The dimensionality of TExtent and the dimensionality of the TDim template parameter have to be "
		                "identical!");
		            static_assert(
		                std::is_same_v<TIdx, Idx<TExtent>>,
		                "The idx type of TExtent and the TIdx template parameter have to be identical!");
		        }

		    public:
		        DevUniformCudaHipRt<TApi> m_dev;
		        Vec<TDim, TIdx> m_extentElements;
		        std::shared_ptr<TElem> m_spMem;
		        TIdx m_pitchBytes;
		    };

		    namespace trait
		    {
		        //! The BufUniformCudaHipRt device type trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct DevType<BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>>
		        {
		            using type = DevUniformCudaHipRt<TApi>;
		        };

		        //! The BufUniformCudaHipRt device get trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct GetDev<BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>>
		        {
		            ALPAKA_FN_HOST static auto getDev(BufUniformCudaHipRt<TApi, TElem, TDim, TIdx> const& buf)
		                -> DevUniformCudaHipRt<TApi>
		            {
		                return buf.m_dev;
		            }
		        };

		        //! The BufUniformCudaHipRt dimension getter trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct DimType<BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>>
		        {
		            using type = TDim;
		        };

		        //! The BufUniformCudaHipRt memory element type get trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct ElemType<BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>>
		        {
		            using type = TElem;
		        };

		        //! The BufUniformCudaHipRt extent get trait specialization.
		        template<typename TApi, typename TIdxIntegralConst, typename TElem, typename TDim, typename TIdx>
		        struct GetExtent<
		            TIdxIntegralConst,
		            BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>,
		            std::enable_if_t<(TDim::value > TIdxIntegralConst::value)>>
		        {
		            ALPAKA_FN_HOST static auto getExtent(BufUniformCudaHipRt<TApi, TElem, TDim, TIdx> const& extent) -> TIdx
		            {
		                return extent.m_extentElements[TIdxIntegralConst::value];
		            }
		        };

		        //! The BufUniformCudaHipRt native pointer get trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct GetPtrNative<BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>>
		        {
		            ALPAKA_FN_HOST static auto getPtrNative(BufUniformCudaHipRt<TApi, TElem, TDim, TIdx> const& buf)
		                -> TElem const*
		            {
		                return buf.m_spMem.get();
		            }
		            ALPAKA_FN_HOST static auto getPtrNative(BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>& buf) -> TElem*
		            {
		                return buf.m_spMem.get();
		            }
		        };

		        //! The BufUniformCudaHipRt pointer on device get trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct GetPtrDev<BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>, DevUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto getPtrDev(
		                BufUniformCudaHipRt<TApi, TElem, TDim, TIdx> const& buf,
		                DevUniformCudaHipRt<TApi> const& dev) -> TElem const*
		            {
		                if(dev == getDev(buf))
		                {
		                    return buf.m_spMem.get();
		                }
		                else
		                {
		                    throw std::runtime_error("The buffer is not accessible from the given device!");
		                }
		            }
		            ALPAKA_FN_HOST static auto getPtrDev(
		                BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>& buf,
		                DevUniformCudaHipRt<TApi> const& dev) -> TElem*
		            {
		                if(dev == getDev(buf))
		                {
		                    return buf.m_spMem.get();
		                }
		                else
		                {
		                    throw std::runtime_error("The buffer is not accessible from the given device!");
		                }
		            }
		        };

		        //! The BufUniformCudaHipRt pitch get trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct GetPitchBytes<DimInt<TDim::value - 1u>, BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>>
		        {
		            ALPAKA_FN_HOST static auto getPitchBytes(BufUniformCudaHipRt<TApi, TElem, TDim, TIdx> const& buf) -> TIdx
		            {
		                return buf.m_pitchBytes; // TODO(bgruber): is this even correct? This reports the pitch for the TDim -
		                                         // 1 dimension, but CUDA's pitch is always the row pitch, independently of the
		                                         // buffer's dimensions.
		            }
		        };

		        //! The CUDA/HIP memory allocation trait specialization.
		        template<typename TApi, typename TElem, typename Dim, typename TIdx>
		        struct BufAlloc<TElem, Dim, TIdx, DevUniformCudaHipRt<TApi>>
		        {
		            template<typename TExtent>
		            ALPAKA_FN_HOST static auto allocBuf(DevUniformCudaHipRt<TApi> const& dev, TExtent const& extent)
		                -> BufUniformCudaHipRt<TApi, TElem, Dim, TIdx>
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(dev.getNativeHandle()));

		                void* memPtr = nullptr;
		                std::size_t pitchBytes = 0u;
		                if(getExtentProduct(extent) != 0)
		                {
		                    if constexpr(Dim::value <= 1)
		                    {
		                        pitchBytes = static_cast<std::size_t>(getWidth(extent)) * sizeof(TElem);
		                        ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::malloc(&memPtr, pitchBytes));
		                    }
		                    else if constexpr(Dim::value == 2)
		                    {
		                        ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::mallocPitch(
		                            &memPtr,
		                            &pitchBytes,
		                            static_cast<std::size_t>(getWidth(extent)) * sizeof(TElem),
		                            static_cast<std::size_t>(getHeight(extent))));
		                    }
		                    else if constexpr(Dim::value == 3)
		                    {
		                        typename TApi::Extent_t const extentVal = TApi::makeExtent(
		                            static_cast<std::size_t>(getWidth(extent)) * sizeof(TElem),
		                            static_cast<std::size_t>(getHeight(extent)),
		                            static_cast<std::size_t>(getDepth(extent)));
		                        typename TApi::PitchedPtr_t pitchedPtrVal;
		                        pitchedPtrVal.ptr = nullptr;
		                        ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::malloc3D(&pitchedPtrVal, extentVal));
		                        memPtr = pitchedPtrVal.ptr;
		                        pitchBytes = pitchedPtrVal.pitch;
		                    }
		                }
		#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                std::cout << __func__;
		                if constexpr(Dim::value >= 1)
		                    std::cout << " ew: " << getWidth(extent);
		                if constexpr(Dim::value >= 2)
		                    std::cout << " eh: " << getHeight(extent);
		                if constexpr(Dim::value >= 3)
		                    std::cout << " ed: " << getDepth(extent);
		                std::cout << " ptr: " << memPtr << " pitch: " << pitchBytes << std::endl;
		#    endif
		                return {
		                    dev,
		                    reinterpret_cast<TElem*>(memPtr),
		                    [](TElem* ptr) { ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_NOEXCEPT(TApi::free(ptr)); },
		                    static_cast<TIdx>(pitchBytes),
		                    extent};
		            }
		        };

		        //! The CUDA/HIP stream-ordered memory allocation trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct AsyncBufAlloc<TElem, TDim, TIdx, DevUniformCudaHipRt<TApi>>
		        {
		#    if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
		            static_assert(
		                std::is_same_v<TApi, ApiCudaRt> && TApi::version >= BOOST_VERSION_NUMBER(11, 2, 0),
		                "Support for stream-ordered memory buffers requires CUDA 11.2 or higher.");
		#    endif
		#    if defined(ALPAKA_ACC_GPU_HIP_ENABLED)
		            static_assert(
		                !std::is_same_v<TApi, ApiHipRt>,
		                "HIP devices do not support stream-ordered memory buffers.");
		#    endif
		            static_assert(
		                TDim::value <= 1,
		                "CUDA/HIP devices support only one-dimensional stream-ordered memory buffers.");

		            template<typename TQueue, typename TExtent>
		            ALPAKA_FN_HOST static auto allocAsyncBuf(TQueue queue, [[maybe_unused]] TExtent const& extent)
		                -> BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                static_assert(TDim::value == Dim<TExtent>::value, "extent must have the same dimension as the buffer");
		                auto const width = getWidth(extent);

		                auto const& dev = getDev(queue);
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(dev.getNativeHandle()));
		                void* memPtr = nullptr;
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::mallocAsync(
		                    &memPtr,
		                    static_cast<std::size_t>(width) * sizeof(TElem),
		                    queue.getNativeHandle()));

		#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                std::cout << __func__ << " ew: " << width << " ptr: " << memPtr << std::endl;
		#    endif
		                return BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>(
		                    dev,
		                    reinterpret_cast<TElem*>(memPtr),
		                    [queue = std::move(queue)](TElem* ptr)
		                    { ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_NOEXCEPT(TApi::freeAsync(ptr, queue.getNativeHandle())); },
		                    width * static_cast<TIdx>(sizeof(TElem)),
		                    extent);
		            }
		        };

		#    if defined(ALPAKA_ACC_GPU_CUDA_ENABLED)
		        //! The CUDA/HIP stream-ordered memory allocation capability trait specialization.
		        template<typename TApi, typename TDim>
		        struct HasAsyncBufSupport<TDim, DevUniformCudaHipRt<TApi>>
		            : std::bool_constant<
		                  std::is_same_v<TApi, ApiCudaRt> && TApi::version >= BOOST_VERSION_NUMBER(11, 2, 0)
		                  && TDim::value <= 1>
		        {
		        };
		#    endif

		        //! The pinned/mapped memory allocation trait specialization for the CUDA/HIP devices.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct BufAllocMapped<PltfUniformCudaHipRt<TApi>, TElem, TDim, TIdx>
		        {
		            template<typename TExtent>
		            ALPAKA_FN_HOST static auto allocMappedBuf(DevCpu const& host, TExtent const& extent)
		                -> BufCpu<TElem, TDim, TIdx>
		            {
		                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

		                // Allocate CUDA/HIP page-locked memory on the host, mapped into the CUDA/HIP address space and
		                // accessible to all CUDA/HIP devices.
		                TElem* memPtr = nullptr;
		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::hostMalloc(
		                    reinterpret_cast<void**>(&memPtr),
		                    sizeof(TElem) * static_cast<std::size_t>(getExtentProduct(extent)),
		                    TApi::hostMallocMapped | TApi::hostMallocPortable));
		                auto deleter = [](TElem* ptr) { ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK_NOEXCEPT(TApi::hostFree(ptr)); };

		                return BufCpu<TElem, TDim, TIdx>(host, memPtr, std::move(deleter), extent);
		            }
		        };

		        //! The pinned/mapped memory allocation capability trait specialization.
		        template<typename TApi>
		        struct HasMappedBufSupport<PltfUniformCudaHipRt<TApi>> : public std::true_type
		        {
		        };

		        //! The BufUniformCudaHipRt offset get trait specialization.
		        template<typename TApi, typename TIdxIntegralConst, typename TElem, typename TDim, typename TIdx>
		        struct GetOffset<TIdxIntegralConst, BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>>
		        {
		            ALPAKA_FN_HOST static auto getOffset(BufUniformCudaHipRt<TApi, TElem, TDim, TIdx> const&) -> TIdx
		            {
		                return 0u;
		            }
		        };

		        //! The BufUniformCudaHipRt idx type trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct IdxType<BufUniformCudaHipRt<TApi, TElem, TDim, TIdx>>
		        {
		            using type = TIdx;
		        };

		        //! The BufCpu pointer on CUDA/HIP device get trait specialization.
		        template<typename TApi, typename TElem, typename TDim, typename TIdx>
		        struct GetPtrDev<BufCpu<TElem, TDim, TIdx>, DevUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto getPtrDev(
		                BufCpu<TElem, TDim, TIdx> const& buf,
		                DevUniformCudaHipRt<TApi> const&) -> TElem const*
		            {
		                // TODO: Check if the memory is mapped at all!
		                TElem* pDev(nullptr);

		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::hostGetDevicePointer(
		                    &pDev,
		                    const_cast<void*>(reinterpret_cast<void const*>(getPtrNative(buf))),
		                    0));

		                return pDev;
		            }
		            ALPAKA_FN_HOST static auto getPtrDev(BufCpu<TElem, TDim, TIdx>& buf, DevUniformCudaHipRt<TApi> const&)
		                -> TElem*
		            {
		                // TODO: Check if the memory is mapped at all!
		                TElem* pDev(nullptr);

		                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::hostGetDevicePointer(&pDev, getPtrNative(buf), 0));

		                return pDev;
		            }
		        };
		    } // namespace trait
		} // namespace alpaka

			// ============================================================================
			// == ./include/alpaka/mem/buf/uniformCudaHip/Copy.hpp ==
			// ==
			/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Erik Zenker, Matthias Werner, René Widera, Andrea Bocci, Jan Stephan,
			 * Bernhard Manfred Gruber, Antonio Di Pilato
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
			// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/QueueUniformCudaHipRtBlocking.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/QueueUniformCudaHipRtNonBlocking.hpp"    // amalgamate: file already expanded

			// #include <cstdint>    // amalgamate: file already included
			// #include <set>    // amalgamate: file already included
			// #include <tuple>    // amalgamate: file already included
			// #include <type_traits>    // amalgamate: file already included

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    namespace detail
			    {
			        //! The CUDA/HIP memory copy trait.
			        template<typename TApi, typename TDim, typename TViewDst, typename TViewSrc, typename TExtent>
			        struct TaskCopyUniformCudaHip;

			        //! The scalar CUDA/HIP memory copy trait.
			        template<typename TApi, typename TViewDst, typename TViewSrc, typename TExtent>
			        struct TaskCopyUniformCudaHip<TApi, DimInt<0u>, TViewDst, TViewSrc, TExtent>
			        {
			            using Idx = alpaka::Idx<TExtent>;

			            template<typename TViewDstFwd>
			            ALPAKA_FN_HOST TaskCopyUniformCudaHip(
			                TViewDstFwd&& viewDst,
			                TViewSrc const& viewSrc,
			                [[maybe_unused]] TExtent const& extent,
			                typename TApi::MemcpyKind_t const& uniformMemCpyKind,
			                int const& iDstDevice,
			                int const& iSrcDevice)
			                : m_uniformMemCpyKind(uniformMemCpyKind)
			                , m_iDstDevice(iDstDevice)
			                , m_iSrcDevice(iSrcDevice)
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                , m_dstWidth(static_cast<Idx>(getWidth(viewDst)))
			                , m_srcWidth(static_cast<Idx>(getWidth(viewSrc)))
			#    endif
			                , m_dstMemNative(reinterpret_cast<void*>(getPtrNative(viewDst)))
			                , m_srcMemNative(reinterpret_cast<void const*>(getPtrNative(viewSrc)))
			            {
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                ALPAKA_ASSERT(Idx(1u) <= m_dstWidth);
			                ALPAKA_ASSERT(Idx(1u) <= m_srcWidth);
			#    endif
			            }

			            template<typename TQueue>
			            auto enqueue(TQueue& queue) const -> void
			            {
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                printDebug();
			#    endif
			                // cudaMemcpy variants on cudaMallocAsync'ed memory need to be called with the correct device,
			                // see https://github.com/fwyzard/nvidia_bug_3446335 .
			                // Set the current device.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(m_iDstDevice));
			                // Initiate the memory copy.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::memcpyAsync(
			                    m_dstMemNative,
			                    m_srcMemNative,
			                    sizeof(Elem<TViewDst>),
			                    m_uniformMemCpyKind,
			                    queue.getNativeHandle()));
			            }

			        private:
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			            ALPAKA_FN_HOST auto printDebug() const -> void
			            {
			                std::cout << __func__ << " ddev: " << m_iDstDevice << " ew: " << Idx(1u)
			                          << " ewb: " << static_cast<Idx>(sizeof(Elem<TViewDst>)) << " dw: " << m_dstWidth
			                          << " dptr: " << m_dstMemNative << " sdev: " << m_iSrcDevice << " sw: " << m_srcWidth
			                          << " sptr: " << m_srcMemNative << std::endl;
			            }
			#    endif

			            typename TApi::MemcpyKind_t m_uniformMemCpyKind;
			            int m_iDstDevice;
			            int m_iSrcDevice;
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			            Idx m_dstWidth;
			            Idx m_srcWidth;
			#    endif
			            void* m_dstMemNative;
			            void const* m_srcMemNative;
			        };

			        //! The 1D CUDA/HIP memory copy trait.
			        template<typename TApi, typename TViewDst, typename TViewSrc, typename TExtent>
			        struct TaskCopyUniformCudaHip<TApi, DimInt<1u>, TViewDst, TViewSrc, TExtent>
			        {
			            using Idx = alpaka::Idx<TExtent>;

			            template<typename TViewDstFwd>
			            ALPAKA_FN_HOST TaskCopyUniformCudaHip(
			                TViewDstFwd&& viewDst,
			                TViewSrc const& viewSrc,
			                TExtent const& extent,
			                typename TApi::MemcpyKind_t const& uniformMemCpyKind,
			                int const& iDstDevice,
			                int const& iSrcDevice)
			                : m_uniformMemCpyKind(uniformMemCpyKind)
			                , m_iDstDevice(iDstDevice)
			                , m_iSrcDevice(iSrcDevice)
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                , m_extentWidth(getWidth(extent))
			                , m_dstWidth(static_cast<Idx>(getWidth(viewDst)))
			                , m_srcWidth(static_cast<Idx>(getWidth(viewSrc)))
			#    endif
			                , m_extentWidthBytes(getWidth(extent) * static_cast<Idx>(sizeof(Elem<TViewDst>)))
			                , m_dstMemNative(reinterpret_cast<void*>(getPtrNative(viewDst)))
			                , m_srcMemNative(reinterpret_cast<void const*>(getPtrNative(viewSrc)))
			            {
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                ALPAKA_ASSERT(m_extentWidth <= m_dstWidth);
			                ALPAKA_ASSERT(m_extentWidth <= m_srcWidth);
			#    endif
			            }

			            template<typename TQueue>
			            auto enqueue(TQueue& queue) const -> void
			            {
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                printDebug();
			#    endif
			                if(m_extentWidthBytes == 0)
			                {
			                    return;
			                }

			                // cudaMemcpy variants on cudaMallocAsync'ed memory need to be called with the correct device,
			                // see https://github.com/fwyzard/nvidia_bug_3446335 .
			                // Set the current device.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(m_iDstDevice));
			                // Initiate the memory copy.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::memcpyAsync(
			                    m_dstMemNative,
			                    m_srcMemNative,
			                    static_cast<std::size_t>(m_extentWidthBytes),
			                    m_uniformMemCpyKind,
			                    queue.getNativeHandle()));
			            }

			        private:
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			            ALPAKA_FN_HOST auto printDebug() const -> void
			            {
			                std::cout << __func__ << " ddev: " << m_iDstDevice << " ew: " << m_extentWidth
			                          << " ewb: " << m_extentWidthBytes << " dw: " << m_dstWidth << " dptr: " << m_dstMemNative
			                          << " sdev: " << m_iSrcDevice << " sw: " << m_srcWidth << " sptr: " << m_srcMemNative
			                          << std::endl;
			            }
			#    endif

			            typename TApi::MemcpyKind_t m_uniformMemCpyKind;
			            int m_iDstDevice;
			            int m_iSrcDevice;
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			            Idx m_extentWidth;
			            Idx m_dstWidth;
			            Idx m_srcWidth;
			#    endif
			            Idx m_extentWidthBytes;
			            void* m_dstMemNative;
			            void const* m_srcMemNative;
			        };

			        //! The 2D CUDA/HIP memory copy trait.
			        template<typename TApi, typename TViewDst, typename TViewSrc, typename TExtent>
			        struct TaskCopyUniformCudaHip<TApi, DimInt<2u>, TViewDst, TViewSrc, TExtent>
			        {
			            using Idx = alpaka::Idx<TExtent>;

			            template<typename TViewDstFwd>
			            ALPAKA_FN_HOST TaskCopyUniformCudaHip(
			                TViewDstFwd&& viewDst,
			                TViewSrc const& viewSrc,
			                TExtent const& extent,
			                typename TApi::MemcpyKind_t const& uniformMemcpyKind,
			                int const& iDstDevice,
			                int const& iSrcDevice)
			                : m_uniformMemCpyKind(uniformMemcpyKind)
			                , m_iDstDevice(iDstDevice)
			                , m_iSrcDevice(iSrcDevice)
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                , m_extentWidth(getWidth(extent))
			#    endif
			                , m_extentWidthBytes(getWidth(extent) * static_cast<Idx>(sizeof(Elem<TViewDst>)))
			                , m_dstWidth(static_cast<Idx>(getWidth(viewDst)))
			                , m_srcWidth(static_cast<Idx>(getWidth(viewSrc)))
			                , m_extentHeight(getHeight(extent))
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                , m_dstHeight(static_cast<Idx>(getHeight(viewDst)))
			                , m_srcHeight(static_cast<Idx>(getHeight(viewSrc)))
			#    endif
			                , m_dstpitchBytesX(static_cast<Idx>(getPitchBytes<Dim<TViewDst>::value - 1u>(viewDst)))
			                , m_srcpitchBytesX(static_cast<Idx>(getPitchBytes<Dim<TViewSrc>::value - 1u>(viewSrc)))
			                , m_dstPitchBytesY(
			                      static_cast<Idx>(getPitchBytes<Dim<TViewDst>::value - (2u % Dim<TViewDst>::value)>(viewDst)))
			                , m_srcPitchBytesY(
			                      static_cast<Idx>(getPitchBytes<Dim<TViewSrc>::value - (2u % Dim<TViewDst>::value)>(viewSrc)))
			                , m_dstMemNative(reinterpret_cast<void*>(getPtrNative(viewDst)))
			                , m_srcMemNative(reinterpret_cast<void const*>(getPtrNative(viewSrc)))
			            {
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                ALPAKA_ASSERT(m_extentWidth <= m_dstWidth);
			                ALPAKA_ASSERT(m_extentHeight <= m_dstHeight);
			                ALPAKA_ASSERT(m_extentWidth <= m_srcWidth);
			                ALPAKA_ASSERT(m_extentHeight <= m_srcHeight);
			                ALPAKA_ASSERT(m_extentWidthBytes <= m_dstpitchBytesX);
			                ALPAKA_ASSERT(m_extentWidthBytes <= m_srcpitchBytesX);
			#    endif
			            }

			            template<typename TQueue>
			            auto enqueue(TQueue& queue) const -> void
			            {
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                printDebug();
			#    endif
			                // This is not only an optimization but also prevents a division by zero.
			                if(m_extentWidthBytes == 0 || m_extentHeight == 0)
			                {
			                    return;
			                }

			                // cudaMemcpy variants on cudaMallocAsync'ed memory need to be called with the correct device,
			                // see https://github.com/fwyzard/nvidia_bug_3446335 .
			                // Set the current device.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(m_iDstDevice));
			                // Initiate the memory copy.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::memcpy2DAsync(
			                    m_dstMemNative,
			                    static_cast<std::size_t>(m_dstpitchBytesX),
			                    m_srcMemNative,
			                    static_cast<std::size_t>(m_srcpitchBytesX),
			                    static_cast<std::size_t>(m_extentWidthBytes),
			                    static_cast<std::size_t>(m_extentHeight),
			                    m_uniformMemCpyKind,
			                    queue.getNativeHandle()));
			            }

			        private:
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			            ALPAKA_FN_HOST auto printDebug() const -> void
			            {
			                std::cout << __func__ << " ew: " << m_extentWidth << " eh: " << m_extentHeight
			                          << " ewb: " << m_extentWidthBytes << " ddev: " << m_iDstDevice << " dw: " << m_dstWidth
			                          << " dh: " << m_dstHeight << " dptr: " << m_dstMemNative << " dpitchb: " << m_dstpitchBytesX
			                          << " sdev: " << m_iSrcDevice << " sw: " << m_srcWidth << " sh: " << m_srcHeight
			                          << " sptr: " << m_srcMemNative << " spitchb: " << m_srcpitchBytesX << std::endl;
			            }
			#    endif

			            typename TApi::MemcpyKind_t m_uniformMemCpyKind;
			            int m_iDstDevice;
			            int m_iSrcDevice;
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			            Idx m_extentWidth;
			#    endif
			            Idx m_extentWidthBytes;
			            Idx m_dstWidth;
			            Idx m_srcWidth;

			            Idx m_extentHeight;
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			            Idx m_dstHeight;
			            Idx m_srcHeight;
			#    endif
			            Idx m_dstpitchBytesX;
			            Idx m_srcpitchBytesX;
			            Idx m_dstPitchBytesY;
			            Idx m_srcPitchBytesY;

			            void* m_dstMemNative;
			            void const* m_srcMemNative;
			        };

			        //! The 3D CUDA/HIP memory copy trait.
			        template<typename TApi, typename TViewDst, typename TViewSrc, typename TExtent>
			        struct TaskCopyUniformCudaHip<TApi, DimInt<3u>, TViewDst, TViewSrc, TExtent>
			        {
			            using Idx = alpaka::Idx<TExtent>;

			            template<typename TViewDstFwd>
			            ALPAKA_FN_HOST TaskCopyUniformCudaHip(
			                TViewDstFwd&& viewDst,
			                TViewSrc const& viewSrc,
			                TExtent const& extent,
			                typename TApi::MemcpyKind_t const& uniformMemcpyKind,
			                int const& iDstDevice,
			                int const& iSrcDevice)
			                : m_uniformMemCpyKind(uniformMemcpyKind)
			                , m_iDstDevice(iDstDevice)
			                , m_iSrcDevice(iSrcDevice)
			                , m_extentWidth(getWidth(extent))
			                , m_extentWidthBytes(m_extentWidth * static_cast<Idx>(sizeof(Elem<TViewDst>)))
			                , m_dstWidth(static_cast<Idx>(getWidth(viewDst)))
			                , m_srcWidth(static_cast<Idx>(getWidth(viewSrc)))
			                , m_extentHeight(getHeight(extent))
			                , m_extentDepth(getDepth(extent))
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                , m_dstHeight(static_cast<Idx>(getHeight(viewDst)))
			                , m_srcHeight(static_cast<Idx>(getHeight(viewSrc)))
			                , m_dstDepth(static_cast<Idx>(getDepth(viewDst)))
			                , m_srcDepth(static_cast<Idx>(getDepth(viewSrc)))
			#    endif
			                , m_dstpitchBytesX(static_cast<Idx>(getPitchBytes<Dim<TViewDst>::value - 1u>(viewDst)))
			                , m_srcpitchBytesX(static_cast<Idx>(getPitchBytes<Dim<TViewSrc>::value - 1u>(viewSrc)))
			                , m_dstPitchBytesY(
			                      static_cast<Idx>(getPitchBytes<Dim<TViewDst>::value - (2u % Dim<TViewDst>::value)>(viewDst)))
			                , m_srcPitchBytesY(
			                      static_cast<Idx>(getPitchBytes<Dim<TViewSrc>::value - (2u % Dim<TViewDst>::value)>(viewSrc)))
			                , m_dstMemNative(reinterpret_cast<void*>(getPtrNative(viewDst)))
			                , m_srcMemNative(reinterpret_cast<void const*>(getPtrNative(viewSrc)))
			            {
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                ALPAKA_ASSERT(m_extentWidth <= m_dstWidth);
			                ALPAKA_ASSERT(m_extentHeight <= m_dstHeight);
			                ALPAKA_ASSERT(m_extentDepth <= m_dstDepth);
			                ALPAKA_ASSERT(m_extentWidth <= m_srcWidth);
			                ALPAKA_ASSERT(m_extentHeight <= m_srcHeight);
			                ALPAKA_ASSERT(m_extentDepth <= m_srcDepth);
			                ALPAKA_ASSERT(m_extentWidthBytes <= m_dstpitchBytesX);
			                ALPAKA_ASSERT(m_extentWidthBytes <= m_srcpitchBytesX);
			#    endif
			            }

			            template<typename TQueue>
			            auto enqueue(TQueue& queue) const -> void
			            {
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			                printDebug();
			#    endif
			                // This is not only an optimization but also prevents a division by zero.
			                if(m_extentWidthBytes == 0 || m_extentHeight == 0 || m_extentDepth == 0)
			                {
			                    return;
			                }

			                // Create the struct describing the copy.
			                typename TApi::Memcpy3DParms_t const uniformCudaHipMemCpy3DParms(buildUniformCudaHipMemcpy3DParms());

			                // cudaMemcpy variants on cudaMallocAsync'ed memory need to be called with the correct device,
			                // see https://github.com/fwyzard/nvidia_bug_3446335 .
			                // Set the current device.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::setDevice(m_iDstDevice));

			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(
			                    TApi::memcpy3DAsync(&uniformCudaHipMemCpy3DParms, queue.getNativeHandle()));
			            }

			        private:
			            ALPAKA_FN_HOST auto buildUniformCudaHipMemcpy3DParms() const -> typename TApi::Memcpy3DParms_t
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                // Fill CUDA/HIP parameter structure.
			                typename TApi::Memcpy3DParms_t memCpy3DParms;
			                memCpy3DParms.srcArray = nullptr; // Either srcArray or srcPtr.
			                memCpy3DParms.srcPos = TApi::makePos(0, 0, 0); // Optional. Offset in bytes.
			                memCpy3DParms.srcPtr = TApi::makePitchedPtr(
			                    const_cast<void*>(m_srcMemNative),
			                    static_cast<std::size_t>(m_srcpitchBytesX),
			                    static_cast<std::size_t>(m_srcWidth),
			                    static_cast<std::size_t>(m_srcPitchBytesY / m_srcpitchBytesX));
			                memCpy3DParms.dstArray = nullptr; // Either dstArray or dstPtr.
			                memCpy3DParms.dstPos = TApi::makePos(0, 0, 0); // Optional. Offset in bytes.
			                memCpy3DParms.dstPtr = TApi::makePitchedPtr(
			                    m_dstMemNative,
			                    static_cast<std::size_t>(m_dstpitchBytesX),
			                    static_cast<std::size_t>(m_dstWidth),
			                    static_cast<std::size_t>(m_dstPitchBytesY / m_dstpitchBytesX));
			                memCpy3DParms.extent = TApi::makeExtent(
			                    static_cast<std::size_t>(m_extentWidthBytes),
			                    static_cast<std::size_t>(m_extentHeight),
			                    static_cast<std::size_t>(m_extentDepth));
			                memCpy3DParms.kind = m_uniformMemCpyKind;
			                return memCpy3DParms;
			            }

			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			            ALPAKA_FN_HOST auto printDebug() const -> void
			            {
			                std::cout << __func__ << " ew: " << m_extentWidth << " eh: " << m_extentHeight
			                          << " ed: " << m_extentDepth << " ewb: " << m_extentWidthBytes << " ddev: " << m_iDstDevice
			                          << " dw: " << m_dstWidth << " dh: " << m_dstHeight << " dd: " << m_dstDepth
			                          << " dptr: " << m_dstMemNative << " dpitchb: " << m_dstpitchBytesX
			                          << " sdev: " << m_iSrcDevice << " sw: " << m_srcWidth << " sh: " << m_srcHeight
			                          << " sd: " << m_srcDepth << " sptr: " << m_srcMemNative << " spitchb: " << m_srcpitchBytesX
			                          << std::endl;
			            }
			#    endif
			            typename TApi::MemcpyKind_t m_uniformMemCpyKind;
			            int m_iDstDevice;
			            int m_iSrcDevice;

			            Idx m_extentWidth;
			            Idx m_extentWidthBytes;
			            Idx m_dstWidth;
			            Idx m_srcWidth;

			            Idx m_extentHeight;
			            Idx m_extentDepth;
			#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
			            Idx m_dstHeight;
			            Idx m_srcHeight;
			            Idx m_dstDepth;
			            Idx m_srcDepth;
			#    endif
			            Idx m_dstpitchBytesX;
			            Idx m_srcpitchBytesX;
			            Idx m_dstPitchBytesY;
			            Idx m_srcPitchBytesY;

			            void* m_dstMemNative;
			            void const* m_srcMemNative;
			        };
			    } // namespace detail

			    // Trait specializations for CreateTaskMemcpy.
			    namespace trait
			    {
			        //! The CUDA/HIP to CPU memory copy trait specialization.
			        template<typename TApi, typename TDim>
			        struct CreateTaskMemcpy<TDim, DevCpu, DevUniformCudaHipRt<TApi>>
			        {
			            template<typename TExtent, typename TViewSrc, typename TViewDstFwd>
			            ALPAKA_FN_HOST static auto createTaskMemcpy(
			                TViewDstFwd&& viewDst,
			                TViewSrc const& viewSrc,
			                TExtent const& extent) -> alpaka::detail::
			                TaskCopyUniformCudaHip<TApi, TDim, std::remove_reference_t<TViewDstFwd>, TViewSrc, TExtent>
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                auto const iDevice = getDev(viewSrc).getNativeHandle();

			                return {
			                    std::forward<TViewDstFwd>(viewDst),
			                    viewSrc,
			                    extent,
			                    TApi::memcpyDeviceToHost,
			                    iDevice,
			                    iDevice};
			            }
			        };

			        //! The CPU to CUDA/HIP memory copy trait specialization.
			        template<typename TApi, typename TDim>
			        struct CreateTaskMemcpy<TDim, DevUniformCudaHipRt<TApi>, DevCpu>
			        {
			            template<typename TExtent, typename TViewSrc, typename TViewDstFwd>
			            ALPAKA_FN_HOST static auto createTaskMemcpy(
			                TViewDstFwd&& viewDst,
			                TViewSrc const& viewSrc,
			                TExtent const& extent) -> alpaka::detail::
			                TaskCopyUniformCudaHip<TApi, TDim, std::remove_reference_t<TViewDstFwd>, TViewSrc, TExtent>
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                auto const iDevice = getDev(viewDst).getNativeHandle();

			                return {
			                    std::forward<TViewDstFwd>(viewDst),
			                    viewSrc,
			                    extent,
			                    TApi::memcpyHostToDevice,
			                    iDevice,
			                    iDevice};
			            }
			        };

			        //! The CUDA/HIP to CUDA/HIP memory copy trait specialization.
			        template<typename TApi, typename TDim>
			        struct CreateTaskMemcpy<TDim, DevUniformCudaHipRt<TApi>, DevUniformCudaHipRt<TApi>>
			        {
			            template<typename TExtent, typename TViewSrc, typename TViewDstFwd>
			            ALPAKA_FN_HOST static auto createTaskMemcpy(
			                TViewDstFwd&& viewDst,
			                TViewSrc const& viewSrc,
			                TExtent const& extent) -> alpaka::detail::
			                TaskCopyUniformCudaHip<TApi, TDim, std::remove_reference_t<TViewDstFwd>, TViewSrc, TExtent>
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                auto const iDstDevice = getDev(viewDst).getNativeHandle();

			                return {
			                    std::forward<TViewDstFwd>(viewDst),
			                    viewSrc,
			                    extent,
			                    TApi::memcpyDeviceToDevice,
			                    iDstDevice,
			                    getDev(viewSrc).getNativeHandle()};
			            }
			        };

			        //! The CUDA/HIP non-blocking device queue scalar copy enqueue trait specialization.
			        template<typename TApi, typename TExtent, typename TViewSrc, typename TViewDst>
			        struct Enqueue<
			            QueueUniformCudaHipRtNonBlocking<TApi>,
			            alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<0u>, TViewDst, TViewSrc, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
			                alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<0u>, TViewDst, TViewSrc, TExtent> const& task)
			                -> void
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                task.enqueue(queue);
			            }
			        };

			        //! The CUDA/HIP blocking device queue scalar copy enqueue trait specialization.
			        template<typename TApi, typename TExtent, typename TViewSrc, typename TViewDst>
			        struct Enqueue<
			            QueueUniformCudaHipRtBlocking<TApi>,
			            alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<0u>, TViewDst, TViewSrc, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtBlocking<TApi>& queue,
			                alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<0u>, TViewDst, TViewSrc, TExtent> const& task)
			                -> void
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                task.enqueue(queue);

			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::streamSynchronize(queue.getNativeHandle()));
			            }
			        };

			        //! The CUDA/HIP non-blocking device queue 1D copy enqueue trait specialization.
			        template<typename TApi, typename TExtent, typename TViewSrc, typename TViewDst>
			        struct Enqueue<
			            QueueUniformCudaHipRtNonBlocking<TApi>,
			            alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<1u>, TViewDst, TViewSrc, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
			                alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<1u>, TViewDst, TViewSrc, TExtent> const& task)
			                -> void
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                task.enqueue(queue);
			            }
			        };

			        //! The CUDA/HIP blocking device queue 1D copy enqueue trait specialization.
			        template<typename TApi, typename TExtent, typename TViewSrc, typename TViewDst>
			        struct Enqueue<
			            QueueUniformCudaHipRtBlocking<TApi>,
			            alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<1u>, TViewDst, TViewSrc, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtBlocking<TApi>& queue,
			                alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<1u>, TViewDst, TViewSrc, TExtent> const& task)
			                -> void
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                task.enqueue(queue);

			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::streamSynchronize(queue.getNativeHandle()));
			            }
			        };

			        //! The CUDA/HIP non-blocking device queue 2D copy enqueue trait specialization.
			        template<typename TApi, typename TExtent, typename TViewSrc, typename TViewDst>
			        struct Enqueue<
			            QueueUniformCudaHipRtNonBlocking<TApi>,
			            alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<2u>, TViewDst, TViewSrc, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
			                alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<2u>, TViewDst, TViewSrc, TExtent> const& task)
			                -> void
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                task.enqueue(queue);
			            }
			        };

			        //! The CUDA/HIP blocking device queue 2D copy enqueue trait specialization.
			        template<typename TApi, typename TExtent, typename TViewSrc, typename TViewDst>
			        struct Enqueue<
			            QueueUniformCudaHipRtBlocking<TApi>,
			            alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<2u>, TViewDst, TViewSrc, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtBlocking<TApi>& queue,
			                alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<2u>, TViewDst, TViewSrc, TExtent> const& task)
			                -> void
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                task.enqueue(queue);

			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::streamSynchronize(queue.getNativeHandle()));
			            }
			        };

			        //! The CUDA/HIP non-blocking device queue 3D copy enqueue trait specialization.
			        template<typename TApi, typename TExtent, typename TViewSrc, typename TViewDst>
			        struct Enqueue<
			            QueueUniformCudaHipRtNonBlocking<TApi>,
			            alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<3u>, TViewDst, TViewSrc, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
			                alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<3u>, TViewDst, TViewSrc, TExtent> const& task)
			                -> void
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                task.enqueue(queue);
			            }
			        };

			        //! The CUDA/HIP blocking device queue 3D copy enqueue trait specialization.
			        template<typename TApi, typename TExtent, typename TViewSrc, typename TViewDst>
			        struct Enqueue<
			            QueueUniformCudaHipRtBlocking<TApi>,
			            alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<3u>, TViewDst, TViewSrc, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtBlocking<TApi>& queue,
			                alpaka::detail::TaskCopyUniformCudaHip<TApi, DimInt<3u>, TViewDst, TViewSrc, TExtent> const& task)
			                -> void
			            {
			                ALPAKA_DEBUG_FULL_LOG_SCOPE;

			                task.enqueue(queue);

			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::streamSynchronize(queue.getNativeHandle()));
			            }
			        };
			    } // namespace trait
			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/mem/buf/uniformCudaHip/Copy.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/mem/buf/uniformCudaHip/Set.hpp ==
			// ==
			/* Copyright 2022 Benjamin Worpitz, Erik Zenker, Matthias Werner, René Widera, Andrea Bocci, Bernhard Manfred Gruber,
			 * Antonio Di Pilato
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
			// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/dim/DimIntegralConst.hpp"    // amalgamate: file already expanded
			// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/QueueUniformCudaHipRtBlocking.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/QueueUniformCudaHipRtNonBlocking.hpp"    // amalgamate: file already expanded
			// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
			// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded

			#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

			namespace alpaka
			{
			    template<typename TApi>
			    class DevUniformCudaHipRt;

			    namespace detail
			    {
			        //! The CUDA/HIP memory set task base.
			        template<typename TApi, typename TDim, typename TView, typename TExtent>
			        struct TaskSetUniformCudaHipBase
			        {
			            TaskSetUniformCudaHipBase(TView& view, std::uint8_t const& byte, TExtent const& extent)
			                : m_view(view)
			                , m_byte(byte)
			                , m_extent(extent)
			                , m_iDevice(getDev(view).getNativeHandle())
			            {
			            }

			        protected:
			            TView& m_view;
			            std::uint8_t const m_byte;
			            TExtent const m_extent;
			            std::int32_t const m_iDevice;
			        };

			        //! The CUDA/HIP memory set task.
			        template<typename TApi, typename TDim, typename TView, typename TExtent>
			        struct TaskSetUniformCudaHip;

			        //! The scalar CUDA/HIP memory set task.
			        template<typename TApi, typename TView, typename TExtent>
			        struct TaskSetUniformCudaHip<TApi, DimInt<0u>, TView, TExtent>
			            : public TaskSetUniformCudaHipBase<TApi, DimInt<0u>, TView, TExtent>
			        {
			            template<typename TViewFwd>
			            TaskSetUniformCudaHip(TViewFwd&& view, std::uint8_t const& byte, TExtent const& extent)
			                : TaskSetUniformCudaHipBase<TApi, DimInt<0u>, TView, TExtent>(
			                    std::forward<TViewFwd>(view),
			                    byte,
			                    extent)
			            {
			            }

			            template<typename TQueue>
			            auto enqueue(TQueue& queue) const -> void
			            {
			                // Initiate the memory set.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::memsetAsync(
			                    getPtrNative(this->m_view),
			                    static_cast<int>(this->m_byte),
			                    sizeof(Elem<TView>),
			                    queue.getNativeHandle()));
			            }
			        };

			        //! The 1D CUDA/HIP memory set task.
			        template<typename TApi, typename TView, typename TExtent>
			        struct TaskSetUniformCudaHip<TApi, DimInt<1u>, TView, TExtent>
			            : public TaskSetUniformCudaHipBase<TApi, DimInt<1u>, TView, TExtent>
			        {
			            template<typename TViewFwd>
			            TaskSetUniformCudaHip(TViewFwd&& view, std::uint8_t const& byte, TExtent const& extent)
			                : TaskSetUniformCudaHipBase<TApi, DimInt<1u>, TView, TExtent>(
			                    std::forward<TViewFwd>(view),
			                    byte,
			                    extent)
			            {
			            }

			            template<typename TQueue>
			            auto enqueue(TQueue& queue) const -> void
			            {
			                using Idx = Idx<TExtent>;

			                auto& view = this->m_view;
			                auto const& extent = this->m_extent;

			                auto const extentWidth = getWidth(extent);
			                ALPAKA_ASSERT(extentWidth <= getWidth(view));

			                if(extentWidth == 0)
			                {
			                    return;
			                }

			                // Initiate the memory set.
			                auto const extentWidthBytes = extentWidth * static_cast<Idx>(sizeof(Elem<TView>));
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::memsetAsync(
			                    getPtrNative(view),
			                    static_cast<int>(this->m_byte),
			                    static_cast<size_t>(extentWidthBytes),
			                    queue.getNativeHandle()));
			            }
			        };

			        //! The 2D CUDA/HIP memory set task.
			        template<typename TApi, typename TView, typename TExtent>
			        struct TaskSetUniformCudaHip<TApi, DimInt<2u>, TView, TExtent>
			            : public TaskSetUniformCudaHipBase<TApi, DimInt<2u>, TView, TExtent>
			        {
			            template<typename TViewFwd>
			            TaskSetUniformCudaHip(TViewFwd&& view, std::uint8_t const& byte, TExtent const& extent)
			                : TaskSetUniformCudaHipBase<TApi, DimInt<2u>, TView, TExtent>(
			                    std::forward<TViewFwd>(view),
			                    byte,
			                    extent)
			            {
			            }

			            template<typename TQueue>
			            auto enqueue(TQueue& queue) const -> void
			            {
			                using Idx = Idx<TExtent>;

			                auto& view = this->m_view;
			                auto const& extent = this->m_extent;

			                auto const extentWidth = getWidth(extent);
			                auto const extentHeight = getHeight(extent);

			                if(extentWidth == 0 || extentHeight == 0)
			                {
			                    return;
			                }

			                auto const extentWidthBytes = extentWidth * static_cast<Idx>(sizeof(Elem<TView>));

			#    if !defined(NDEBUG)
			                auto const dstWidth = getWidth(view);
			                auto const dstHeight = getHeight(view);
			#    endif
			                auto const dstPitchBytesX = getPitchBytes<Dim<TView>::value - 1u>(view);
			                auto const dstNativePtr = reinterpret_cast<void*>(getPtrNative(view));
			                ALPAKA_ASSERT(extentWidth <= dstWidth);
			                ALPAKA_ASSERT(extentHeight <= dstHeight);

			                // Initiate the memory set.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::memset2DAsync(
			                    dstNativePtr,
			                    static_cast<size_t>(dstPitchBytesX),
			                    static_cast<int>(this->m_byte),
			                    static_cast<size_t>(extentWidthBytes),
			                    static_cast<size_t>(extentHeight),
			                    queue.getNativeHandle()));
			            }
			        };

			        //! The 3D CUDA/HIP memory set task.
			        template<typename TApi, typename TView, typename TExtent>
			        struct TaskSetUniformCudaHip<TApi, DimInt<3u>, TView, TExtent>
			            : public TaskSetUniformCudaHipBase<TApi, DimInt<3u>, TView, TExtent>
			        {
			            template<typename TViewFwd>
			            TaskSetUniformCudaHip(TViewFwd&& view, std::uint8_t const& byte, TExtent const& extent)
			                : TaskSetUniformCudaHipBase<TApi, DimInt<3u>, TView, TExtent>(
			                    std::forward<TViewFwd>(view),
			                    byte,
			                    extent)
			            {
			            }

			            template<typename TQueue>
			            auto enqueue(TQueue& queue) const -> void
			            {
			                using Elem = alpaka::Elem<TView>;
			                using Idx = Idx<TExtent>;

			                auto& view = this->m_view;
			                auto const& extent = this->m_extent;

			                auto const extentWidth = getWidth(extent);
			                auto const extentHeight = getHeight(extent);
			                auto const extentDepth = getDepth(extent);

			                // This is not only an optimization but also prevents a division by zero.
			                if(extentWidth == 0 || extentHeight == 0 || extentDepth == 0)
			                {
			                    return;
			                }

			                auto const dstWidth = getWidth(view);
			#    if !defined(NDEBUG)
			                auto const dstHeight = getHeight(view);
			                auto const dstDepth = getDepth(view);
			#    endif
			                auto const dstPitchBytesX = getPitchBytes<Dim<TView>::value - 1u>(view);
			                auto const dstPitchBytesY = getPitchBytes<Dim<TView>::value - (2u % Dim<TView>::value)>(view);
			                auto const dstNativePtr = reinterpret_cast<void*>(getPtrNative(view));
			                ALPAKA_ASSERT(extentWidth <= dstWidth);
			                ALPAKA_ASSERT(extentHeight <= dstHeight);
			                ALPAKA_ASSERT(extentDepth <= dstDepth);

			                // Fill CUDA parameter structures.
			                typename TApi::PitchedPtr_t const pitchedPtrVal = TApi::makePitchedPtr(
			                    dstNativePtr,
			                    static_cast<size_t>(dstPitchBytesX),
			                    static_cast<size_t>(dstWidth * static_cast<Idx>(sizeof(Elem))),
			                    static_cast<size_t>(dstPitchBytesY / dstPitchBytesX));

			                typename TApi::Extent_t const extentVal = TApi::makeExtent(
			                    static_cast<size_t>(extentWidth * static_cast<Idx>(sizeof(Elem))),
			                    static_cast<size_t>(extentHeight),
			                    static_cast<size_t>(extentDepth));

			                // Initiate the memory set.
			                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::memset3DAsync(
			                    pitchedPtrVal,
			                    static_cast<int>(this->m_byte),
			                    extentVal,
			                    queue.getNativeHandle()));
			            }
			        };
			    } // namespace detail

			    namespace trait
			    {
			        //! The CUDA device memory set trait specialization.
			        template<typename TApi, typename TDim>
			        struct CreateTaskMemset<TDim, DevUniformCudaHipRt<TApi>>
			        {
			            template<typename TExtent, typename TView>
			            ALPAKA_FN_HOST static auto createTaskMemset(TView& view, std::uint8_t const& byte, TExtent const& extent)
			                -> alpaka::detail::TaskSetUniformCudaHip<TApi, TDim, TView, TExtent>
			            {
			                return alpaka::detail::TaskSetUniformCudaHip<TApi, TDim, TView, TExtent>(view, byte, extent);
			            }
			        };

			        //! The CUDA non-blocking device queue scalar set enqueue trait specialization.
			        template<typename TApi, typename TView, typename TExtent>
			        struct Enqueue<
			            QueueUniformCudaHipRtNonBlocking<TApi>,
			            alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<0u>, TView, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
			                alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<0u>, TView, TExtent> const& task) -> void
			            {
			                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

			                task.enqueue(queue);
			            }
			        };

			        //! The CUDA blocking device queue scalar set enqueue trait specialization.
			        template<typename TApi, typename TView, typename TExtent>
			        struct Enqueue<
			            QueueUniformCudaHipRtBlocking<TApi>,
			            alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<0u>, TView, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtBlocking<TApi>& queue,
			                alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<0u>, TView, TExtent> const& task) -> void
			            {
			                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

			                task.enqueue(queue);

			                wait(queue);
			            }
			        };

			        //! The CUDA non-blocking device queue 1D set enqueue trait specialization.
			        template<typename TApi, typename TView, typename TExtent>
			        struct Enqueue<
			            QueueUniformCudaHipRtNonBlocking<TApi>,
			            alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<1u>, TView, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
			                alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<1u>, TView, TExtent> const& task) -> void
			            {
			                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

			                task.enqueue(queue);
			            }
			        };

			        //! The CUDA blocking device queue 1D set enqueue trait specialization.
			        template<typename TApi, typename TView, typename TExtent>
			        struct Enqueue<
			            QueueUniformCudaHipRtBlocking<TApi>,
			            alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<1u>, TView, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtBlocking<TApi>& queue,
			                alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<1u>, TView, TExtent> const& task) -> void
			            {
			                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

			                task.enqueue(queue);

			                wait(queue);
			            }
			        };

			        //! The CUDA non-blocking device queue 2D set enqueue trait specialization.
			        template<typename TApi, typename TView, typename TExtent>
			        struct Enqueue<
			            QueueUniformCudaHipRtNonBlocking<TApi>,
			            alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<2u>, TView, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
			                alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<2u>, TView, TExtent> const& task) -> void
			            {
			                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

			                task.enqueue(queue);
			            }
			        };

			        //! The CUDA blocking device queue 2D set enqueue trait specialization.
			        template<typename TApi, typename TView, typename TExtent>
			        struct Enqueue<
			            QueueUniformCudaHipRtBlocking<TApi>,
			            alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<2u>, TView, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtBlocking<TApi>& queue,
			                alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<2u>, TView, TExtent> const& task) -> void
			            {
			                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

			                task.enqueue(queue);

			                wait(queue);
			            }
			        };

			        //! The CUDA non-blocking device queue 3D set enqueue trait specialization.
			        template<typename TApi, typename TView, typename TExtent>
			        struct Enqueue<
			            QueueUniformCudaHipRtNonBlocking<TApi>,
			            alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<3u>, TView, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtNonBlocking<TApi>& queue,
			                alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<3u>, TView, TExtent> const& task) -> void
			            {
			                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

			                task.enqueue(queue);
			            }
			        };

			        //! The CUDA blocking device queue 3D set enqueue trait specialization.
			        template<typename TApi, typename TView, typename TExtent>
			        struct Enqueue<
			            QueueUniformCudaHipRtBlocking<TApi>,
			            alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<3u>, TView, TExtent>>
			        {
			            ALPAKA_FN_HOST static auto enqueue(
			                QueueUniformCudaHipRtBlocking<TApi>& queue,
			                alpaka::detail::TaskSetUniformCudaHip<TApi, DimInt<3u>, TView, TExtent> const& task) -> void
			            {
			                ALPAKA_DEBUG_MINIMAL_LOG_SCOPE;

			                task.enqueue(queue);

			                wait(queue);
			            }
			        };
			    } // namespace trait
			} // namespace alpaka

			#endif
			// ==
			// == ./include/alpaka/mem/buf/uniformCudaHip/Set.hpp ==
			// ============================================================================


		#endif
		// ==
		// == ./include/alpaka/mem/buf/BufUniformCudaHipRt.hpp ==
		// ============================================================================


	#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

	namespace alpaka
	{
	    template<typename TElem, typename TDim, typename TIdx>
	    using BufCudaRt = BufUniformCudaHipRt<ApiCudaRt, TElem, TDim, TIdx>;
	}

	#endif // ALPAKA_ACC_GPU_CUDA_ENABLED
	// ==
	// == ./include/alpaka/mem/buf/BufCudaRt.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/mem/buf/BufFpgaSyclIntel.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */


	// #pragma once
	// #include "alpaka/dev/DevFpgaSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/buf/BufGenericSycl.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_FPGA)

	namespace alpaka
	{
	    template<typename TElem, typename TDim, typename TIdx>
	    using BufFpgaSyclIntel = BufGenericSycl<TElem, TDim, TIdx, DevFpgaSyclIntel>;
	}

	#endif
	// ==
	// == ./include/alpaka/mem/buf/BufFpgaSyclIntel.hpp ==
	// ============================================================================

// #include "alpaka/mem/buf/BufGenericSycl.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/mem/buf/BufGpuSyclIntel.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevGpuSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/buf/BufGenericSycl.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_GPU)

	namespace alpaka
	{
	    template<typename TElem, typename TDim, typename TIdx>
	    using BufGpuSyclIntel = BufGenericSycl<TElem, TDim, TIdx, DevGpuSyclIntel>;
	}

	#endif
	// ==
	// == ./include/alpaka/mem/buf/BufGpuSyclIntel.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/mem/buf/BufHipRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/buf/BufUniformCudaHipRt.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_HIP_ENABLED

	namespace alpaka
	{
	    template<typename TElem, typename TDim, typename TIdx>
	    using BufHipRt = BufUniformCudaHipRt<ApiHipRt, TElem, TDim, TIdx>;
	}

	#endif // ALPAKA_ACC_GPU_HIP_ENABLED
	// ==
	// == ./include/alpaka/mem/buf/BufHipRt.hpp ==
	// ============================================================================

// #include "alpaka/mem/buf/Traits.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/fence/MemFenceCpu.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/fence/MemFenceCpuSerial.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/fence/MemFenceGenericSycl.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/fence/MemFenceOmp2Blocks.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/fence/MemFenceOmp2Threads.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/fence/MemFenceUniformCudaHipBuiltIn.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/fence/Traits.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/view/Accessor.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
// #include "alpaka/mem/view/ViewAccessor.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/mem/view/ViewConst.hpp ==
	// ==
	/* Copyright 2022 Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/view/ViewAccessOps.hpp"    // amalgamate: file already expanded
	// #include "alpaka/offset/Traits.hpp"    // amalgamate: file already expanded

	namespace alpaka
	{
	    //! A non-modifiable wrapper around a view. This view acts as the wrapped view, but the underlying data is only
	    //! exposed const-qualified.
	    template<typename TView>
	    struct ViewConst : internal::ViewAccessOps<ViewConst<TView>>
	    {
	        static_assert(!std::is_const_v<TView>, "ViewConst must be instantiated with a non-const type");
	        static_assert(
	            !std::is_reference_v<TView>,
	            "This is not implemented"); // It might even be dangerous for ViewConst to store a reference to the wrapped
	                                        // view, as this decouples the wrapped view's lifetime.

	        ALPAKA_FN_HOST ViewConst(TView const& view) : m_view(view)
	        {
	        }

	        ALPAKA_FN_HOST ViewConst(TView&& view) : m_view(std::move(view))
	        {
	        }

	        TView m_view;
	    };

	    template<typename TView>
	    ViewConst(TView) -> ViewConst<std::decay_t<TView>>;

	    namespace trait
	    {
	        template<typename TView>
	        struct DevType<ViewConst<TView>> : DevType<TView>
	        {
	        };

	        template<typename TView>
	        struct GetDev<ViewConst<TView>>
	        {
	            ALPAKA_FN_HOST static auto getDev(ViewConst<TView> const& view)
	            {
	                return alpaka::getDev(view.m_view);
	            }
	        };

	        template<typename TView>
	        struct DimType<ViewConst<TView>> : DimType<TView>
	        {
	        };

	        template<typename TView>
	        struct ElemType<ViewConst<TView>>
	        {
	            // const qualify the element type of the inner view
	            using type = typename ElemType<TView>::type const;
	        };

	        template<typename I, typename TView>
	        struct GetExtent<I, ViewConst<TView>>
	        {
	            ALPAKA_FN_HOST static auto getExtent(ViewConst<TView> const& view)
	            {
	                return alpaka::getExtent<I::value>(view.m_view);
	            }
	        };

	        template<typename TView>
	        struct GetPtrNative<ViewConst<TView>>
	        {
	            using TElem = typename ElemType<TView>::type;

	            // const qualify the element type of the inner view
	            ALPAKA_FN_HOST static auto getPtrNative(ViewConst<TView> const& view) -> TElem const*
	            {
	                return alpaka::getPtrNative(view.m_view);
	            }
	        };

	        template<typename I, typename TView>
	        struct GetPitchBytes<I, ViewConst<TView>>
	        {
	            ALPAKA_FN_HOST static auto getPitchBytes(ViewConst<TView> const& view)
	            {
	                return alpaka::getPitchBytes<I::value>(view.m_view);
	            }
	        };

	        template<typename I, typename TView>
	        struct GetOffset<I, ViewConst<TView>>
	        {
	            ALPAKA_FN_HOST static auto getOffset(ViewConst<TView> const& view)
	            {
	                return alpaka::getOffset<I::value>(view.m_view);
	            }
	        };

	        template<typename TView>
	        struct IdxType<ViewConst<TView>> : IdxType<TView>
	        {
	        };
	    } // namespace trait
	} // namespace alpaka
	// ==
	// == ./include/alpaka/mem/view/ViewConst.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/mem/view/ViewPlainPtr.hpp ==
	// ==
	/* Copyright 2023 Benjamin Worpitz, Matthias Werner, René Widera, Bernhard Manfred Gruber, Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/view/ViewAccessOps.hpp"    // amalgamate: file already expanded
	// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

	// #include <type_traits>    // amalgamate: file already included
	// #include <utility>    // amalgamate: file already included

	namespace alpaka
	{
	    //! The memory view to wrap plain pointers.
	    template<typename TDev, typename TElem, typename TDim, typename TIdx>
	    class ViewPlainPtr final : public internal::ViewAccessOps<ViewPlainPtr<TDev, TElem, TDim, TIdx>>
	    {
	        static_assert(!std::is_const_v<TIdx>, "The idx type of the view can not be const!");

	        using Dev = alpaka::Dev<TDev>;

	    public:
	        template<typename TExtent>
	        ALPAKA_FN_HOST ViewPlainPtr(TElem* pMem, Dev dev, TExtent const& extent = TExtent())
	            : m_pMem(pMem)
	            , m_dev(std::move(dev))
	            , m_extentElements(getExtentVecEnd<TDim>(extent))
	            , m_pitchBytes(detail::calculatePitchesFromExtents<TElem>(m_extentElements))
	        {
	        }

	        template<typename TExtent, typename TPitch>
	        ALPAKA_FN_HOST ViewPlainPtr(TElem* pMem, Dev const dev, TExtent const& extent, TPitch const& pitchBytes)
	            : m_pMem(pMem)
	            , m_dev(dev)
	            , m_extentElements(getExtentVecEnd<TDim>(extent))
	            , m_pitchBytes(subVecEnd<TDim>(static_cast<Vec<TDim, TIdx>>(pitchBytes)))
	        {
	        }

	        ViewPlainPtr(ViewPlainPtr const&) = default;
	        ALPAKA_FN_HOST
	        ViewPlainPtr(ViewPlainPtr&& other) noexcept
	            : m_pMem(other.m_pMem)
	            , m_dev(other.m_dev)
	            , m_extentElements(other.m_extentElements)
	            , m_pitchBytes(other.m_pitchBytes)
	        {
	        }
	        ALPAKA_FN_HOST
	        auto operator=(ViewPlainPtr const&) -> ViewPlainPtr& = delete;
	        ALPAKA_FN_HOST
	        auto operator=(ViewPlainPtr&&) -> ViewPlainPtr& = delete;

	    public:
	        TElem* const m_pMem;
	        Dev const m_dev;
	        Vec<TDim, TIdx> const m_extentElements;
	        Vec<TDim, TIdx> const m_pitchBytes;
	    };

	    // Trait specializations for ViewPlainPtr.
	    namespace trait
	    {
	        //! The ViewPlainPtr device type trait specialization.
	        template<typename TDev, typename TElem, typename TDim, typename TIdx>
	        struct DevType<ViewPlainPtr<TDev, TElem, TDim, TIdx>>
	        {
	            using type = alpaka::Dev<TDev>;
	        };

	        //! The ViewPlainPtr device get trait specialization.
	        template<typename TDev, typename TElem, typename TDim, typename TIdx>
	        struct GetDev<ViewPlainPtr<TDev, TElem, TDim, TIdx>>
	        {
	            static auto getDev(ViewPlainPtr<TDev, TElem, TDim, TIdx> const& view) -> alpaka::Dev<TDev>
	            {
	                return view.m_dev;
	            }
	        };

	        //! The ViewPlainPtr dimension getter trait.
	        template<typename TDev, typename TElem, typename TDim, typename TIdx>
	        struct DimType<ViewPlainPtr<TDev, TElem, TDim, TIdx>>
	        {
	            using type = TDim;
	        };

	        //! The ViewPlainPtr memory element type get trait specialization.
	        template<typename TDev, typename TElem, typename TDim, typename TIdx>
	        struct ElemType<ViewPlainPtr<TDev, TElem, TDim, TIdx>>
	        {
	            using type = TElem;
	        };
	    } // namespace trait
	    namespace trait
	    {
	        //! The ViewPlainPtr width get trait specialization.
	        template<typename TIdxIntegralConst, typename TDev, typename TElem, typename TDim, typename TIdx>
	        struct GetExtent<
	            TIdxIntegralConst,
	            ViewPlainPtr<TDev, TElem, TDim, TIdx>,
	            std::enable_if_t<(TDim::value > TIdxIntegralConst::value)>>
	        {
	            ALPAKA_FN_HOST
	            static auto getExtent(ViewPlainPtr<TDev, TElem, TDim, TIdx> const& extent) -> TIdx
	            {
	                return extent.m_extentElements[TIdxIntegralConst::value];
	            }
	        };
	    } // namespace trait

	    namespace trait
	    {
	        //! The ViewPlainPtr native pointer get trait specialization.
	        template<typename TDev, typename TElem, typename TDim, typename TIdx>
	        struct GetPtrNative<ViewPlainPtr<TDev, TElem, TDim, TIdx>>
	        {
	            static auto getPtrNative(ViewPlainPtr<TDev, TElem, TDim, TIdx> const& view) -> TElem const*
	            {
	                return view.m_pMem;
	            }
	            static auto getPtrNative(ViewPlainPtr<TDev, TElem, TDim, TIdx>& view) -> TElem*
	            {
	                return view.m_pMem;
	            }
	        };

	        //! The ViewPlainPtr memory pitch get trait specialization.
	        template<typename TIdxIntegralConst, typename TDev, typename TElem, typename TDim, typename TIdx>
	            struct GetPitchBytes < TIdxIntegralConst,
	            ViewPlainPtr<TDev, TElem, TDim, TIdx>, std::enable_if_t<TIdxIntegralConst::value<TDim::value>>
	        {
	            ALPAKA_FN_HOST static auto getPitchBytes(ViewPlainPtr<TDev, TElem, TDim, TIdx> const& view) -> TIdx
	            {
	                return view.m_pitchBytes[TIdxIntegralConst::value];
	            }
	        };

	        //! The CPU device CreateStaticDevMemView trait specialization.
	        template<>
	        struct CreateStaticDevMemView<DevCpu>
	        {
	            template<typename TElem, typename TExtent>
	            static auto createStaticDevMemView(TElem* pMem, DevCpu const& dev, TExtent const& extent)
	            {
	                return alpaka::ViewPlainPtr<DevCpu, TElem, alpaka::Dim<TExtent>, alpaka::Idx<TExtent>>(
	                    pMem,
	                    dev,
	                    extent);
	            }
	        };

	#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)
	        //! The CUDA/HIP RT device CreateStaticDevMemView trait specialization.
	        template<typename TApi>
	        struct CreateStaticDevMemView<DevUniformCudaHipRt<TApi>>
	        {
	            template<typename TElem, typename TExtent>
	            static auto createStaticDevMemView(
	                TElem* pMem,
	                DevUniformCudaHipRt<TApi> const& dev,
	                TExtent const& extent)
	            {
	                TElem* pMemAcc(nullptr);
	                ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::getSymbolAddress(reinterpret_cast<void**>(&pMemAcc), *pMem));

	                return alpaka::
	                    ViewPlainPtr<DevUniformCudaHipRt<TApi>, TElem, alpaka::Dim<TExtent>, alpaka::Idx<TExtent>>(
	                        pMemAcc,
	                        dev,
	                        extent);
	            }
	        };
	#endif

	        //! The CPU device CreateViewPlainPtr trait specialization.
	        template<>
	        struct CreateViewPlainPtr<DevCpu>
	        {
	            template<typename TElem, typename TExtent, typename TPitch>
	            static auto createViewPlainPtr(DevCpu const& dev, TElem* pMem, TExtent const& extent, TPitch const& pitch)
	            {
	                return alpaka::ViewPlainPtr<DevCpu, TElem, alpaka::Dim<TExtent>, alpaka::Idx<TExtent>>(
	                    pMem,
	                    dev,
	                    extent,
	                    pitch);
	            }
	        };

	#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)
	        //! The CUDA/HIP RT device CreateViewPlainPtr trait specialization.
	        template<typename TApi>
	        struct CreateViewPlainPtr<DevUniformCudaHipRt<TApi>>
	        {
	            template<typename TElem, typename TExtent, typename TPitch>
	            static auto createViewPlainPtr(
	                DevUniformCudaHipRt<TApi> const& dev,
	                TElem* pMem,
	                TExtent const& extent,
	                TPitch const& pitch)
	            {
	                return alpaka::
	                    ViewPlainPtr<DevUniformCudaHipRt<TApi>, TElem, alpaka::Dim<TExtent>, alpaka::Idx<TExtent>>(
	                        pMem,
	                        dev,
	                        extent,
	                        pitch);
	            }
	        };
	#endif

	        //! The ViewPlainPtr offset get trait specialization.
	        template<typename TIdxIntegralConst, typename TDev, typename TElem, typename TDim, typename TIdx>
	        struct GetOffset<TIdxIntegralConst, ViewPlainPtr<TDev, TElem, TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST
	            static auto getOffset(ViewPlainPtr<TDev, TElem, TDim, TIdx> const&) -> TIdx
	            {
	                return 0u;
	            }
	        };

	        //! The ViewPlainPtr idx type trait specialization.
	        template<typename TDev, typename TElem, typename TDim, typename TIdx>
	        struct IdxType<ViewPlainPtr<TDev, TElem, TDim, TIdx>>
	        {
	            using type = TIdx;
	        };
	    } // namespace trait
	} // namespace alpaka
	// ==
	// == ./include/alpaka/mem/view/ViewPlainPtr.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/mem/view/ViewStdArray.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	/* TODO: Once C++20 is available remove this file and replace with a generic ContiguousContainer solution based on
	 * concepts. It should be sufficient to check for the existence of Container.size() and Container.data() */

	// #pragma once
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/pltf/PltfCpu.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
		// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded

		// #include <sstream>    // amalgamate: file already included
		// #include <vector>    // amalgamate: file already included

		namespace alpaka
		{
		    //! The CPU device platform.
		    class PltfCpu : public concepts::Implements<ConceptPltf, PltfCpu>
		    {
		    public:
		        ALPAKA_FN_HOST PltfCpu() = delete;
		    };

		    namespace trait
		    {
		        //! The CPU device device type trait specialization.
		        template<>
		        struct DevType<PltfCpu>
		        {
		            using type = DevCpu;
		        };

		        //! The CPU platform device count get trait specialization.
		        template<>
		        struct GetDevCount<PltfCpu>
		        {
		            ALPAKA_FN_HOST static auto getDevCount() -> std::size_t
		            {
		                ALPAKA_DEBUG_FULL_LOG_SCOPE;

		                return 1;
		            }
		        };

		        //! The CPU platform device get trait specialization.
		        template<>
		        struct GetDevByIdx<PltfCpu>
		        {
		            ALPAKA_FN_HOST static auto getDevByIdx(std::size_t const& devIdx) -> DevCpu
		            {
		                ALPAKA_DEBUG_FULL_LOG_SCOPE;

		                std::size_t const devCount(getDevCount<PltfCpu>());
		                if(devIdx >= devCount)
		                {
		                    std::stringstream ssErr;
		                    ssErr << "Unable to return device handle for CPU device with index " << devIdx
		                          << " because there are only " << devCount << " devices!";
		                    throw std::runtime_error(ssErr.str());
		                }

		                return {};
		            }
		        };
		    } // namespace trait
		} // namespace alpaka
		// ==
		// == ./include/alpaka/pltf/PltfCpu.hpp ==
		// ============================================================================


	// #include <array>    // amalgamate: file already included

	namespace alpaka::trait
	{
	    //! The std::array device type trait specialization.
	    template<typename TElem, std::size_t Tsize>
	    struct DevType<std::array<TElem, Tsize>>
	    {
	        using type = DevCpu;
	    };

	    //! The std::array device get trait specialization.
	    template<typename TElem, std::size_t Tsize>
	    struct GetDev<std::array<TElem, Tsize>>
	    {
	        ALPAKA_FN_HOST static auto getDev(std::array<TElem, Tsize> const& /* view */) -> DevCpu
	        {
	            return getDevByIdx<PltfCpu>(0u);
	        }
	    };

	    //! The std::array dimension getter trait specialization.
	    template<typename TElem, std::size_t Tsize>
	    struct DimType<std::array<TElem, Tsize>>
	    {
	        using type = DimInt<1u>;
	    };

	    //! The std::array memory element type get trait specialization.
	    template<typename TElem, std::size_t Tsize>
	    struct ElemType<std::array<TElem, Tsize>>
	    {
	        using type = TElem;
	    };

	    //! The std::array width get trait specialization.
	    template<typename TElem, std::size_t Tsize>
	    struct GetExtent<DimInt<0u>, std::array<TElem, Tsize>>
	    {
	        ALPAKA_FN_HOST static constexpr auto getExtent(std::array<TElem, Tsize> const& extent)
	            -> Idx<std::array<TElem, Tsize>>
	        {
	            return std::size(extent);
	        }
	    };

	    //! The std::array native pointer get trait specialization.
	    template<typename TElem, std::size_t Tsize>
	    struct GetPtrNative<std::array<TElem, Tsize>>
	    {
	        ALPAKA_FN_HOST static auto getPtrNative(std::array<TElem, Tsize> const& view) -> TElem const*
	        {
	            return std::data(view);
	        }
	        ALPAKA_FN_HOST static auto getPtrNative(std::array<TElem, Tsize>& view) -> TElem*
	        {
	            return std::data(view);
	        }
	    };

	    //! The std::array offset get trait specialization.
	    template<typename TIdx, typename TElem, std::size_t Tsize>
	    struct GetOffset<TIdx, std::array<TElem, Tsize>>
	    {
	        ALPAKA_FN_HOST static auto getOffset(std::array<TElem, Tsize> const&) -> Idx<std::array<TElem, Tsize>>
	        {
	            return 0u;
	        }
	    };

	    //! The std::vector idx type trait specialization.
	    template<typename TElem, std::size_t Tsize>
	    struct IdxType<std::array<TElem, Tsize>>
	    {
	        using type = std::size_t;
	    };
	} // namespace alpaka::trait
	// ==
	// == ./include/alpaka/mem/view/ViewStdArray.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/mem/view/ViewStdVector.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	/* TODO: Once C++20 is available remove this file and replace with a generic ContiguousContainer solution based on
	 * concepts. It should be sufficient to check for the existence of Container.size() and Container.data() */

	// #pragma once
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/DevCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/PltfCpu.hpp"    // amalgamate: file already expanded

	// #include <vector>    // amalgamate: file already included

	namespace alpaka::trait
	{
	    //! The std::vector device type trait specialization.
	    template<typename TElem, typename TAllocator>
	    struct DevType<std::vector<TElem, TAllocator>>
	    {
	        using type = DevCpu;
	    };

	    //! The std::vector device get trait specialization.
	    template<typename TElem, typename TAllocator>
	    struct GetDev<std::vector<TElem, TAllocator>>
	    {
	        ALPAKA_FN_HOST static auto getDev(std::vector<TElem, TAllocator> const& /* view */) -> DevCpu
	        {
	            return getDevByIdx<PltfCpu>(0u);
	        }
	    };

	    //! The std::vector dimension getter trait specialization.
	    template<typename TElem, typename TAllocator>
	    struct DimType<std::vector<TElem, TAllocator>>
	    {
	        using type = DimInt<1u>;
	    };

	    //! The std::vector memory element type get trait specialization.
	    template<typename TElem, typename TAllocator>
	    struct ElemType<std::vector<TElem, TAllocator>>
	    {
	        using type = TElem;
	    };

	    //! The std::vector width get trait specialization.
	    template<typename TElem, typename TAllocator>
	    struct GetExtent<DimInt<0u>, std::vector<TElem, TAllocator>>
	    {
	        ALPAKA_FN_HOST static auto getExtent(std::vector<TElem, TAllocator> const& extent)
	            -> Idx<std::vector<TElem, TAllocator>>
	        {
	            return std::size(extent);
	        }
	    };

	    //! The std::vector native pointer get trait specialization.
	    template<typename TElem, typename TAllocator>
	    struct GetPtrNative<std::vector<TElem, TAllocator>>
	    {
	        ALPAKA_FN_HOST static auto getPtrNative(std::vector<TElem, TAllocator> const& view) -> TElem const*
	        {
	            return std::data(view);
	        }
	        ALPAKA_FN_HOST static auto getPtrNative(std::vector<TElem, TAllocator>& view) -> TElem*
	        {
	            return std::data(view);
	        }
	    };

	    //! The std::vector offset get trait specialization.
	    template<typename TIdx, typename TElem, typename TAllocator>
	    struct GetOffset<TIdx, std::vector<TElem, TAllocator>>
	    {
	        ALPAKA_FN_HOST static auto getOffset(std::vector<TElem, TAllocator> const&)
	            -> Idx<std::vector<TElem, TAllocator>>
	        {
	            return 0u;
	        }
	    };

	    //! The std::vector idx type trait specialization.
	    template<typename TElem, typename TAllocator>
	    struct IdxType<std::vector<TElem, TAllocator>>
	    {
	        using type = std::size_t;
	    };
	} // namespace alpaka::trait
	// ==
	// == ./include/alpaka/mem/view/ViewStdVector.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/mem/view/ViewSubView.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Matthias Werner, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/Assert.hpp"    // amalgamate: file already expanded
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/dim/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/extent/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/view/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/view/ViewAccessOps.hpp"    // amalgamate: file already expanded
	// #include "alpaka/mem/view/ViewPlainPtr.hpp"    // amalgamate: file already expanded
	// #include "alpaka/offset/Traits.hpp"    // amalgamate: file already expanded
	// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded

	// #include <type_traits>    // amalgamate: file already included
	// #include <utility>    // amalgamate: file already included

	namespace alpaka
	{
	    //! A sub-view to a view.
	    template<typename TDev, typename TElem, typename TDim, typename TIdx>
	    class ViewSubView : public internal::ViewAccessOps<ViewSubView<TDev, TElem, TDim, TIdx>>
	    {
	        static_assert(!std::is_const_v<TIdx>, "The idx type of the view can not be const!");

	        using Dev = alpaka::Dev<TDev>;

	    public:
	        //! Constructor.
	        //! \param view The view this view is a sub-view of.
	        //! \param extentElements The extent in elements.
	        //! \param relativeOffsetsElements The offsets in elements.
	        template<typename TView, typename TOffsets, typename TExtent>
	        ViewSubView(
	            TView const& view,
	            TExtent const& extentElements,
	            TOffsets const& relativeOffsetsElements = TOffsets())
	            : m_viewParentView(getPtrNative(view), getDev(view), getExtentVec(view), getPitchBytesVec(view))
	            , m_extentElements(getExtentVec(extentElements))
	            , m_offsetsElements(getOffsetVec(relativeOffsetsElements))
	        {
	            ALPAKA_DEBUG_FULL_LOG_SCOPE;

	            static_assert(
	                std::is_same_v<Dev, alpaka::Dev<TView>>,
	                "The dev type of TView and the Dev template parameter have to be identical!");

	            static_assert(
	                std::is_same_v<TIdx, Idx<TView>>,
	                "The idx type of TView and the TIdx template parameter have to be identical!");
	            static_assert(
	                std::is_same_v<TIdx, Idx<TExtent>>,
	                "The idx type of TExtent and the TIdx template parameter have to be identical!");
	            static_assert(
	                std::is_same_v<TIdx, Idx<TOffsets>>,
	                "The idx type of TOffsets and the TIdx template parameter have to be identical!");

	            static_assert(
	                std::is_same_v<TDim, Dim<TView>>,
	                "The dim type of TView and the TDim template parameter have to be identical!");
	            static_assert(
	                std::is_same_v<TDim, Dim<TExtent>>,
	                "The dim type of TExtent and the TDim template parameter have to be identical!");
	            static_assert(
	                std::is_same_v<TDim, Dim<TOffsets>>,
	                "The dim type of TOffsets and the TDim template parameter have to be identical!");

	            ALPAKA_ASSERT(((m_offsetsElements + m_extentElements) <= getExtentVec(view))
	                              .foldrAll(std::logical_and<bool>(), true));
	        }
	        //! Constructor.
	        //! \param view The view this view is a sub-view of.
	        //! \param extentElements The extent in elements.
	        //! \param relativeOffsetsElements The offsets in elements.
	        template<typename TView, typename TOffsets, typename TExtent>
	        ViewSubView(TView& view, TExtent const& extentElements, TOffsets const& relativeOffsetsElements = TOffsets())
	            : m_viewParentView(getPtrNative(view), getDev(view), getExtentVec(view), getPitchBytesVec(view))
	            , m_extentElements(getExtentVec(extentElements))
	            , m_offsetsElements(getOffsetVec(relativeOffsetsElements))
	        {
	            ALPAKA_DEBUG_FULL_LOG_SCOPE;

	            static_assert(
	                std::is_same_v<Dev, alpaka::Dev<TView>>,
	                "The dev type of TView and the Dev template parameter have to be identical!");

	            static_assert(
	                std::is_same_v<TIdx, Idx<TView>>,
	                "The idx type of TView and the TIdx template parameter have to be identical!");
	            static_assert(
	                std::is_same_v<TIdx, Idx<TExtent>>,
	                "The idx type of TExtent and the TIdx template parameter have to be identical!");
	            static_assert(
	                std::is_same_v<TIdx, Idx<TOffsets>>,
	                "The idx type of TOffsets and the TIdx template parameter have to be identical!");

	            static_assert(
	                std::is_same_v<TDim, Dim<TView>>,
	                "The dim type of TView and the TDim template parameter have to be identical!");
	            static_assert(
	                std::is_same_v<TDim, Dim<TExtent>>,
	                "The dim type of TExtent and the TDim template parameter have to be identical!");
	            static_assert(
	                std::is_same_v<TDim, Dim<TOffsets>>,
	                "The dim type of TOffsets and the TDim template parameter have to be identical!");

	            ALPAKA_ASSERT(((m_offsetsElements + m_extentElements) <= getExtentVec(view))
	                              .foldrAll(std::logical_and<bool>(), true));
	        }

	        //! \param view The view this view is a sub-view of.
	        template<typename TView>
	        explicit ViewSubView(TView const& view) : ViewSubView(view, view, Vec<TDim, TIdx>::all(0))
	        {
	            ALPAKA_DEBUG_FULL_LOG_SCOPE;
	        }

	        //! \param view The view this view is a sub-view of.
	        template<typename TView>
	        explicit ViewSubView(TView& view) : ViewSubView(view, view, Vec<TDim, TIdx>::all(0))
	        {
	            ALPAKA_DEBUG_FULL_LOG_SCOPE;
	        }

	    public:
	        ViewPlainPtr<Dev, TElem, TDim, TIdx> m_viewParentView; // This wraps the parent view.
	        Vec<TDim, TIdx> m_extentElements; // The extent of this view.
	        Vec<TDim, TIdx> m_offsetsElements; // The offset relative to the parent view.
	    };

	    // Trait specializations for ViewSubView.
	    namespace trait
	    {
	        //! The ViewSubView device type trait specialization.
	        template<typename TElem, typename TDim, typename TDev, typename TIdx>
	        struct DevType<ViewSubView<TDev, TElem, TDim, TIdx>>
	        {
	            using type = alpaka::Dev<TDev>;
	        };

	        //! The ViewSubView device get trait specialization.
	        template<typename TElem, typename TDim, typename TDev, typename TIdx>
	        struct GetDev<ViewSubView<TDev, TElem, TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getDev(ViewSubView<TDev, TElem, TDim, TIdx> const& view) -> alpaka::Dev<TDev>
	            {
	                return alpaka::getDev(view.m_viewParentView);
	            }
	        };

	        //! The ViewSubView dimension getter trait specialization.
	        template<typename TElem, typename TDim, typename TDev, typename TIdx>
	        struct DimType<ViewSubView<TDev, TElem, TDim, TIdx>>
	        {
	            using type = TDim;
	        };

	        //! The ViewSubView memory element type get trait specialization.
	        template<typename TElem, typename TDim, typename TDev, typename TIdx>
	        struct ElemType<ViewSubView<TDev, TElem, TDim, TIdx>>
	        {
	            using type = TElem;
	        };

	        //! The ViewSubView width get trait specialization.
	        template<typename TIdxIntegralConst, typename TElem, typename TDim, typename TDev, typename TIdx>
	        struct GetExtent<
	            TIdxIntegralConst,
	            ViewSubView<TDev, TElem, TDim, TIdx>,
	            std::enable_if_t<(TDim::value > TIdxIntegralConst::value)>>
	        {
	            ALPAKA_FN_HOST static auto getExtent(ViewSubView<TDev, TElem, TDim, TIdx> const& extent) -> TIdx
	            {
	                return extent.m_extentElements[TIdxIntegralConst::value];
	            }
	        };

	#if BOOST_COMP_GNUC
	#    pragma GCC diagnostic push
	#    pragma GCC diagnostic ignored                                                                                    \
	        "-Wcast-align" // "cast from 'std::uint8_t*' to 'TElem*' increases required alignment of target type"
	#endif
	        //! The ViewSubView native pointer get trait specialization.
	        template<typename TElem, typename TDim, typename TDev, typename TIdx>
	        struct GetPtrNative<ViewSubView<TDev, TElem, TDim, TIdx>>
	        {
	        private:
	            using IdxSequence = std::make_index_sequence<TDim::value>;

	        public:
	            ALPAKA_FN_HOST static auto getPtrNative(ViewSubView<TDev, TElem, TDim, TIdx> const& view) -> TElem const*
	            {
	                // \TODO: pre-calculate this pointer for faster execution.
	                return reinterpret_cast<TElem const*>(
	                    reinterpret_cast<std::uint8_t const*>(alpaka::getPtrNative(view.m_viewParentView))
	                    + pitchedOffsetBytes(view, IdxSequence()));
	            }
	            ALPAKA_FN_HOST static auto getPtrNative(ViewSubView<TDev, TElem, TDim, TIdx>& view) -> TElem*
	            {
	                // \TODO: pre-calculate this pointer for faster execution.
	                return reinterpret_cast<TElem*>(
	                    reinterpret_cast<std::uint8_t*>(alpaka::getPtrNative(view.m_viewParentView))
	                    + pitchedOffsetBytes(view, IdxSequence()));
	            }

	        private:
	            //! For a 3D vector this calculates:
	            //!
	            //! getOffset<0u>(view) * getPitchBytes<1u>(view)
	            //! + getOffset<1u>(view) * getPitchBytes<2u>(view)
	            //! + getOffset<2u>(view) * getPitchBytes<3u>(view)
	            //! while getPitchBytes<3u>(view) is equivalent to sizeof(TElem)
	            template<typename TView, std::size_t... TIndices>
	            ALPAKA_FN_HOST static auto pitchedOffsetBytes(TView const& view, std::index_sequence<TIndices...> const&)
	                -> TIdx
	            {
	                return meta::foldr(std::plus<TIdx>(), pitchedOffsetBytesDim<TIndices>(view)..., TIdx{0});
	            }
	            template<std::size_t Tidx, typename TView>
	            ALPAKA_FN_HOST static auto pitchedOffsetBytesDim(TView const& view) -> TIdx
	            {
	                return getOffset<Tidx>(view) * getPitchBytes<Tidx + 1u>(view);
	            }
	        };
	#if BOOST_COMP_GNUC
	#    pragma GCC diagnostic pop
	#endif

	        //! The ViewSubView pitch get trait specialization.
	        template<typename TIdxIntegralConst, typename TDev, typename TElem, typename TDim, typename TIdx>
	        struct GetPitchBytes<TIdxIntegralConst, ViewSubView<TDev, TElem, TDim, TIdx>>
	        {
	            ALPAKA_FN_HOST static auto getPitchBytes(ViewSubView<TDev, TElem, TDim, TIdx> const& view) -> TIdx
	            {
	                return alpaka::getPitchBytes<TIdxIntegralConst::value>(view.m_viewParentView);
	            }
	        };

	        //! The ViewSubView x offset get trait specialization.
	        template<typename TIdxIntegralConst, typename TElem, typename TDim, typename TDev, typename TIdx>
	        struct GetOffset<
	            TIdxIntegralConst,
	            ViewSubView<TDev, TElem, TDim, TIdx>,
	            std::enable_if_t<(TDim::value > TIdxIntegralConst::value)>>
	        {
	            ALPAKA_FN_HOST static auto getOffset(ViewSubView<TDev, TElem, TDim, TIdx> const& offset) -> TIdx
	            {
	                return offset.m_offsetsElements[TIdxIntegralConst::value];
	            }
	        };

	        //! The ViewSubView idx type trait specialization.
	        template<typename TElem, typename TDim, typename TDev, typename TIdx>
	        struct IdxType<ViewSubView<TDev, TElem, TDim, TIdx>>
	        {
	            using type = TIdx;
	        };

	        //! The CPU device CreateSubView trait default implementation
	        template<typename TDev, typename TSfinae>
	        struct CreateSubView
	        {
	            template<typename TView, typename TExtent, typename TOffsets>
	            static auto createSubView(
	                TView& view,
	                TExtent const& extentElements,
	                TOffsets const& relativeOffsetsElements)
	            {
	                using Dim = alpaka::Dim<TExtent>;
	                using Idx = alpaka::Idx<TExtent>;
	                using Elem = typename trait::ElemType<TView>::type;
	                return ViewSubView<TDev, Elem, Dim, Idx>(view, extentElements, relativeOffsetsElements);
	            }
	        };
	    } // namespace trait
	} // namespace alpaka
	// ==
	// == ./include/alpaka/mem/view/ViewSubView.hpp ==
	// ============================================================================

// meta
	// ============================================================================
	// == ./include/alpaka/meta/Apply.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	namespace alpaka::meta
	{
	    namespace detail
	    {
	        template<typename TList, template<typename...> class TApplicant>
	        struct ApplyImpl;
	        template<template<typename...> class TList, template<typename...> class TApplicant, typename... T>
	        struct ApplyImpl<TList<T...>, TApplicant>
	        {
	            using type = TApplicant<T...>;
	        };
	    } // namespace detail
	    template<typename TList, template<typename...> class TApplicant>
	    using Apply = typename detail::ApplyImpl<TList, TApplicant>::type;
	} // namespace alpaka::meta
	// ==
	// == ./include/alpaka/meta/Apply.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/meta/CartesianProduct.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/meta/Concatenate.hpp"    // amalgamate: file already expanded

	namespace alpaka::meta
	{
	    // This is based on code by Patrick Fromberg.
	    // See
	    // http://stackoverflow.com/questions/9122028/how-to-create-the-cartesian-product-of-a-type-list/19611856#19611856
	    namespace detail
	    {
	        template<typename... Ts>
	        struct CartesianProductImplHelper;
	        // Stop condition.
	        template<template<typename...> class TList, typename... Ts>
	        struct CartesianProductImplHelper<TList<Ts...>>
	        {
	            using type = TList<Ts...>;
	        };
	        // Catches first empty tuple.
	        template<template<typename...> class TList, typename... Ts>
	        struct CartesianProductImplHelper<TList<TList<>>, Ts...>
	        {
	            using type = TList<>;
	        };
	        // Catches any empty tuple except first.
	        template<template<typename...> class TList, typename... Ts, typename... Rests>
	        struct CartesianProductImplHelper<TList<Ts...>, TList<>, Rests...>
	        {
	            using type = TList<>;
	        };
	        template<template<typename...> class TList, typename... X, typename H, typename... Rests>
	        struct CartesianProductImplHelper<TList<X...>, TList<H>, Rests...>
	        {
	            using type1 = TList<Concatenate<X, TList<H>>...>;
	            using type = typename CartesianProductImplHelper<type1, Rests...>::type;
	        };
	        template<
	            template<typename...>
	            class TList,
	            typename... X,
	            template<typename...>
	            class Head,
	            typename T,
	            typename... Ts,
	            typename... Rests>
	        struct CartesianProductImplHelper<TList<X...>, Head<T, Ts...>, Rests...>
	        {
	            using type1 = TList<Concatenate<X, TList<T>>...>;
	            using type2 = typename CartesianProductImplHelper<TList<X...>, TList<Ts...>>::type;
	            using type3 = Concatenate<type1, type2>;
	            using type = typename CartesianProductImplHelper<type3, Rests...>::type;
	        };

	        template<template<typename...> class TList, typename... Ts>
	        struct CartesianProductImpl;
	        // The base case for no input returns an empty sequence.
	        template<template<typename...> class TList>
	        struct CartesianProductImpl<TList>
	        {
	            using type = TList<>;
	        };
	        // R is the return type, Head<A...> is the first input list
	        template<template<typename...> class TList, template<typename...> class Head, typename... Ts, typename... Tail>
	        struct CartesianProductImpl<TList, Head<Ts...>, Tail...>
	        {
	            using type = typename detail::CartesianProductImplHelper<TList<TList<Ts>...>, Tail...>::type;
	        };
	    } // namespace detail

	    template<template<typename...> class TList, typename... Ts>
	    using CartesianProduct = typename detail::CartesianProductImpl<TList, Ts...>::type;
	} // namespace alpaka::meta
	// ==
	// == ./include/alpaka/meta/CartesianProduct.hpp ==
	// ============================================================================

// #include "alpaka/meta/Concatenate.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/meta/CudaVectorArrayWrapper.hpp ==
	// ==
	/* Copyright 2022 Jiří Vyskočil, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

	// #include <functional>    // amalgamate: file already included
	#include <initializer_list>
	#include <numeric>
	// #include <type_traits>    // amalgamate: file already included

	#if defined(ALPAKA_ACC_GPU_HIP_ENABLED) || defined(ALPAKA_ACC_GPU_CUDA_ENABLED)

	namespace alpaka::meta
	{
	    namespace detail
	    {
	        template<typename TScalar, unsigned N>
	        struct CudaVectorArrayTypeTraits;

	        template<>
	        struct CudaVectorArrayTypeTraits<float, 1>
	        {
	            using type = float1;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<float, 2>
	        {
	            using type = float2;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<float, 3>
	        {
	            using type = float3;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<float, 4>
	        {
	            using type = float4;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<double, 1>
	        {
	            using type = double1;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<double, 2>
	        {
	            using type = double2;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<double, 3>
	        {
	            using type = double3;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<double, 4>
	        {
	            using type = double4;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<unsigned, 1>
	        {
	            using type = uint1;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<unsigned, 2>
	        {
	            using type = uint2;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<unsigned, 3>
	        {
	            using type = uint3;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<unsigned, 4>
	        {
	            using type = uint4;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<int, 1>
	        {
	            using type = int1;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<int, 2>
	        {
	            using type = int2;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<int, 3>
	        {
	            using type = int3;
	        };

	        template<>
	        struct CudaVectorArrayTypeTraits<int, 4>
	        {
	            using type = int4;
	        };
	    } // namespace detail

	    /// Helper struct providing [] subscript access to CUDA vector types
	    template<typename TScalar, unsigned N>
	    struct CudaVectorArrayWrapper;

	    template<typename TScalar>
	    struct CudaVectorArrayWrapper<TScalar, 4> : public detail::CudaVectorArrayTypeTraits<TScalar, 4>::type
	    {
	        using value_type = TScalar;
	        constexpr static unsigned size = 4;
	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE CudaVectorArrayWrapper(std::initializer_list<TScalar> init)
	        {
	            auto it = std::begin(init);
	            this->x = *it++;
	            this->y = *it++;
	            this->z = *it++;
	            this->w = *it++;
	        }
	        template<class Other>
	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE CudaVectorArrayWrapper(Other const& o)
	        {
	            static_assert(std::tuple_size_v<Other> == size, "Can only convert between vectors of same size.");
	            static_assert(
	                std::is_same_v<typename Other::value_type, value_type>,
	                "Can only convert between vectors of same element type.");
	            this->x = o[0];
	            this->y = o[1];
	            this->z = o[2];
	            this->w = o[3];
	        }
	        ALPAKA_FN_HOST_ACC constexpr operator std::array<value_type, size>() const
	        {
	            std::array<value_type, size> ret;
	            ret[0] = this->x;
	            ret[1] = this->y;
	            ret[2] = this->z;
	            ret[3] = this->w;
	            return ret;
	        }

	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE constexpr value_type& operator[](int const k) noexcept
	        {
	            assert(k >= 0 && k < 4);
	            return k == 0 ? this->x : (k == 1 ? this->y : (k == 2 ? this->z : this->w));
	        }

	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE constexpr value_type const& operator[](int const k) const noexcept
	        {
	            assert(k >= 0 && k < 4);
	            return k == 0 ? this->x : (k == 1 ? this->y : (k == 2 ? this->z : this->w));
	        }
	    };

	    template<typename TScalar>
	    struct CudaVectorArrayWrapper<TScalar, 3> : public detail::CudaVectorArrayTypeTraits<TScalar, 3>::type
	    {
	        using value_type = TScalar;
	        constexpr static unsigned size = 3;
	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE CudaVectorArrayWrapper(std::initializer_list<TScalar> init)
	        {
	            auto it = std::begin(init);
	            this->x = *it++;
	            this->y = *it++;
	            this->z = *it++;
	        }
	        template<class Other>
	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE CudaVectorArrayWrapper(Other const& o)
	        {
	            static_assert(std::tuple_size<Other>::value == size, "Can only convert between vectors of same size.");
	            static_assert(
	                std::is_same<typename Other::value_type, value_type>::value,
	                "Can only convert between vectors of same element type.");
	            this->x = o[0];
	            this->y = o[1];
	            this->z = o[2];
	        }
	        ALPAKA_FN_HOST_ACC constexpr operator std::array<value_type, size>() const
	        {
	            std::array<value_type, size> ret;
	            ret[0] = this->x;
	            ret[1] = this->y;
	            ret[2] = this->z;
	            return ret;
	        }

	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE constexpr value_type& operator[](int const k) noexcept
	        {
	            assert(k >= 0 && k < 3);
	            return k == 0 ? this->x : (k == 1 ? this->y : this->z);
	        }

	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE constexpr value_type const& operator[](int const k) const noexcept
	        {
	            assert(k >= 0 && k < 3);
	            return k == 0 ? this->x : (k == 1 ? this->y : this->z);
	        }
	    };

	    template<typename TScalar>
	    struct CudaVectorArrayWrapper<TScalar, 2> : public detail::CudaVectorArrayTypeTraits<TScalar, 2>::type
	    {
	        using value_type = TScalar;
	        constexpr static unsigned size = 2;
	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE CudaVectorArrayWrapper(std::initializer_list<TScalar> init)
	        {
	            auto it = std::begin(init);
	            this->x = *it++;
	            this->y = *it++;
	        }
	        template<class Other>
	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE CudaVectorArrayWrapper(Other const& o)
	        {
	            static_assert(std::tuple_size<Other>::value == size, "Can only convert between vectors of same size.");
	            static_assert(
	                std::is_same<typename Other::value_type, value_type>::value,
	                "Can only convert between vectors of same element type.");
	            this->x = o[0];
	            this->y = o[1];
	        }
	        ALPAKA_FN_HOST_ACC constexpr operator std::array<value_type, size>() const
	        {
	            std::array<value_type, size> ret;
	            ret[0] = this->x;
	            ret[1] = this->y;
	            return ret;
	        }

	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE constexpr value_type& operator[](int const k) noexcept
	        {
	            assert(k >= 0 && k < 2);
	            return k == 0 ? this->x : this->y;
	        }

	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE constexpr value_type const& operator[](int const k) const noexcept
	        {
	            assert(k >= 0 && k < 2);
	            return k == 0 ? this->x : this->y;
	        }
	    };

	    template<typename TScalar>
	    struct CudaVectorArrayWrapper<TScalar, 1> : public detail::CudaVectorArrayTypeTraits<TScalar, 1>::type
	    {
	        using value_type = TScalar;
	        constexpr static unsigned size = 1;
	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE CudaVectorArrayWrapper(std::initializer_list<TScalar> init)
	        {
	            auto it = std::begin(init);
	            this->x = *it;
	        }
	        template<class Other>
	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE CudaVectorArrayWrapper(Other const& o)
	        {
	            static_assert(std::tuple_size<Other>::value == size, "Can only convert between vectors of same size.");
	            static_assert(
	                std::is_same<typename Other::value_type, value_type>::value,
	                "Can only convert between vectors of same element type.");
	            this->x = o[0];
	        }
	        ALPAKA_FN_HOST_ACC constexpr operator std::array<value_type, size>() const
	        {
	            std::array<value_type, size> ret;
	            ret[0] = this->x;
	            return ret;
	        }

	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE constexpr value_type& operator[]([[maybe_unused]] int const k) noexcept
	        {
	            assert(k == 0);
	            return this->x;
	        }

	        ALPAKA_FN_HOST_ACC ALPAKA_FN_INLINE constexpr value_type const& operator[](
	            [[maybe_unused]] int const k) const noexcept
	        {
	            assert(k == 0);
	            return this->x;
	        }
	    };
	} // namespace alpaka::meta

	namespace std
	{
	    /// Specialization of std::tuple_size for \a float4_array
	    template<typename T, unsigned N>
	    struct tuple_size<alpaka::meta::CudaVectorArrayWrapper<T, N>> : integral_constant<size_t, N>
	    {
	    };
	} // namespace std

	#endif
	// ==
	// == ./include/alpaka/meta/CudaVectorArrayWrapper.hpp ==
	// ============================================================================

// #include "alpaka/meta/DependentFalseType.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/meta/Filter.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/meta/Concatenate.hpp"    // amalgamate: file already expanded

	// #include <type_traits>    // amalgamate: file already included

	namespace alpaka::meta
	{
	    namespace detail
	    {
	        template<template<typename...> class TList, template<typename> class TPred, typename... Ts>
	        struct FilterImplHelper;
	        template<template<typename...> class TList, template<typename> class TPred>
	        struct FilterImplHelper<TList, TPred>
	        {
	            using type = TList<>;
	        };
	        template<template<typename...> class TList, template<typename> class TPred, typename T, typename... Ts>
	        struct FilterImplHelper<TList, TPred, T, Ts...>
	        {
	            using type = std::conditional_t<
	                TPred<T>::value,
	                Concatenate<TList<T>, typename FilterImplHelper<TList, TPred, Ts...>::type>,
	                typename FilterImplHelper<TList, TPred, Ts...>::type>;
	        };

	        template<typename TList, template<typename> class TPred>
	        struct FilterImpl;
	        template<template<typename...> class TList, template<typename> class TPred, typename... Ts>
	        struct FilterImpl<TList<Ts...>, TPred>
	        {
	            using type = typename detail::FilterImplHelper<TList, TPred, Ts...>::type;
	        };
	    } // namespace detail
	    template<typename TList, template<typename> class TPred>
	    using Filter = typename detail::FilterImpl<TList, TPred>::type;
	} // namespace alpaka::meta
	// ==
	// == ./include/alpaka/meta/Filter.hpp ==
	// ============================================================================

// #include "alpaka/meta/Fold.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/meta/ForEachType.hpp ==
	// ==
	/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

	// #include <utility>    // amalgamate: file already included

	namespace alpaka::meta
	{
	    namespace detail
	    {
	        template<typename TList>
	        struct ForEachTypeHelper;
	        template<template<typename...> class TList>
	        struct ForEachTypeHelper<TList<>>
	        {
	            ALPAKA_NO_HOST_ACC_WARNING
	            template<typename TFnObj, typename... TArgs>
	            ALPAKA_FN_HOST_ACC static auto forEachTypeHelper(TFnObj&& /* f */, TArgs&&... /* args */) -> void
	            {
	            }
	        };
	        template<template<typename...> class TList, typename T, typename... Ts>
	        struct ForEachTypeHelper<TList<T, Ts...>>
	        {
	            ALPAKA_NO_HOST_ACC_WARNING
	            template<typename TFnObj, typename... TArgs>
	            ALPAKA_FN_HOST_ACC static auto forEachTypeHelper(TFnObj&& f, TArgs&&... args) -> void
	            {
	                f.template operator()<T>(std::forward<TArgs>(args)...);
	                ForEachTypeHelper<TList<Ts...>>::forEachTypeHelper(
	                    std::forward<TFnObj>(f),
	                    std::forward<TArgs>(args)...);
	            }
	        };
	    } // namespace detail

	    //! Equivalent to boost::mpl::for_each but does not require the types of the sequence to be default
	    //! constructible. This function does not create instances of the types instead it passes the types as template
	    //! parameter.
	    ALPAKA_NO_HOST_ACC_WARNING
	    template<typename TList, typename TFnObj, typename... TArgs>
	    ALPAKA_FN_HOST_ACC auto forEachType(TFnObj&& f, TArgs&&... args) -> void
	    {
	        detail::ForEachTypeHelper<TList>::forEachTypeHelper(std::forward<TFnObj>(f), std::forward<TArgs>(args)...);
	    }
	} // namespace alpaka::meta
	// ==
	// == ./include/alpaka/meta/ForEachType.hpp ==
	// ============================================================================

// #include "alpaka/meta/Functional.hpp"    // amalgamate: file already expanded
// #include "alpaka/meta/IntegerSequence.hpp"    // amalgamate: file already expanded
// #include "alpaka/meta/Integral.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/meta/IsArrayOrVector.hpp ==
	// ==
	/* Copyright 2022 Jiri Vyskocil, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/meta/CudaVectorArrayWrapper.hpp"    // amalgamate: file already expanded

	// #include <functional>    // amalgamate: file already included
	// #include <numeric>    // amalgamate: file already included
	// #include <type_traits>    // amalgamate: file already included
	// #include <vector>    // amalgamate: file already included

	namespace alpaka::meta
	{
	    /** Checks whether T is an array or a vector type
	     *
	     * @tparam T a type to check
	     */
	    template<typename T>
	    struct IsArrayOrVector : std::false_type
	    {
	    };

	    /** Specialization of \a IsArrayOrVector for vector types
	     *
	     * @tparam T inner type held in the vector
	     * @tparam A vector allocator
	     */
	    template<typename T, typename A>
	    struct IsArrayOrVector<std::vector<T, A>> : std::true_type
	    {
	    };

	    /** Specialization of \a IsArrayOrVector for plain arrays
	     *
	     * @tparam T inner type held in the array
	     * @tparam N size of the array
	     */
	    template<typename T, std::size_t N>
	    struct IsArrayOrVector<T[N]> : std::true_type
	    {
	    };

	    /** Specialization of \a IsArrayOrVector for std::array
	     *
	     * @tparam T inner type held in the array
	     * @tparam N size of the array
	     */
	    template<typename T, std::size_t N>
	    struct IsArrayOrVector<std::array<T, N>> : std::true_type
	    {
	    };

	#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)
	    /// Specialization of \a IsArrayOrVector for CUDA vector array wrapper
	    template<typename T, unsigned N>
	    struct IsArrayOrVector<CudaVectorArrayWrapper<T, N>> : std::true_type
	    {
	    };
	#endif
	} // namespace alpaka::meta
	// ==
	// == ./include/alpaka/meta/IsArrayOrVector.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/meta/IsStrictBase.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include <type_traits>    // amalgamate: file already included

	namespace alpaka::meta
	{
	    //! The trait is true if TDerived is derived from TBase but is not TBase itself.
	    template<typename TBase, typename TDerived>
	    using IsStrictBase = std::
	        integral_constant<bool, std::is_base_of_v<TBase, TDerived> && !std::is_same_v<TBase, std::decay_t<TDerived>>>;
	} // namespace alpaka::meta
	// ==
	// == ./include/alpaka/meta/IsStrictBase.hpp ==
	// ============================================================================

// #include "alpaka/meta/NdLoop.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/meta/NonZero.hpp ==
	// ==
	/* Copyright 2023 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include <type_traits>    // amalgamate: file already included

	namespace alpaka::meta
	{
	    namespace detail
	    {
	        template<typename T>
	        struct NonZeroImpl : std::false_type
	        {
	        };

	        template<typename T, T TValue>
	        struct NonZeroImpl<std::integral_constant<T, TValue>> : std::bool_constant<TValue != static_cast<T>(0)>
	        {
	        };
	    } // namespace detail

	    template<typename T>
	    using NonZero = typename detail::NonZeroImpl<T>;

	} // namespace alpaka::meta
	// ==
	// == ./include/alpaka/meta/NonZero.hpp ==
	// ============================================================================

// #include "alpaka/meta/Set.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/meta/Transform.hpp ==
	// ==
	/* Copyright 2022 Benjamin Worpitz, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	namespace alpaka::meta
	{
	    namespace detail
	    {
	        template<typename Ts, template<typename...> class TOp>
	        struct TransformImpl;
	        template<template<typename...> class TList, typename... Ts, template<typename...> class TOp>
	        struct TransformImpl<TList<Ts...>, TOp>
	        {
	            using type = TList<TOp<Ts>...>;
	        };
	    } // namespace detail
	    template<typename Ts, template<typename...> class TOp>
	    using Transform = typename detail::TransformImpl<Ts, TOp>::type;
	} // namespace alpaka::meta
	// ==
	// == ./include/alpaka/meta/Transform.hpp ==
	// ============================================================================

// #include "alpaka/meta/TypeListOps.hpp"    // amalgamate: file already expanded
// offset
// #include "alpaka/offset/Traits.hpp"    // amalgamate: file already expanded
// platform
// #include "alpaka/pltf/PltfCpu.hpp"    // amalgamate: file already expanded
// #include "alpaka/pltf/PltfCpuSyclIntel.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/pltf/PltfCudaRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiCudaRt.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/pltf/PltfUniformCudaHipRt.hpp ==
		// ==
		/* Copyright 2022 Benjamin Worpitz, René Widera, Andrea Bocci, Bernhard Manfred Gruber, Antonio Di Pilato
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Concepts.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Cuda.hpp"    // amalgamate: file already expanded
		// #include "alpaka/core/Hip.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/DevUniformCudaHipRt.hpp"    // amalgamate: file already expanded
		// #include "alpaka/dev/Traits.hpp"    // amalgamate: file already expanded

		// #include <iostream>    // amalgamate: file already included
		// #include <sstream>    // amalgamate: file already included
		// #include <stdexcept>    // amalgamate: file already included
		// #include <tuple>    // amalgamate: file already included

		#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)

		namespace alpaka
		{
		    // Forward declarations.
		    struct ApiCudaRt;
		    struct ApiHipRt;

		    //! The CUDA/HIP RT platform.
		    template<typename TApi>
		    class PltfUniformCudaHipRt : public concepts::Implements<ConceptPltf, PltfUniformCudaHipRt<TApi>>
		    {
		    public:
		        ALPAKA_FN_HOST PltfUniformCudaHipRt() = delete;
		    };

		    namespace trait
		    {
		        //! The CUDA/HIP RT platform device type trait specialization.
		        template<typename TApi>
		        struct DevType<PltfUniformCudaHipRt<TApi>>
		        {
		            using type = DevUniformCudaHipRt<TApi>;
		        };

		        //! The CUDA/HIP RT platform device count get trait specialization.
		        template<typename TApi>
		        struct GetDevCount<PltfUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto getDevCount() -> std::size_t
		            {
		                ALPAKA_DEBUG_FULL_LOG_SCOPE;

		                int iNumDevices(0);
		                typename TApi::Error_t error = TApi::getDeviceCount(&iNumDevices);
		                if(error != TApi::success)
		                    iNumDevices = 0;

		                return static_cast<std::size_t>(iNumDevices);
		            }
		        };

		        //! The CUDA/HIP RT platform device get trait specialization.
		        template<typename TApi>
		        struct GetDevByIdx<PltfUniformCudaHipRt<TApi>>
		        {
		            ALPAKA_FN_HOST static auto getDevByIdx(std::size_t const& devIdx) -> DevUniformCudaHipRt<TApi>
		            {
		                ALPAKA_DEBUG_FULL_LOG_SCOPE;

		                std::size_t const devCount(getDevCount<PltfUniformCudaHipRt<TApi>>());
		                if(devIdx >= devCount)
		                {
		                    std::stringstream ssErr;
		                    ssErr << "Unable to return device handle for device " << devIdx << ". There are only " << devCount
		                          << " devices!";
		                    throw std::runtime_error(ssErr.str());
		                }

		                if(isDevUsable(devIdx))
		                {
		                    DevUniformCudaHipRt<TApi> dev(static_cast<int>(devIdx));

		                    // Log this device.
		#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
		                    typename TApi::DeviceProp_t devProp;
		                    ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::getDeviceProperties(&devProp, dev.getNativeHandle()));
		#    endif
		#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		                    printDeviceProperties(devProp);
		#    elif ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
		                    std::cout << __func__ << devProp.name << std::endl;
		#    endif
		                    return dev;
		                }
		                else
		                {
		                    std::stringstream ssErr;
		                    ssErr << "Unable to return device handle for device " << devIdx << ". It is not accessible!";
		                    throw std::runtime_error(ssErr.str());
		                }
		            }

		        private:
		            //! \return If the device is usable.
		            ALPAKA_FN_HOST static auto isDevUsable(std::size_t iDevice) -> bool
		            {
		                typename TApi::Error_t rc = TApi::setDevice(static_cast<int>(iDevice));
		                typename TApi::Stream_t queue = {};
		                // Create a dummy queue to check if the device is already used by an other process.
		                // cuda/hip-SetDevice never returns an error if another process already uses the selected device and
		                // gpu compute mode is set "process exclusive". \TODO: Check if this workaround is needed!
		                if(rc == TApi::success)
		                {
		                    rc = TApi::streamCreate(&queue);
		                }

		                if(rc == TApi::success)
		                {
		                    // Destroy the dummy queue.
		                    ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(TApi::streamDestroy(queue));
		                    return true;
		                }
		                else
		                {
		                    // Return the previous error from cudaStreamCreate.
		                    ALPAKA_UNIFORM_CUDA_HIP_RT_CHECK(rc);
		                    // Reset the Error state.
		                    std::ignore = TApi::getLastError();
		                    return false;
		                }
		            }

		#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_FULL
		            //! Prints all the device properties to std::cout.
		            ALPAKA_FN_HOST static auto printDeviceProperties(typename TApi::DeviceProp_t const& devProp) -> void
		            {
		                ALPAKA_DEBUG_FULL_LOG_SCOPE;

		                std::size_t const kiB(1024);
		                std::size_t const miB(kiB * kiB);
		                std::cout << "name: " << devProp.name << std::endl;
		                std::cout << "totalGlobalMem: " << devProp.totalGlobalMem / miB << " MiB" << std::endl;
		                std::cout << "sharedMemPerBlock: " << devProp.sharedMemPerBlock / kiB << " KiB" << std::endl;
		                std::cout << "regsPerBlock: " << devProp.regsPerBlock << std::endl;
		                std::cout << "warpSize: " << devProp.warpSize << std::endl;
		                std::cout << "maxThreadsPerBlock: " << devProp.maxThreadsPerBlock << std::endl;
		                std::cout << "maxThreadsDim[3]: (" << devProp.maxThreadsDim[0] << ", " << devProp.maxThreadsDim[1]
		                          << ", " << devProp.maxThreadsDim[2] << ")" << std::endl;
		                std::cout << "maxGridSize[3]: (" << devProp.maxGridSize[0] << ", " << devProp.maxGridSize[1] << ", "
		                          << devProp.maxGridSize[2] << ")" << std::endl;
		                std::cout << "clockRate: " << devProp.clockRate << " kHz" << std::endl;
		                std::cout << "totalConstMem: " << devProp.totalConstMem / kiB << " KiB" << std::endl;
		                std::cout << "major: " << devProp.major << std::endl;
		                std::cout << "minor: " << devProp.minor << std::endl;

		                // std::cout << "deviceOverlap: " << devProp.deviceOverlap << std::endl;    // Deprecated
		                std::cout << "multiProcessorCount: " << devProp.multiProcessorCount << std::endl;
		                std::cout << "integrated: " << devProp.integrated << std::endl;
		                std::cout << "canMapHostMemory: " << devProp.canMapHostMemory << std::endl;
		                std::cout << "computeMode: " << devProp.computeMode << std::endl;
		                std::cout << "concurrentKernels: " << devProp.concurrentKernels << std::endl;
		                std::cout << "pciBusID: " << devProp.pciBusID << std::endl;
		                std::cout << "pciDeviceID: " << devProp.pciDeviceID << std::endl;
		                std::cout << "pciDomainID: " << devProp.pciDomainID << std::endl;
		                std::cout << "memoryClockRate: " << devProp.memoryClockRate << " kHz" << std::endl;
		                std::cout << "memoryBusWidth: " << devProp.memoryBusWidth << " b" << std::endl;
		                std::cout << "l2CacheSize: " << devProp.l2CacheSize << " B" << std::endl;
		                std::cout << "maxThreadsPerMultiProcessor: " << devProp.maxThreadsPerMultiProcessor << std::endl;
		                std::cout << "isMultiGpuBoard: " << devProp.isMultiGpuBoard << std::endl;
		                if constexpr(std::is_same_v<TApi, ApiCudaRt>)
		                {
		                    std::cout << "memPitch: " << devProp.memPitch << " B" << std::endl;
		                    std::cout << "textureAlignment: " << devProp.textureAlignment << std::endl;
		                    std::cout << "texturePitchAlignment: " << devProp.texturePitchAlignment << std::endl;
		                    std::cout << "kernelExecTimeoutEnabled: " << devProp.kernelExecTimeoutEnabled << std::endl;
		                    std::cout << "unifiedAddressing: " << devProp.unifiedAddressing << std::endl;
		                    std::cout << "multiGpuBoardGroupID: " << devProp.multiGpuBoardGroupID << std::endl;
		                    std::cout << "singleToDoublePrecisionPerfRatio: " << devProp.singleToDoublePrecisionPerfRatio
		                              << std::endl;
		                    std::cout << "pageableMemoryAccess: " << devProp.pageableMemoryAccess << std::endl;
		                    std::cout << "concurrentManagedAccess: " << devProp.concurrentManagedAccess << std::endl;
		                    std::cout << "computePreemptionSupported: " << devProp.computePreemptionSupported << std::endl;
		                    std::cout << "canUseHostPointerForRegisteredMem: " << devProp.canUseHostPointerForRegisteredMem
		                              << std::endl;
		                    std::cout << "cooperativeLaunch: " << devProp.cooperativeLaunch << std::endl;
		                    std::cout << "cooperativeMultiDeviceLaunch: " << devProp.cooperativeMultiDeviceLaunch << std::endl;
		                    std::cout << "maxTexture1D: " << devProp.maxTexture1D << std::endl;
		                    std::cout << "maxTexture1DLinear: " << devProp.maxTexture1DLinear << std::endl;
		                    std::cout << "maxTexture2D[2]: " << devProp.maxTexture2D[0] << "x" << devProp.maxTexture2D[1]
		                              << std::endl;
		                    std::cout << "maxTexture2DLinear[3]: " << devProp.maxTexture2DLinear[0] << "x"
		                              << devProp.maxTexture2DLinear[1] << "x" << devProp.maxTexture2DLinear[2] << std::endl;
		                    std::cout << "maxTexture2DGather[2]: " << devProp.maxTexture2DGather[0] << "x"
		                              << devProp.maxTexture2DGather[1] << std::endl;
		                    std::cout << "maxTexture3D[3]: " << devProp.maxTexture3D[0] << "x" << devProp.maxTexture3D[1]
		                              << "x" << devProp.maxTexture3D[2] << std::endl;
		                    std::cout << "maxTextureCubemap: " << devProp.maxTextureCubemap << std::endl;
		                    std::cout << "maxTexture1DLayered[2]: " << devProp.maxTexture1DLayered[0] << "x"
		                              << devProp.maxTexture1DLayered[1] << std::endl;
		                    std::cout << "maxTexture2DLayered[3]: " << devProp.maxTexture2DLayered[0] << "x"
		                              << devProp.maxTexture2DLayered[1] << "x" << devProp.maxTexture2DLayered[2] << std::endl;
		                    std::cout << "maxTextureCubemapLayered[2]: " << devProp.maxTextureCubemapLayered[0] << "x"
		                              << devProp.maxTextureCubemapLayered[1] << std::endl;
		                    std::cout << "maxSurface1D: " << devProp.maxSurface1D << std::endl;
		                    std::cout << "maxSurface2D[2]: " << devProp.maxSurface2D[0] << "x" << devProp.maxSurface2D[1]
		                              << std::endl;
		                    std::cout << "maxSurface3D[3]: " << devProp.maxSurface3D[0] << "x" << devProp.maxSurface3D[1]
		                              << "x" << devProp.maxSurface3D[2] << std::endl;
		                    std::cout << "maxSurface1DLayered[2]: " << devProp.maxSurface1DLayered[0] << "x"
		                              << devProp.maxSurface1DLayered[1] << std::endl;
		                    std::cout << "maxSurface2DLayered[3]: " << devProp.maxSurface2DLayered[0] << "x"
		                              << devProp.maxSurface2DLayered[1] << "x" << devProp.maxSurface2DLayered[2] << std::endl;
		                    std::cout << "maxSurfaceCubemap: " << devProp.maxSurfaceCubemap << std::endl;
		                    std::cout << "maxSurfaceCubemapLayered[2]: " << devProp.maxSurfaceCubemapLayered[0] << "x"
		                              << devProp.maxSurfaceCubemapLayered[1] << std::endl;
		                    std::cout << "surfaceAlignment: " << devProp.surfaceAlignment << std::endl;
		                    std::cout << "ECCEnabled: " << devProp.ECCEnabled << std::endl;
		                    std::cout << "tccDriver: " << devProp.tccDriver << std::endl;
		                    std::cout << "asyncEngineCount: " << devProp.asyncEngineCount << std::endl;
		                    std::cout << "streamPrioritiesSupported: " << devProp.streamPrioritiesSupported << std::endl;
		                    std::cout << "globalL1CacheSupported: " << devProp.globalL1CacheSupported << std::endl;
		                    std::cout << "localL1CacheSupported: " << devProp.localL1CacheSupported << std::endl;
		                    std::cout << "sharedMemPerMultiprocessor: " << devProp.sharedMemPerMultiprocessor << std::endl;
		                    std::cout << "regsPerMultiprocessor: " << devProp.regsPerMultiprocessor << std::endl;
		                    std::cout << "managedMemory: " << devProp.managedMemory << std::endl;
		                }
		                else
		                { // ApiHipRt
		                    std::cout << "clockInstructionRate: " << devProp.clockInstructionRate << "kHz" << std::endl;
		                    std::cout << "maxSharedMemoryPerMultiProcessor: " << devProp.maxSharedMemoryPerMultiProcessor / kiB
		                              << " KiB" << std::endl;
		                    std::cout << "gcnArch: " << devProp.gcnArch << std::endl;
		                    std::cout << "arch: " << std::endl;
		                    std::cout << "    hasGlobalInt32Atomics: " << devProp.arch.hasGlobalInt32Atomics << std::endl;
		                    std::cout << "    hasGlobalFloatAtomicExch: " << devProp.arch.hasGlobalFloatAtomicExch
		                              << std::endl;
		                    std::cout << "    hasSharedInt32Atomics: " << devProp.arch.hasSharedInt32Atomics << std::endl;
		                    std::cout << "    hasSharedFloatAtomicExch: " << devProp.arch.hasSharedFloatAtomicExch
		                              << std::endl;
		                    std::cout << "    hasFloatAtomicAdd: " << devProp.arch.hasFloatAtomicAdd << std::endl;
		                    std::cout << "    hasGlobalInt64Atomics: " << devProp.arch.hasGlobalInt64Atomics << std::endl;
		                    std::cout << "    hasSharedInt64Atomics: " << devProp.arch.hasSharedInt64Atomics << std::endl;
		                    std::cout << "    hasDoubles: " << devProp.arch.hasDoubles << std::endl;
		                    std::cout << "    hasWarpVote: " << devProp.arch.hasWarpVote << std::endl;
		                    std::cout << "    hasWarpBallot: " << devProp.arch.hasWarpBallot << std::endl;
		                    std::cout << "    hasWarpShuffle: " << devProp.arch.hasWarpShuffle << std::endl;
		                    std::cout << "    hasFunnelShift: " << devProp.arch.hasFunnelShift << std::endl;
		                    std::cout << "    hasThreadFenceSystem: " << devProp.arch.hasThreadFenceSystem << std::endl;
		                    std::cout << "    hasSyncThreadsExt: " << devProp.arch.hasSyncThreadsExt << std::endl;
		                    std::cout << "    hasSurfaceFuncs: " << devProp.arch.hasSurfaceFuncs << std::endl;
		                    std::cout << "    has3dGrid: " << devProp.arch.has3dGrid << std::endl;
		                    std::cout << "    hasDynamicParallelism: " << devProp.arch.hasDynamicParallelism << std::endl;
		                }
		            }
		#    endif
		        };
		    } // namespace trait
		} // namespace alpaka

		#endif
		// ==
		// == ./include/alpaka/pltf/PltfUniformCudaHipRt.hpp ==
		// ============================================================================


	#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

	namespace alpaka
	{
	    //! The CUDA RT platform.
	    using PltfCudaRt = PltfUniformCudaHipRt<ApiCudaRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_CUDA_ENABLED
	// ==
	// == ./include/alpaka/pltf/PltfCudaRt.hpp ==
	// ============================================================================

// #include "alpaka/pltf/PltfFpgaSyclIntel.hpp"    // amalgamate: file already expanded
// #include "alpaka/pltf/PltfGpuSyclIntel.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/pltf/PltfHipRt.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/pltf/PltfUniformCudaHipRt.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_HIP_ENABLED

	namespace alpaka
	{
	    //! The HIP RT platform.
	    using PltfHipRt = PltfUniformCudaHipRt<ApiHipRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_HIP_ENABLED
	// ==
	// == ./include/alpaka/pltf/PltfHipRt.hpp ==
	// ============================================================================

// #include "alpaka/pltf/Traits.hpp"    // amalgamate: file already expanded
// rand
	// ============================================================================
	// == ./include/alpaka/rand/RandDefault.hpp ==
	// ==
	/* Copyright 2022 Jeffrey Kelling, Jan Stephan, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
	// #include "alpaka/math/Traits.hpp"    // amalgamate: file already expanded
		// ============================================================================
		// == ./include/alpaka/rand/RandPhilox.hpp ==
		// ==
		/* Copyright 2022 Jiří Vyskočil, Jan Stephan, Bernhard Manfred Gruber
		 * SPDX-License-Identifier: MPL-2.0
		 */

		// #pragma once
		// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded
		// #include "alpaka/meta/IsArrayOrVector.hpp"    // amalgamate: file already expanded
			// ============================================================================
			// == ./include/alpaka/rand/Philox/PhiloxSingle.hpp ==
			// ==
			/* Copyright 2022 Jiri Vyskocil, Rene Widera, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
				// ============================================================================
				// == ./include/alpaka/rand/Philox/MultiplyAndSplit64to32.hpp ==
				// ==
				/* Copyright 2022 Jiri Vyskocil, Bernhard Manfred Gruber
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/core/Common.hpp"    // amalgamate: file already expanded

				namespace alpaka::rand
				{
				    /// Get high 32 bits of a 64-bit number
				    ALPAKA_FN_HOST_ACC constexpr static auto high32Bits(std::uint64_t const x) -> std::uint32_t
				    {
				        return static_cast<std::uint32_t>(x >> 32);
				    }

				    /// Get low 32 bits of a 64-bit number
				    ALPAKA_FN_HOST_ACC constexpr static auto low32Bits(std::uint64_t const x) -> std::uint32_t
				    {
				        return static_cast<std::uint32_t>(x & 0xffffffff);
				    }

				    /** Multiply two 64-bit numbers and split the result into high and low 32 bits, also known as "mulhilo32"
				     *
				     * @param a first 64-bit multiplier
				     * @param b second 64-bit multiplier
				     * @param resultHigh high 32 bits of the product a*b
				     * @param resultLow low 32 bits of the product a*b
				     */
				    // TODO: See single-instruction implementations in original Philox source code
				    ALPAKA_FN_HOST_ACC constexpr static void multiplyAndSplit64to32(
				        std::uint64_t const a,
				        std::uint64_t const b,
				        std::uint32_t& resultHigh,
				        std::uint32_t& resultLow)
				    {
				        std::uint64_t res64 = a * b;
				        resultHigh = high32Bits(res64);
				        resultLow = low32Bits(res64);
				    }
				} // namespace alpaka::rand
				// ==
				// == ./include/alpaka/rand/Philox/MultiplyAndSplit64to32.hpp ==
				// ============================================================================

				// ============================================================================
				// == ./include/alpaka/rand/Philox/PhiloxBaseTraits.hpp ==
				// ==
				/* Copyright 2022 Jiří Vyskočil, Bernhard Manfred Gruber, Jeffrey Kelling, Jan Stephan
				 * SPDX-License-Identifier: MPL-2.0
				 */

				// #pragma once
				// #include "alpaka/acc/AccGpuUniformCudaHipRt.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/rand/Philox/PhiloxBaseCommon.hpp ==
					// ==
					/* Copyright 2022 Jiri Vyskocil, Bernhard Manfred Gruber, Jeffrey Kelling
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
						// ============================================================================
						// == ./include/alpaka/rand/Philox/PhiloxStateless.hpp ==
						// ==
						/* Copyright 2022 Jiri Vyskocil, Bernhard Manfred Gruber, Jeffrey Kelling
						 * SPDX-License-Identifier: MPL-2.0
						 */

						// #pragma once
						// #include "alpaka/core/Unroll.hpp"    // amalgamate: file already expanded
						// #include "alpaka/rand/Philox/MultiplyAndSplit64to32.hpp"    // amalgamate: file already expanded
							// ============================================================================
							// == ./include/alpaka/rand/Philox/PhiloxConstants.hpp ==
							// ==
							/* Copyright 2022 Jiri Vyskocil, Bernhard Manfred Gruber
							 * SPDX-License-Identifier: MPL-2.0
							 */

							// #pragma once
							// #include "alpaka/rand/Philox/MultiplyAndSplit64to32.hpp"    // amalgamate: file already expanded

							// #include <cstdint>    // amalgamate: file already included
							// #include <utility>    // amalgamate: file already included


							namespace alpaka::rand::engine
							{
							    /** Constants used in the Philox algorithm
							     *
							     * The numbers are taken from the reference Philox implementation:
							     *
							     * J. K. Salmon, M. A. Moraes, R. O. Dror and D. E. Shaw, "Parallel random numbers: As easy as 1, 2, 3,"
							     * SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking,
							     * Storage and Analysis, 2011, pp. 1-12, doi: 10.1145/2063384.2063405.
							     *
							     * @tparam TParams basic Philox algorithm parameters
							     *
							     * static const data members are transformed into functions, because GCC
							     * assumes types with static data members to be not mappable and makes not
							     * exception for constexpr ones. This is a valid interpretation of the
							     * OpenMP <= 4.5 standard. In OpenMP >= 5.0 types with any kind of static
							     * data member are mappable.
							     */
							    template<typename TParams>
							    class PhiloxConstants
							    {
							    public:
							        static constexpr std::uint64_t WEYL_64_0()
							        {
							            return 0x9E3779B97F4A7C15; ///< First Weyl sequence parameter: the golden ratio
							        }
							        static constexpr std::uint64_t WEYL_64_1()
							        {
							            return 0xBB67AE8584CAA73B; ///< Second Weyl sequence parameter: \f$ \sqrt{3}-1 \f$
							        }

							        static constexpr std::uint32_t WEYL_32_0()
							        {
							            return high32Bits(WEYL_64_0()); ///< 1st Weyl sequence parameter, 32 bits
							        }
							        static constexpr std::uint32_t WEYL_32_1()
							        {
							            return high32Bits(WEYL_64_1()); ///< 2nd Weyl sequence parameter, 32 bits
							        }

							        static constexpr std::uint32_t MULTIPLITER_4x32_0()
							        {
							            return 0xCD9E8D57; ///< First Philox S-box multiplier
							        }
							        static constexpr std::uint32_t MULTIPLITER_4x32_1()
							        {
							            return 0xD2511F53; ///< Second Philox S-box multiplier
							        }
							    };
							} // namespace alpaka::rand::engine
							// ==
							// == ./include/alpaka/rand/Philox/PhiloxConstants.hpp ==
							// ============================================================================


						// #include <utility>    // amalgamate: file already included


						namespace alpaka::rand::engine
						{
						    /** Philox algorithm parameters
						     *
						     * @tparam TCounterSize number of elements in the counter
						     * @tparam TWidth width of one counter element (in bits)
						     * @tparam TRounds number of S-box rounds
						     */
						    template<unsigned TCounterSize, unsigned TWidth, unsigned TRounds>
						    struct PhiloxParams
						    {
						        static constexpr unsigned counterSize = TCounterSize;
						        static constexpr unsigned width = TWidth;
						        static constexpr unsigned rounds = TRounds;
						    };

						    /** Class basic Philox family counter-based PRNG
						     *
						     * Checks the validity of passed-in parameters and calls the \a TBackend methods to perform N rounds of the
						     * Philox shuffle.
						     *
						     * @tparam TBackend device-dependent backend, specifies the array types
						     * @tparam TParams Philox algorithm parameters \sa PhiloxParams
						     */
						    template<typename TBackend, typename TParams>
						    class PhiloxStateless : public PhiloxConstants<TParams>
						    {
						        static constexpr unsigned numRounds()
						        {
						            return TParams::rounds;
						        }
						        static constexpr unsigned vectorSize()
						        {
						            return TParams::counterSize;
						        }
						        static constexpr unsigned numberWidth()
						        {
						            return TParams::width;
						        }

						        static_assert(numRounds() > 0, "Number of Philox rounds must be > 0.");
						        static_assert(vectorSize() % 2 == 0, "Philox counter size must be an even number.");
						        static_assert(vectorSize() <= 16, "Philox SP network is not specified for sizes > 16.");
						        static_assert(numberWidth() % 8 == 0, "Philox number width in bits must be a multiple of 8.");

						        static_assert(numberWidth() == 32, "Philox implemented only for 32 bit numbers.");

						    public:
						        using Counter = typename TBackend::Counter;
						        using Key = typename TBackend::Key;
						        using Constants = PhiloxConstants<TParams>;

						    protected:
						        /** Single round of the Philox shuffle
						         *
						         * @param counter state of the counter
						         * @param key value of the key
						         * @return shuffled counter
						         */
						        static ALPAKA_FN_HOST_ACC auto singleRound(Counter const& counter, Key const& key)
						        {
						            std::uint32_t H0, L0, H1, L1;
						            multiplyAndSplit64to32(counter[0], Constants::MULTIPLITER_4x32_0(), H0, L0);
						            multiplyAndSplit64to32(counter[2], Constants::MULTIPLITER_4x32_1(), H1, L1);
						            return Counter{H1 ^ counter[1] ^ key[0], L1, H0 ^ counter[3] ^ key[1], L0};
						        }

						        /** Bump the \a key by the Weyl sequence step parameter
						         *
						         * @param key the key to be bumped
						         * @return the bumped key
						         */
						        static ALPAKA_FN_HOST_ACC auto bumpKey(Key const& key)
						        {
						            return Key{key[0] + Constants::WEYL_32_0(), key[1] + Constants::WEYL_32_1()};
						        }

						        /** Performs N rounds of the Philox shuffle
						         *
						         * @param counter_in initial state of the counter
						         * @param key_in initial state of the key
						         * @return result of the PRNG shuffle; has the same size as the counter
						         */
						        static ALPAKA_FN_HOST_ACC auto nRounds(Counter const& counter_in, Key const& key_in) -> Counter
						        {
						            Key key{key_in};
						            Counter counter = singleRound(counter_in, key);

						            ALPAKA_UNROLL(numRounds())
						            for(unsigned int n = 0; n < numRounds(); ++n)
						            {
						                key = bumpKey(key);
						                counter = singleRound(counter, key);
						            }

						            return counter;
						        }

						    public:
						        /** Generates a random number (\p TCounterSize x32-bit)
						         *
						         * @param counter initial state of the counter
						         * @param key initial state of the key
						         * @return result of the PRNG shuffle; has the same size as the counter
						         */
						        static ALPAKA_FN_HOST_ACC auto generate(Counter const& counter, Key const& key) -> Counter
						        {
						            return nRounds(counter, key);
						        }
						    };
						} // namespace alpaka::rand::engine
						// ==
						// == ./include/alpaka/rand/Philox/PhiloxStateless.hpp ==
						// ============================================================================


					// #include <utility>    // amalgamate: file already included


					namespace alpaka::rand::engine
					{
					    /** Common class for Philox family engines
					     *
					     * Relies on `PhiloxStateless` to provide the PRNG and adds state to handling the counting.
					     *
					     * @tparam TBackend device-dependent backend, specifies the array types
					     * @tparam TParams Philox algorithm parameters \sa PhiloxParams
					     * @tparam TImpl engine type implementation (CRTP)
					     *
					     * static const data members are transformed into functions, because GCC
					     * assumes types with static data members to be not mappable and makes not
					     * exception for constexpr ones. This is a valid interpretation of the
					     * OpenMP <= 4.5 standard. In OpenMP >= 5.0 types with any kind of static
					     * data member are mappable.
					     */
					    template<typename TBackend, typename TParams, typename TImpl>
					    class PhiloxBaseCommon
					        : public TBackend
					        , public PhiloxStateless<TBackend, TParams>
					    {
					    public:
					        using Counter = typename PhiloxStateless<TBackend, TParams>::Counter;
					        using Key = typename PhiloxStateless<TBackend, TParams>::Key;

					    protected:
					        /** Advance the \a counter to the next state
					         *
					         * Increments the passed-in \a counter by one with a 128-bit carry.
					         *
					         * @param counter reference to the counter which is to be advanced
					         */
					        ALPAKA_FN_HOST_ACC void advanceCounter(Counter& counter)
					        {
					            counter[0]++;
					            /* 128-bit carry */
					            if(counter[0] == 0)
					            {
					                counter[1]++;
					                if(counter[1] == 0)
					                {
					                    counter[2]++;
					                    if(counter[2] == 0)
					                    {
					                        counter[3]++;
					                    }
					                }
					            }
					        }

					        /** Advance the internal state counter by \a offset N-vectors (N = counter size)
					         *
					         * Advances the internal value of this->state.counter
					         *
					         * @param offset number of N-vectors to skip
					         */
					        ALPAKA_FN_HOST_ACC void skip4(uint64_t offset)
					        {
					            Counter& counter = static_cast<TImpl*>(this)->state.counter;
					            Counter temp = counter;
					            counter[0] += low32Bits(offset);
					            counter[1] += high32Bits(offset) + (counter[0] < temp[0] ? 1 : 0);
					            counter[2] += (counter[0] < temp[1] ? 1u : 0u);
					            counter[3] += (counter[0] < temp[2] ? 1u : 0u);
					        }

					        /** Advance the counter by the length of \a subsequence
					         *
					         * Advances the internal value of this->state.counter
					         *
					         * @param subsequence number of subsequences to skip
					         */
					        ALPAKA_FN_HOST_ACC void skipSubsequence(uint64_t subsequence)
					        {
					            Counter& counter = static_cast<TImpl*>(this)->state.counter;
					            Counter temp = counter;
					            counter[2] += low32Bits(subsequence);
					            counter[3] += high32Bits(subsequence) + (counter[2] < temp[2] ? 1 : 0);
					        }
					    };
					} // namespace alpaka::rand::engine
					// ==
					// == ./include/alpaka/rand/Philox/PhiloxBaseCommon.hpp ==
					// ============================================================================

					// ============================================================================
					// == ./include/alpaka/rand/Philox/PhiloxBaseCudaArray.hpp ==
					// ==
					/* Copyright 2022 Jiri Vyskocil
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/meta/CudaVectorArrayWrapper.hpp"    // amalgamate: file already expanded

					#if defined(ALPAKA_ACC_GPU_HIP_ENABLED) || defined(ALPAKA_ACC_GPU_CUDA_ENABLED)

					namespace alpaka::rand::engine
					{
					    namespace trait
					    {
					        template<typename TScalar>
					        struct PhiloxResultContainerTraits;

					        template<>
					        struct PhiloxResultContainerTraits<float>
					        {
					            using type = meta::CudaVectorArrayWrapper<float, 4>;
					        };

					        template<>
					        struct PhiloxResultContainerTraits<double>
					        {
					            using type = meta::CudaVectorArrayWrapper<double, 4>;
					        };

					        template<>
					        struct PhiloxResultContainerTraits<int>
					        {
					            using type = meta::CudaVectorArrayWrapper<int, 4>;
					        };

					        template<>
					        struct PhiloxResultContainerTraits<unsigned>
					        {
					            using type = meta::CudaVectorArrayWrapper<unsigned, 4>;
					        };

					        template<typename TScalar>
					        using PhiloxResultContainer = typename PhiloxResultContainerTraits<TScalar>::type;
					    } // namespace trait

					    /** Philox backend using array-like interface to CUDA uintN types for the storage of Key and Counter
					     *
					     * @tparam TParams Philox algorithm parameters \sa PhiloxParams
					     */
					    template<typename TParams>
					    class PhiloxBaseCudaArray
					    {
					        static_assert(TParams::counterSize == 4, "GPU Philox implemented only for counters of width == 4");

					    public:
					        using Counter
					            = meta::CudaVectorArrayWrapper<unsigned, 4>; ///< Counter type = array-like interface to CUDA uint4
					        using Key = meta::CudaVectorArrayWrapper<unsigned, 2>; ///< Key type = array-like interface to CUDA uint2
					        template<typename TDistributionResultScalar>
					        using ResultContainer = trait::PhiloxResultContainer<TDistributionResultScalar>; ///< Vector template for
					                                                                                         ///< distribution results
					    };
					} // namespace alpaka::rand::engine

					#endif
					// ==
					// == ./include/alpaka/rand/Philox/PhiloxBaseCudaArray.hpp ==
					// ============================================================================

					// ============================================================================
					// == ./include/alpaka/rand/Philox/PhiloxBaseStdArray.hpp ==
					// ==
					/* Copyright 2022 Jiri Vyskocil, Bernhard Manfred Gruber
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include <array>    // amalgamate: file already included
					// #include <cstdint>    // amalgamate: file already included

					namespace alpaka::rand::engine
					{
					    /** Philox backend using std::array for Key and Counter storage
					     *
					     * @tparam TParams Philox algorithm parameters \sa PhiloxParams
					     */
					    template<typename TParams>
					    class PhiloxBaseStdArray
					    {
					    public:
					        using Counter = std::array<std::uint32_t, TParams::counterSize>; ///< Counter type = std::array
					        using Key = std::array<std::uint32_t, TParams::counterSize / 2>; ///< Key type = std::array
					        template<typename TScalar>
					        using ResultContainer
					            = std::array<TScalar, TParams::counterSize>; ///< Vector template for distribution results
					    };
					} // namespace alpaka::rand::engine
					// ==
					// == ./include/alpaka/rand/Philox/PhiloxBaseStdArray.hpp ==
					// ============================================================================

				// #include "alpaka/rand/Philox/PhiloxStateless.hpp"    // amalgamate: file already expanded
					// ============================================================================
					// == ./include/alpaka/rand/Philox/PhiloxStatelessKeyedBase.hpp ==
					// ==
					/* Copyright 2022 Jeffrey Kelling
					 * SPDX-License-Identifier: MPL-2.0
					 */

					// #pragma once
					// #include "alpaka/rand/Philox/PhiloxStateless.hpp"    // amalgamate: file already expanded

					namespace alpaka::rand::engine
					{
					    /** Common class for Philox family engines
					     *
					     * Checks the validity of passed-in parameters and calls the \a TBackend methods to perform N rounds of the
					     * Philox shuffle.
					     *
					     * @tparam TBackend device-dependent backend, specifies the array types
					     * @tparam TParams Philox algorithm parameters \sa PhiloxParams
					     */
					    template<typename TBackend, typename TParams>
					    struct PhiloxStatelessKeyedBase : public PhiloxStateless<TBackend, TParams>
					    {
					    public:
					        using Counter = typename PhiloxStateless<TBackend, TParams>::Counter;
					        using Key = typename PhiloxStateless<TBackend, TParams>::Key;

					        const Key m_key;

					        PhiloxStatelessKeyedBase(Key&& key) : m_key(std::move(key))
					        {
					        }

					        ALPAKA_FN_HOST_ACC auto operator()(Counter const& counter) const
					        {
					            return this->generate(counter, m_key);
					        }
					    };
					} // namespace alpaka::rand::engine
					// ==
					// == ./include/alpaka/rand/Philox/PhiloxStatelessKeyedBase.hpp ==
					// ============================================================================


				namespace alpaka::rand::engine::trait
				{
				    template<typename TAcc>
				    constexpr inline bool isGPU = false;

				#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)
				    template<typename TApi, typename TDim, typename TIdx>
				    constexpr inline bool isGPU<AccGpuUniformCudaHipRt<TApi, TDim, TIdx>> = true;
				#endif

				    /** Selection of default backend
				     *
				     * Selects the data backend based on the accelerator device type. As of now, different backends operate
				     * on different array types.
				     *
				     * @tparam TAcc the accelerator as defined in alpaka/acc
				     * @tparam TParams Philox algorithm parameters
				     * @tparam TSfinae internal parameter to stop substitution search and provide the default
				     */
				    template<typename TAcc, typename TParams, typename TSfinae = void>
				    struct PhiloxStatelessBaseTraits
				    {
				        // template <typename Acc, typename TParams, typename TImpl>
				#if defined(ALPAKA_ACC_GPU_CUDA_ENABLED) || defined(ALPAKA_ACC_GPU_HIP_ENABLED)
				        using Backend = std::conditional_t<isGPU<TAcc>, PhiloxBaseCudaArray<TParams>, PhiloxBaseStdArray<TParams>>;
				#else
				        using Backend = PhiloxBaseStdArray<TParams>;
				#endif
				        using Counter = typename Backend::Counter; ///< Counter array type
				        using Key = typename Backend::Key; ///< Key array type
				        template<typename TDistributionResultScalar>
				        using ResultContainer =
				            typename Backend::template ResultContainer<TDistributionResultScalar>; ///< Distribution
				                                                                                   ///< container type
				        /// Base type to be inherited from by stateless keyed engine
				        using Base = PhiloxStateless<Backend, TParams>;
				    };

				    /** Selection of default backend
				     *
				     * Selects the data backend based on the accelerator device type. As of now, different backends operate
				     * on different array types.
				     *
				     * @tparam TAcc the accelerator as defined in alpaka/acc
				     * @tparam TParams Philox algorithm parameters
				     * @tparam TSfinae internal parameter to stop substitution search and provide the default
				     */
				    template<typename TAcc, typename TParams, typename TSfinae = void>
				    struct PhiloxStatelessKeyedBaseTraits : public PhiloxStatelessBaseTraits<TAcc, TParams>
				    {
				        using Backend = typename PhiloxStatelessBaseTraits<TAcc, TParams>::Backend;
				        /// Base type to be inherited from by counting engines
				        using Base = PhiloxStatelessKeyedBase<Backend, TParams>;
				    };

				    /** Selection of default backend
				     *
				     * Selects the data backend based on the accelerator device type. As of now, different backends operate
				     * on different array types.
				     *
				     * @tparam TAcc the accelerator as defined in alpaka/acc
				     * @tparam TParams Philox algorithm parameters
				     * @tparam TImpl engine type implementation (CRTP)
				     * @tparam TSfinae internal parameter to stop substitution search and provide the default
				     */
				    template<typename TAcc, typename TParams, typename TImpl, typename TSfinae = void>
				    struct PhiloxBaseTraits : public PhiloxStatelessBaseTraits<TAcc, TParams>
				    {
				        using Backend = typename PhiloxStatelessBaseTraits<TAcc, TParams>::Backend;
				        /// Base type to be inherited from by counting engines
				        using Base = PhiloxBaseCommon<Backend, TParams, TImpl>;
				    };
				} // namespace alpaka::rand::engine::trait
				// ==
				// == ./include/alpaka/rand/Philox/PhiloxBaseTraits.hpp ==
				// ============================================================================


			// #include <utility>    // amalgamate: file already included

			namespace alpaka::rand::engine
			{
			    /** Philox state for single value engine
			     *
			     * @tparam TCounter Type of the Counter array
			     * @tparam TKey Type of the Key array
			     */
			    template<typename TCounter, typename TKey>
			    struct PhiloxStateSingle
			    {
			        using Counter = TCounter;
			        using Key = TKey;

			        Counter counter; ///< Counter array
			        Key key; ///< Key array
			        Counter result; ///< Intermediate result array
			        std::uint32_t position; ///< Pointer to the active intermediate result element
			        // TODO: Box-Muller states
			    };

			    /** Philox engine generating a single number
			     *
			     * This engine's operator() will return a single number. Since the result is the same size as the counter,
			     * and so it contains more than one number, it has to be stored between individual invocations of
			     * operator(). Additionally a pointer has to be stored indicating which part of the result array is to be
			     * returned next.
			     *
			     * @tparam TAcc Accelerator type as defined in alpaka/acc
			     * @tparam TParams Basic parameters for the Philox algorithm
			     */
			    template<typename TAcc, typename TParams>
			    class PhiloxSingle : public trait::PhiloxBaseTraits<TAcc, TParams, PhiloxSingle<TAcc, TParams>>::Base
			    {
			    public:
			        /// Specialization for different TAcc backends
			        using Traits = typename trait::PhiloxBaseTraits<TAcc, TParams, PhiloxSingle<TAcc, TParams>>;

			        using Counter = typename Traits::Counter; ///< Backend-dependent Counter type
			        using Key = typename Traits::Key; ///< Backend-dependent Key type
			        using State = PhiloxStateSingle<Counter, Key>; ///< Backend-dependent State type

			        State state; ///< Internal engine state

			    protected:
			        /** Advance internal counter to the next value
			         *
			         * Advances the full internal counter array, resets the position pointer and stores the intermediate
			         * result to be recalled when the user requests a number.
			         */
			        ALPAKA_FN_HOST_ACC void advanceState()
			        {
			            this->advanceCounter(state.counter);
			            state.result = this->nRounds(state.counter, state.key);
			            state.position = 0;
			        }

			        /** Get the next random number and advance internal state
			         *
			         * The intermediate result stores N = TParams::counterSize numbers. Check if we've already given out
			         * all of them. If so, generate a new intermediate result (this also resets the pointer to the position
			         * of the actual number). Finally, we return the actual number.
			         *
			         * @return The next random number
			         */
			        ALPAKA_FN_HOST_ACC auto nextNumber()
			        {
			            // Element zero will always contain the next valid random number.
			            auto result = state.result[0];
			            state.position++;
			            if(state.position == TParams::counterSize)
			            {
			                advanceState();
			            }
			            else
			            {
			                // Shift state results to allow hard coded access to element zero.
			                // This will avoid high register usage on NVIDIA devices.
			                // \todo Check if this shifting of the result vector is decreasing CPU performance.
			                //       If so this optimization for GPUs (mostly NVIDIA) should be moved into
			                //       PhiloxBaseCudaArray.
			                state.result[0] = state.result[1];
			                state.result[1] = state.result[2];
			                state.result[2] = state.result[3];
			            }

			            return result;
			        }

			        /// Skips the next \a offset numbers
			        ALPAKA_FN_HOST_ACC void skip(uint64_t offset)
			        {
			            static_assert(TParams::counterSize == 4, "Only counterSize is supported.");
			            state.position = static_cast<decltype(state.position)>(state.position + (offset & 3));
			            offset += state.position < 4 ? 0 : 4;
			            state.position -= state.position < 4 ? 0 : 4u;
			            for(auto numShifts = state.position; numShifts > 0; --numShifts)
			            {
			                // Shift state results to allow hard coded access to element zero.
			                // This will avoid high register usage on NVIDIA devices.
			                state.result[0] = state.result[1];
			                state.result[1] = state.result[2];
			                state.result[2] = state.result[3];
			            }
			            this->skip4(offset / 4);
			        }

			    public:
			        /** Construct a new Philox engine with single-value output
			         *
			         * @param seed Set the Philox generator key
			         * @param subsequence Select a subsequence of size 2^64
			         * @param offset Skip \a offset numbers form the start of the subsequence
			         */
			        ALPAKA_FN_HOST_ACC PhiloxSingle(uint64_t seed = 0, uint64_t subsequence = 0, uint64_t offset = 0)
			            : state{{0, 0, 0, 0}, {low32Bits(seed), high32Bits(seed)}, {0, 0, 0, 0}, 0}
			        {
			            this->skipSubsequence(subsequence);
			            skip(offset);
			            advanceState();
			        }

			        /** Get the next random number
			         *
			         * @return The next random number
			         */
			        ALPAKA_FN_HOST_ACC auto operator()()
			        {
			            return nextNumber();
			        }
			    };
			} // namespace alpaka::rand::engine
			// ==
			// == ./include/alpaka/rand/Philox/PhiloxSingle.hpp ==
			// ============================================================================

			// ============================================================================
			// == ./include/alpaka/rand/Philox/PhiloxVector.hpp ==
			// ==
			/* Copyright 2022 Jiri Vyskocil, Bernhard Manfred Gruber
			 * SPDX-License-Identifier: MPL-2.0
			 */

			// #pragma once
			// #include "alpaka/rand/Philox/MultiplyAndSplit64to32.hpp"    // amalgamate: file already expanded
			// #include "alpaka/rand/Philox/PhiloxBaseTraits.hpp"    // amalgamate: file already expanded

			// #include <utility>    // amalgamate: file already included


			namespace alpaka::rand::engine
			{
			    /** Philox state for vector generator
			     *
			     * @tparam TCounter Type of the Counter array
			     * @tparam TKey Type of the Key array
			     */
			    template<typename TCounter, typename TKey>
			    struct PhiloxStateVector
			    {
			        using Counter = TCounter;
			        using Key = TKey;

			        Counter counter; ///< Counter array
			        Key key; ///< Key array
			    };

			    /** Philox engine generating a vector of numbers
			     *
			     * This engine's operator() will return a vector of numbers corresponding to the full size of its counter.
			     * This is a convenience vs. memory size tradeoff since the user has to deal with the output array
			     * themselves, but the internal state comprises only of a single counter and a key.
			     *
			     * @tparam TAcc Accelerator type as defined in alpaka/acc
			     * @tparam TParams Basic parameters for the Philox algorithm
			     */
			    template<typename TAcc, typename TParams>
			    class PhiloxVector : public trait::PhiloxBaseTraits<TAcc, TParams, PhiloxVector<TAcc, TParams>>::Base
			    {
			    public:
			        /// Specialization for different TAcc backends
			        using Traits = trait::PhiloxBaseTraits<TAcc, TParams, PhiloxVector<TAcc, TParams>>;

			        using Counter = typename Traits::Counter; ///< Backend-dependent Counter type
			        using Key = typename Traits::Key; ///< Backend-dependent Key type
			        using State = PhiloxStateVector<Counter, Key>; ///< Backend-dependent State type
			        template<typename TDistributionResultScalar>
			        using ResultContainer = typename Traits::template ResultContainer<TDistributionResultScalar>;

			        State state;

			    protected:
			        /** Get the next array of random numbers and advance internal state
			         *
			         * @return The next array of random numbers
			         */
			        ALPAKA_FN_HOST_ACC auto nextVector()
			        {
			            this->advanceCounter(state.counter);
			            return this->nRounds(state.counter, state.key);
			        }

			        /** Skips the next \a offset vectors
			         *
			         * Unlike its counterpart in \a PhiloxSingle, this function advances the state in multiples of the
			         * counter size thus skipping the entire array of numbers.
			         */
			        ALPAKA_FN_HOST_ACC void skip(uint64_t offset)
			        {
			            this->skip4(offset);
			        }

			    public:
			        /** Construct a new Philox engine with vector output
			         *
			         * @param seed Set the Philox generator key
			         * @param subsequence Select a subsequence of size 2^64
			         * @param offset Skip \a offset numbers form the start of the subsequence
			         */
			        ALPAKA_FN_HOST_ACC PhiloxVector(uint64_t seed = 0, uint64_t subsequence = 0, uint64_t offset = 0)
			            : state{{0, 0, 0, 0}, {low32Bits(seed), high32Bits(seed)}}
			        {
			            this->skipSubsequence(subsequence);
			            skip(offset);
			            nextVector();
			        }

			        /** Get the next vector of random numbers
			         *
			         * @return The next vector of random numbers
			         */
			        ALPAKA_FN_HOST_ACC auto operator()()
			        {
			            return nextVector();
			        }
			    };
			} // namespace alpaka::rand::engine
			// ==
			// == ./include/alpaka/rand/Philox/PhiloxVector.hpp ==
			// ============================================================================

		// #include "alpaka/rand/Traits.hpp"    // amalgamate: file already expanded

		// #include <cstdint>    // amalgamate: file already included
		// #include <limits>    // amalgamate: file already included
		// #include <random>    // amalgamate: file already included
		// #include <type_traits>    // amalgamate: file already included

		namespace alpaka::rand
		{
		    /** Most common Philox engine variant, outputs single number
		     *
		     * This is a variant of the Philox engine generator which outputs a single float. The counter size is \f$4
		     * \times 32 = 128\f$ bits. Since the engine returns a single number, the generated result, which has the same
		     * size as the counter, has to be stored between invocations. Additionally a 32 bit pointer is stored. The
		     * total size of the state is 352 bits = 44 bytes.
		     *
		     * Ref.: J. K. Salmon, M. A. Moraes, R. O. Dror and D. E. Shaw, "Parallel random numbers: As easy as 1, 2, 3,"
		     * SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and
		     * Analysis, 2011, pp. 1-12, doi: 10.1145/2063384.2063405.
		     *
		     * @tparam TAcc Accelerator type as defined in alpaka/acc
		     */
		    template<typename TAcc>
		    class Philox4x32x10 : public concepts::Implements<ConceptRand, Philox4x32x10<TAcc>>
		    {
		    public:
		        using EngineParams = engine::PhiloxParams<4, 32, 10>; ///< Philox algorithm: 10 rounds, 4 numbers of size 32.
		        using EngineVariant = engine::PhiloxSingle<TAcc, EngineParams>; ///< Engine outputs a single number

		        /** Initialize a new Philox engine
		         *
		         * @param seed Set the Philox generator key
		         * @param subsequence Select a subsequence of size 2^64
		         * @param offset Skip \a offset numbers form the start of the subsequence
		         */
		        ALPAKA_FN_HOST_ACC Philox4x32x10(
		            std::uint64_t const seed = 0,
		            std::uint64_t const subsequence = 0,
		            std::uint64_t const offset = 0)
		            : engineVariant(seed, subsequence, offset)
		        {
		        }

		        // STL UniformRandomBitGenerator concept
		        // https://en.cppreference.com/w/cpp/named_req/UniformRandomBitGenerator
		        using result_type = std::uint32_t;
		        ALPAKA_FN_HOST_ACC constexpr auto min() -> result_type
		        {
		            return 0;
		        }
		        ALPAKA_FN_HOST_ACC constexpr auto max() -> result_type
		        {
		            return std::numeric_limits<result_type>::max();
		        }
		        ALPAKA_FN_HOST_ACC auto operator()() -> result_type
		        {
		            return engineVariant();
		        }

		    private:
		        EngineVariant engineVariant;
		    };

		    /** Most common Philox engine variant, outputs a 4-vector of floats
		     *
		     * This is a variant of the Philox engine generator which outputs a vector containing 4 floats. The counter
		     * size is \f$4 \times 32 = 128\f$ bits. Since the engine returns the whole generated vector, it is up to the
		     * user to extract individual floats as they need. The benefit is smaller state size since the state does not
		     * contain the intermediate results. The total size of the state is 192 bits = 24 bytes.
		     *
		     * Ref.: J. K. Salmon, M. A. Moraes, R. O. Dror and D. E. Shaw, "Parallel random numbers: As easy as 1, 2, 3,"
		     * SC '11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and
		     * Analysis, 2011, pp. 1-12, doi: 10.1145/2063384.2063405.
		     *
		     * @tparam TAcc Accelerator type as defined in alpaka/acc
		     */
		    template<typename TAcc>
		    class Philox4x32x10Vector : public concepts::Implements<ConceptRand, Philox4x32x10Vector<TAcc>>
		    {
		    public:
		        using EngineParams = engine::PhiloxParams<4, 32, 10>;
		        using EngineVariant = engine::PhiloxVector<TAcc, EngineParams>;

		        /** Initialize a new Philox engine
		         *
		         * @param seed Set the Philox generator key
		         * @param subsequence Select a subsequence of size 2^64
		         * @param offset Number of numbers to skip form the start of the subsequence.
		         */
		        ALPAKA_FN_HOST_ACC Philox4x32x10Vector(
		            std::uint32_t const seed = 0,
		            std::uint32_t const subsequence = 0,
		            std::uint32_t const offset = 0)
		            : engineVariant(seed, subsequence, offset)
		        {
		        }

		        template<typename TScalar>
		        using ResultContainer = typename EngineVariant::template ResultContainer<TScalar>;

		        using ResultInt = std::uint32_t;
		        using ResultVec = decltype(std::declval<EngineVariant>()());
		        ALPAKA_FN_HOST_ACC constexpr auto min() -> ResultInt
		        {
		            return 0;
		        }
		        ALPAKA_FN_HOST_ACC constexpr auto max() -> ResultInt
		        {
		            return std::numeric_limits<ResultInt>::max();
		        }
		        ALPAKA_FN_HOST_ACC auto operator()() -> ResultVec
		        {
		            return engineVariant();
		        }

		    private:
		        EngineVariant engineVariant;
		    };

		    // The following exists because you "cannot call __device__ function from a __host__ __device__ function"
		    // directly, but wrapping that call in a struct is just fine.
		    template<typename TEngine>
		    struct EngineCallHostAccProxy
		    {
		        ALPAKA_FN_HOST_ACC auto operator()(TEngine& engine) -> decltype(engine())
		        {
		            return engine();
		        }
		    };

		    /// TEMP: Distributions to be decided on later. The generator should be compatible with STL as of now.
		    template<typename TResult, typename TSfinae = void>
		    class UniformReal : public concepts::Implements<ConceptRand, UniformReal<TResult>>
		    {
		        template<typename TRes, typename TEnable = void>
		        struct ResultType
		        {
		            using type = TRes;
		        };

		        template<typename TRes>
		        struct ResultType<TRes, std::enable_if_t<meta::IsArrayOrVector<TRes>::value>>
		        {
		            using type = typename TRes::value_type;
		        };

		        using T = typename ResultType<TResult>::type;
		        static_assert(std::is_floating_point_v<T>, "Only floating-point types are supported");

		    public:
		        ALPAKA_FN_HOST_ACC UniformReal() : UniformReal(0, 1)
		        {
		        }

		        ALPAKA_FN_HOST_ACC UniformReal(T min, T max) : _min(min), _max(max), _range(_max - _min)
		        {
		        }

		        template<typename TEngine>
		        ALPAKA_FN_HOST_ACC auto operator()(TEngine& engine) -> TResult
		        {
		            if constexpr(meta::IsArrayOrVector<TResult>::value)
		            {
		                auto result = engine();
		                T scale = static_cast<T>(1) / engine.max() * _range;
		                TResult ret{
		                    static_cast<T>(result[0]) * scale + _min,
		                    static_cast<T>(result[1]) * scale + _min,
		                    static_cast<T>(result[2]) * scale + _min,
		                    static_cast<T>(result[3]) * scale + _min};
		                return ret;
		            }
		            else
		            {
		                // Since it's possible to get a host-only engine here, the call has to go through proxy
		                return static_cast<T>(EngineCallHostAccProxy<TEngine>{}(engine)) / engine.max() * _range + _min;
		            }

		            ALPAKA_UNREACHABLE(TResult{});
		        }

		    private:
		        const T _min;
		        const T _max;
		        const T _range;
		    };
		} // namespace alpaka::rand
		// ==
		// == ./include/alpaka/rand/RandPhilox.hpp ==
		// ============================================================================

	// #include "alpaka/rand/Traits.hpp"    // amalgamate: file already expanded

	// #include <algorithm>    // amalgamate: file already included
	// #include <limits>    // amalgamate: file already included
	// #include <type_traits>    // amalgamate: file already included

	namespace alpaka::rand
	{
	    class RandDefault : public concepts::Implements<ConceptRand, RandDefault>
	    {
	    };

	    namespace distribution::gpu
	    {
	        namespace detail
	        {
	            template<typename TFloat>
	            struct BitsType;

	            template<>
	            struct BitsType<float>
	            {
	                using type = std::uint32_t;
	            };
	            template<>
	            struct BitsType<double>
	            {
	                using type = std::uint64_t;
	            };
	        } // namespace detail

	        //! The GPU random number normal distribution.
	        template<typename T>
	        class UniformUint
	        {
	            static_assert(std::is_integral_v<T>, "Return type of UniformUint must be integral.");

	        public:
	            UniformUint() = default;

	            template<typename TEngine>
	            ALPAKA_FN_HOST_ACC auto operator()(TEngine& engine) -> T
	            {
	                using BitsT = typename TEngine::result_type;
	                T ret = 0;
	                constexpr auto N = sizeof(T) / sizeof(BitsT);
	                for(unsigned int a = 0; a < N; ++a)
	                {
	                    ret
	                        ^= (static_cast<T>(engine())
	                            << (sizeof(BitsT) * std::numeric_limits<unsigned char>::digits * a));
	                }
	                return ret;
	            }
	        };

	        //! The GPU random number uniform distribution.
	        template<typename T>
	        class UniformReal
	        {
	            static_assert(std::is_floating_point_v<T>, "Return type of UniformReal must be floating point.");

	            using BitsT = typename detail::BitsType<T>::type;

	        public:
	            UniformReal() = default;

	            template<typename TEngine>
	            ALPAKA_FN_HOST_ACC auto operator()(TEngine& engine) -> T
	            {
	                constexpr BitsT limit = static_cast<BitsT>(1) << std::numeric_limits<T>::digits;
	                const BitsT b = UniformUint<BitsT>()(engine);
	                auto const ret = static_cast<T>(b & (limit - 1)) / limit;
	                return ret;
	            }
	        };

	        /*! The GPU random number normal distribution.
	         *
	         * \note
	         * This type contains state and is not thread-safe: To be used
	         * per thread, not shared.
	         *
	         * \note When reproducibility is a concern, each instance of
	         * this class should be used with only on random engine
	         * instance, or two consecutive number should be generated with
	         * each engine used. This is due to the implicit caching of one
	         * Gaussian random number.
	         */
	        template<typename Acc, typename T>
	        class NormalReal
	        {
	            static_assert(std::is_floating_point_v<T>, "Return type of NormalReal must be floating point.");

	            Acc const* m_acc;
	            T m_cache = std::numeric_limits<T>::quiet_NaN();

	        public:
	            /*! \warning Retains a reference to \p acc, thus must not outlive it.
	             */
	            ALPAKA_FN_HOST_ACC constexpr NormalReal(Acc const& acc) : m_acc(&acc)
	            {
	            }

	            // All copy operations (and thus also move since we don't declare those and they fall back to copy) do NOT
	            // copy m_cache. This way we can ensure that the following holds:
	            // NormalReal<Acc> a(acc), b(acc);
	            // Engine<Acc> e(acc);
	            // assert(a(e) != b(e)); // because of two engine invocations
	            // b = a;
	            // assert(a(e) != b(e)); // because of two engine invocations

	            ALPAKA_FN_HOST_ACC constexpr NormalReal(NormalReal const& other) : m_acc(other.m_acc)
	            {
	            }

	            ALPAKA_FN_HOST_ACC constexpr auto operator=(NormalReal const& other) -> NormalReal&
	            {
	                m_acc = other.m_acc;
	                return *this;
	            }

	            template<typename TEngine>
	            ALPAKA_FN_HOST_ACC auto operator()(TEngine& engine) -> T
	            {
	                constexpr auto sigma = T{1};
	                constexpr auto mu = T{0};
	                if(math::isnan(*m_acc, m_cache))
	                {
	                    UniformReal<T> uni;

	                    T u1, u2;
	                    do
	                    {
	                        u1 = uni(engine);
	                        u2 = uni(engine);
	                    } while(u1 <= std::numeric_limits<T>::epsilon());

	                    // compute z0 and z1
	                    const T mag = sigma * math::sqrt(*m_acc, static_cast<T>(-2.) * math::log(*m_acc, u1));
	                    constexpr T twoPi = static_cast<T>(2. * math::constants::pi);
	                    // getting two normal number out of this, store one for later
	                    m_cache = mag * static_cast<T>(math::cos(*m_acc, twoPi * u2)) + mu;

	                    return mag * static_cast<T>(math::sin(*m_acc, twoPi * u2)) + mu;
	                }

	                const T ret = m_cache;
	                m_cache = std::numeric_limits<T>::quiet_NaN();
	                return ret;
	            }
	        };
	    } // namespace distribution::gpu

	    namespace distribution::trait
	    {
	        //! The GPU device random number float normal distribution get trait specialization.
	        template<typename T>
	        struct CreateNormalReal<RandDefault, T, std::enable_if_t<std::is_floating_point_v<T>>>
	        {
	            template<typename TAcc>
	            ALPAKA_FN_HOST_ACC static auto createNormalReal(TAcc const& acc) -> gpu::NormalReal<TAcc, T>
	            {
	                return {acc};
	            }
	        };
	        //! The GPU device random number float uniform distribution get trait specialization.
	        template<typename T>
	        struct CreateUniformReal<RandDefault, T, std::enable_if_t<std::is_floating_point_v<T>>>
	        {
	            ALPAKA_FN_HOST_ACC static auto createUniformReal(RandDefault const& /* rand */) -> gpu::UniformReal<T>
	            {
	                return {};
	            }
	        };
	        //! The GPU device random number integer uniform distribution get trait specialization.
	        template<typename T>
	        struct CreateUniformUint<RandDefault, T, std::enable_if_t<std::is_integral_v<T>>>
	        {
	            ALPAKA_FN_HOST_ACC static auto createUniformUint(RandDefault const& /* rand */) -> gpu::UniformUint<T>
	            {
	                return {};
	            }
	        };
	    } // namespace distribution::trait

	    namespace engine::trait
	    {
	        //! The GPU device random number default generator get trait specialization.
	        template<>
	        struct CreateDefault<RandDefault>
	        {
	            template<typename TAcc>
	            ALPAKA_FN_HOST_ACC static auto createDefault(
	                TAcc const& /* acc */,
	                std::uint32_t const& seed,
	                std::uint32_t const& subsequence,
	                std::uint32_t const& offset) -> Philox4x32x10<TAcc>
	            {
	                return {seed, subsequence, offset};
	            }
	        };
	    } // namespace engine::trait
	} // namespace alpaka::rand
	// ==
	// == ./include/alpaka/rand/RandDefault.hpp ==
	// ============================================================================

// #include "alpaka/rand/RandPhilox.hpp"    // amalgamate: file already expanded
// #include "alpaka/rand/RandStdLib.hpp"    // amalgamate: file already expanded
// #include "alpaka/rand/RandUniformCudaHipRand.hpp"    // amalgamate: file already expanded
// #include "alpaka/rand/Traits.hpp"    // amalgamate: file already expanded
// idx
// #include "alpaka/idx/Traits.hpp"    // amalgamate: file already expanded
// queue
// #include "alpaka/queue/Properties.hpp"    // amalgamate: file already expanded
	// ============================================================================
	// == ./include/alpaka/queue/QueueCpuBlocking.hpp ==
	// ==
	/* Copyright 2020 Jeffrey Kelling, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/event/EventCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueGenericThreadsBlocking.hpp"    // amalgamate: file already expanded

	namespace alpaka
	{
	    using QueueCpuBlocking = QueueGenericThreadsBlocking<DevCpu>;
	}
	// ==
	// == ./include/alpaka/queue/QueueCpuBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueCpuNonBlocking.hpp ==
	// ==
	/* Copyright 2020 Jeffrey Kelling, Bernhard Manfred Gruber
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/event/EventCpu.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueGenericThreadsNonBlocking.hpp"    // amalgamate: file already expanded

	namespace alpaka
	{
	    using QueueCpuNonBlocking = QueueGenericThreadsNonBlocking<DevCpu>;
	}
	// ==
	// == ./include/alpaka/queue/QueueCpuNonBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueCpuSyclIntelBlocking.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevCpuSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueGenericSyclBlocking.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_CPU)

	namespace alpaka
	{
	    using QueueCpuSyclIntelBlocking = QueueGenericSyclBlocking<DevCpuSyclIntel>;
	}

	#endif
	// ==
	// == ./include/alpaka/queue/QueueCpuSyclIntelBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueCpuSyclIntelNonBlocking.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevCpuSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueGenericSyclNonBlocking.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_CPU)

	namespace alpaka
	{
	    using QueueCpuSyclIntelNonBlocking = QueueGenericSyclNonBlocking<DevCpuSyclIntel>;
	}

	#endif
	// ==
	// == ./include/alpaka/queue/QueueCpuSyclIntelNonBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueCudaRtBlocking.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiCudaRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueUniformCudaHipRtBlocking.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

	namespace alpaka
	{
	    //! The CUDA RT blocking queue.
	    using QueueCudaRtBlocking = QueueUniformCudaHipRtBlocking<ApiCudaRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_CUDA_ENABLED
	// ==
	// == ./include/alpaka/queue/QueueCudaRtBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueCudaRtNonBlocking.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiCudaRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueUniformCudaHipRtNonBlocking.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

	namespace alpaka
	{
	    //! The CUDA RT non-blocking queue.
	    using QueueCudaRtNonBlocking = QueueUniformCudaHipRtNonBlocking<ApiCudaRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_CUDA_ENABLED
	// ==
	// == ./include/alpaka/queue/QueueCudaRtNonBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueFpgaSyclIntelBlocking.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */


	// #pragma once
	// #include "alpaka/dev/DevFpgaSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueGenericSyclBlocking.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_FPGA)

	namespace alpaka
	{
	    using QueueFpgaSyclIntelBlocking = QueueGenericSyclBlocking<DevFpgaSyclIntel>;
	}

	#endif
	// ==
	// == ./include/alpaka/queue/QueueFpgaSyclIntelBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueFpgaSyclIntelNonBlocking.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevFpgaSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueGenericSyclNonBlocking.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_FPGA)

	namespace alpaka
	{
	    using QueueFpgaSyclIntelNonBlocking = QueueGenericSyclNonBlocking<DevFpgaSyclIntel>;
	}

	#endif
	// ==
	// == ./include/alpaka/queue/QueueFpgaSyclIntelNonBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueGpuSyclIntelBlocking.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevGpuSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueGenericSyclBlocking.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_GPU)

	namespace alpaka
	{
	    using QueueGpuSyclIntelBlocking = QueueGenericSyclBlocking<DevGpuSyclIntel>;
	}

	#endif
	// ==
	// == ./include/alpaka/queue/QueueGpuSyclIntelBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueGpuSyclIntelNonBlocking.hpp ==
	// ==
	/* Copyright 2022 Jan Stephan
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/dev/DevGpuSyclIntel.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueGenericSyclNonBlocking.hpp"    // amalgamate: file already expanded

	#if defined(ALPAKA_ACC_SYCL_ENABLED) && defined(ALPAKA_SYCL_BACKEND_ONEAPI) && defined(ALPAKA_SYCL_ONEAPI_GPU)

	namespace alpaka
	{
	    using QueueGpuSyclIntelNonBlocking = QueueGenericSyclNonBlocking<DevGpuSyclIntel>;
	}

	#endif
	// ==
	// == ./include/alpaka/queue/QueueGpuSyclIntelNonBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueHipRtBlocking.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueUniformCudaHipRtBlocking.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_HIP_ENABLED

	namespace alpaka
	{
	    //! The HIP RT blocking queue.
	    using QueueHipRtBlocking = QueueUniformCudaHipRtBlocking<ApiHipRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_HIP_ENABLED
	// ==
	// == ./include/alpaka/queue/QueueHipRtBlocking.hpp ==
	// ============================================================================

	// ============================================================================
	// == ./include/alpaka/queue/QueueHipRtNonBlocking.hpp ==
	// ==
	/* Copyright 2022 Andrea Bocci
	 * SPDX-License-Identifier: MPL-2.0
	 */

	// #pragma once
	// #include "alpaka/core/ApiHipRt.hpp"    // amalgamate: file already expanded
	// #include "alpaka/queue/QueueUniformCudaHipRtNonBlocking.hpp"    // amalgamate: file already expanded

	#ifdef ALPAKA_ACC_GPU_HIP_ENABLED

	namespace alpaka
	{
	    //! The HIP RT non-blocking queue.
	    using QueueHipRtNonBlocking = QueueUniformCudaHipRtNonBlocking<ApiHipRt>;
	} // namespace alpaka

	#endif // ALPAKA_ACC_GPU_HIP_ENABLED
	// ==
	// == ./include/alpaka/queue/QueueHipRtNonBlocking.hpp ==
	// ============================================================================

// #include "alpaka/queue/Traits.hpp"    // amalgamate: file already expanded
// traits
// #include "alpaka/traits/Traits.hpp"    // amalgamate: file already expanded
// wait
// #include "alpaka/wait/Traits.hpp"    // amalgamate: file already expanded
// workdiv
// #include "alpaka/workdiv/Traits.hpp"    // amalgamate: file already expanded
// #include "alpaka/workdiv/WorkDivHelpers.hpp"    // amalgamate: file already expanded
// #include "alpaka/workdiv/WorkDivMembers.hpp"    // amalgamate: file already expanded
// vec
// #include "alpaka/vec/Traits.hpp"    // amalgamate: file already expanded
// #include "alpaka/vec/Vec.hpp"    // amalgamate: file already expanded
// ==
// == ./include/alpaka/alpaka.hpp ==
// ============================================================================



