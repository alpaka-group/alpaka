/* Copyright 2022 Axel Huebl, Benjamin Worpitz, Matthias Werner, Ren√© Widera, Andrea Bocci, Bernhard Manfred Gruber
 *
 * This file is part of alpaka.
 *
 * This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/.
 */

#pragma once

#ifdef ALPAKA_ACC_GPU_CUDA_ENABLED

#    include <alpaka/core/BoostPredef.hpp>
#    include <alpaka/elem/Traits.hpp>
#    include <alpaka/extent/Traits.hpp>
#    include <alpaka/idx/Traits.hpp>
#    include <alpaka/meta/Concatenate.hpp>
#    include <alpaka/meta/TypeListOps.hpp>
#    include <alpaka/offset/Traits.hpp>
#    include <alpaka/vec/Vec.hpp>

// cuda_runtime_api.h: CUDA Runtime API C-style interface that does not require compiling with nvcc.
// cuda_runtime.h: CUDA Runtime API  C++-style interface built on top of the C API.
//  It wraps some of the C API routines, using overloading, references and default arguments.
//  These wrappers can be used from C++ code and can be compiled with any C++ compiler.
//  The C++ API also has some CUDA-specific wrappers that wrap C API routines that deal with symbols, textures, and
//  device functions. These wrappers require the use of \p nvcc because they depend on code being generated by the
//  compiler. For example, the execution configuration syntax to invoke kernels is only available in source code
//  compiled with nvcc.
#    include <cuda.h>
#    include <cuda_runtime.h>

#    include <cstddef>
#    include <stdexcept>
#    include <string>
#    include <type_traits>
#    include <utility>

#    if(!defined(CUDART_VERSION) || (CUDART_VERSION < 9000))
#        error "CUDA version 9.0 or greater required!"
#    endif

#    if(!defined(CUDA_VERSION) || (CUDA_VERSION < 9000))
#        error "CUDA version 9.0 or greater required!"
#    endif

#    define ALPAKA_PP_CONCAT_DO(X, Y) X##Y
#    define ALPAKA_PP_CONCAT(X, Y) ALPAKA_PP_CONCAT_DO(X, Y)
//! prefix a name with `cuda`
#    define ALPAKA_API_PREFIX(name) ALPAKA_PP_CONCAT_DO(cuda, name)

namespace alpaka::cuda::detail
{
    //! CUDA driver API error checking with log and exception, ignoring specific error values
    ALPAKA_FN_HOST inline auto cudaDrvCheck(CUresult const& error, char const* desc, char const* file, int const& line)
        -> void
    {
        if(error == CUDA_SUCCESS)
            return;

        char const* cu_err_name = nullptr;
        char const* cu_err_string = nullptr;
        CUresult cu_result_name = cuGetErrorName(error, &cu_err_name);
        CUresult cu_result_string = cuGetErrorString(error, &cu_err_string);
        std::string sError = std::string(file) + "(" + std::to_string(line) + ") " + std::string(desc) + " : '";
        if(cu_result_name == CUDA_SUCCESS && cu_result_string == CUDA_SUCCESS)
        {
            sError += std::string(cu_err_name) + "': '" + std::string(cu_err_string) + "'!";
        }
        else
        {
            // cuGetError*() failed, so append corresponding error message
            if(cu_result_name == CUDA_ERROR_INVALID_VALUE)
            {
                sError += " cuGetErrorName: 'Invalid Value'!";
            }
            if(cu_result_string == CUDA_ERROR_INVALID_VALUE)
            {
                sError += " cuGetErrorString: 'Invalid Value'!";
            }
        }
#    if ALPAKA_DEBUG >= ALPAKA_DEBUG_MINIMAL
        std::cerr << sError << std::endl;
#    endif
        ALPAKA_DEBUG_BREAK;
        throw std::runtime_error(sError);
    }
} // namespace alpaka::cuda::detail

//! CUDA driver error checking with log and exception.
#    define ALPAKA_CUDA_DRV_CHECK(cmd) ::alpaka::cuda::detail::cudaDrvCheck(cmd, #    cmd, __FILE__, __LINE__)


// CUDA vector_types.h trait specializations.
namespace alpaka
{
    //! The CUDA specifics.
    namespace cuda::traits
    {
        namespace detail
        {
            using BuiltinTypes1 = std::tuple<
                char1,
                double1,
                float1,
                int1,
                long1,
                longlong1,
                short1,
                uchar1,
                uint1,
                ulong1,
                ulonglong1,
                ushort1>;
            using BuiltinTypes2 = std::tuple<
                char2,
                double2,
                float2,
                int2,
                long2,
                longlong2,
                short2,
                uchar2,
                uint2,
                ulong2,
                ulonglong2,
                ushort2>;
            using BuiltinTypes3 = std::tuple<
                char3,
                double3,
                float3,
                int3,
                long3,
                longlong3,
                short3,
                uchar3,
                uint3,
                ulong3,
                ulonglong3,
                ushort3
            // CUDA built-in variables have special types in clang native CUDA compilation
// defined in cuda_builtin_vars.h
#    if BOOST_COMP_CLANG_CUDA
                ,
                __cuda_builtin_threadIdx_t,
                __cuda_builtin_blockIdx_t,
                __cuda_builtin_blockDim_t,
                __cuda_builtin_gridDim_t
#    endif
                >;
            using BuiltinTypes4 = std::tuple<
                char4,
                double4,
                float4,
                int4,
                long4,
                longlong4,
                short4,
                uchar4,
                uint4,
                ulong4,
                ulonglong4,
                ushort4>;
            using BuiltinTypes = meta::Concatenate<BuiltinTypes1, BuiltinTypes2, BuiltinTypes3, BuiltinTypes4>;
        } // namespace detail

        //! The CUDA vectors 1D dimension get trait specialization.
        template<typename T>
        inline constexpr auto isCudaBuiltInType = meta::Contains<detail::BuiltinTypes, T>::value;
    } // namespace cuda::traits
    namespace traits
    {
        //! The CUDA vectors 1D dimension get trait specialization.
        template<typename T>
        struct DimType<T, std::enable_if_t<meta::Contains<cuda::traits::detail::BuiltinTypes1, T>::value>>
        {
            using type = DimInt<1u>;
        };
        //! The CUDA vectors 2D dimension get trait specialization.
        template<typename T>
        struct DimType<T, std::enable_if_t<meta::Contains<cuda::traits::detail::BuiltinTypes2, T>::value>>
        {
            using type = DimInt<2u>;
        };
        //! The CUDA vectors 3D dimension get trait specialization.
        template<typename T>
        struct DimType<T, std::enable_if_t<meta::Contains<cuda::traits::detail::BuiltinTypes3, T>::value>>
        {
            using type = DimInt<3u>;
        };
        //! The CUDA vectors 4D dimension get trait specialization.
        template<typename T>
        struct DimType<T, std::enable_if_t<meta::Contains<cuda::traits::detail::BuiltinTypes4, T>::value>>
        {
            using type = DimInt<4u>;
        };

        //! The CUDA vectors elem type trait specialization.
        template<typename T>
        struct ElemType<T, std::enable_if_t<cuda::traits::isCudaBuiltInType<T>>>
        {
            using type = decltype(std::declval<T>().x);
        };

        //! The CUDA vectors extent get trait specialization.
        template<typename TExtent>
        struct GetExtent<
            DimInt<Dim<TExtent>::value - 1u>,
            TExtent,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TExtent> && (Dim<TExtent>::value >= 1)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const& extent)
            {
                return extent.x;
            }
        };
        //! The CUDA vectors extent get trait specialization.
        template<typename TExtent>
        struct GetExtent<
            DimInt<Dim<TExtent>::value - 2u>,
            TExtent,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TExtent> && (Dim<TExtent>::value >= 2)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const& extent)
            {
                return extent.y;
            }
        };
        //! The CUDA vectors extent get trait specialization.
        template<typename TExtent>
        struct GetExtent<
            DimInt<Dim<TExtent>::value - 3u>,
            TExtent,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TExtent> && (Dim<TExtent>::value >= 3)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const& extent)
            {
                return extent.z;
            }
        };
        //! The CUDA vectors extent get trait specialization.
        template<typename TExtent>
        struct GetExtent<
            DimInt<Dim<TExtent>::value - 4u>,
            TExtent,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TExtent> && (Dim<TExtent>::value >= 4)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto getExtent(TExtent const& extent)
            {
                return extent.w;
            }
        };
        //! The CUDA vectors extent set trait specialization.
        template<typename TExtent, typename TExtentVal>
        struct SetExtent<
            DimInt<Dim<TExtent>::value - 1u>,
            TExtent,
            TExtentVal,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TExtent> && (Dim<TExtent>::value >= 1)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto setExtent(TExtent const& extent, TExtentVal const& extentVal) -> void
            {
                extent.x = extentVal;
            }
        };
        //! The CUDA vectors extent set trait specialization.
        template<typename TExtent, typename TExtentVal>
        struct SetExtent<
            DimInt<Dim<TExtent>::value - 2u>,
            TExtent,
            TExtentVal,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TExtent> && (Dim<TExtent>::value >= 2)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto setExtent(TExtent const& extent, TExtentVal const& extentVal) -> void
            {
                extent.y = extentVal;
            }
        };
        //! The CUDA vectors extent set trait specialization.
        template<typename TExtent, typename TExtentVal>
        struct SetExtent<
            DimInt<Dim<TExtent>::value - 3u>,
            TExtent,
            TExtentVal,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TExtent> && (Dim<TExtent>::value >= 3)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto setExtent(TExtent const& extent, TExtentVal const& extentVal) -> void
            {
                extent.z = extentVal;
            }
        };
        //! The CUDA vectors extent set trait specialization.
        template<typename TExtent, typename TExtentVal>
        struct SetExtent<
            DimInt<Dim<TExtent>::value - 4u>,
            TExtent,
            TExtentVal,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TExtent> && (Dim<TExtent>::value >= 4)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto setExtent(TExtent const& extent, TExtentVal const& extentVal) -> void
            {
                extent.w = extentVal;
            }
        };

        //! The CUDA vectors offset get trait specialization.
        template<typename TOffsets>
        struct GetOffset<
            DimInt<Dim<TOffsets>::value - 1u>,
            TOffsets,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 1)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const& offsets)
            {
                return offsets.x;
            }
        };
        //! The CUDA vectors offset get trait specialization.
        template<typename TOffsets>
        struct GetOffset<
            DimInt<Dim<TOffsets>::value - 2u>,
            TOffsets,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 2)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const& offsets)
            {
                return offsets.y;
            }
        };
        //! The CUDA vectors offset get trait specialization.
        template<typename TOffsets>
        struct GetOffset<
            DimInt<Dim<TOffsets>::value - 3u>,
            TOffsets,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 3)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const& offsets)
            {
                return offsets.z;
            }
        };
        //! The CUDA vectors offset get trait specialization.
        template<typename TOffsets>
        struct GetOffset<
            DimInt<Dim<TOffsets>::value - 4u>,
            TOffsets,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TOffsets> && (Dim<TOffsets>::value >= 4)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto getOffset(TOffsets const& offsets)
            {
                return offsets.w;
            }
        };
        //! The CUDA vectors offset set trait specialization.
        template<typename TOffsets, typename TOffset>
        struct SetOffset<
            DimInt<Dim<TOffsets>::value - 1u>,
            TOffsets,
            TOffset,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TOffset> && (Dim<TOffsets>::value >= 1)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
            {
                offsets.x = offset;
            }
        };
        //! The CUDA vectors offset set trait specialization.
        template<typename TOffsets, typename TOffset>
        struct SetOffset<
            DimInt<Dim<TOffsets>::value - 2u>,
            TOffsets,
            TOffset,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TOffset> && (Dim<TOffsets>::value >= 2)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
            {
                offsets.y = offset;
            }
        };
        //! The CUDA vectors offset set trait specialization.
        template<typename TOffsets, typename TOffset>
        struct SetOffset<
            DimInt<Dim<TOffsets>::value - 3u>,
            TOffsets,
            TOffset,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TOffset> && (Dim<TOffsets>::value >= 3)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
            {
                offsets.z = offset;
            }
        };
        //! The CUDA vectors offset set trait specialization.
        template<typename TOffsets, typename TOffset>
        struct SetOffset<
            DimInt<Dim<TOffsets>::value - 4u>,
            TOffsets,
            TOffset,
            std::enable_if_t<cuda::traits::isCudaBuiltInType<TOffset> && (Dim<TOffsets>::value >= 4)>>
        {
            ALPAKA_NO_HOST_ACC_WARNING
            ALPAKA_FN_HOST_ACC static auto setOffset(TOffsets const& offsets, TOffset const& offset) -> void
            {
                offsets.w = offset;
            }
        };

        //! The CUDA vectors idx type trait specialization.
        template<typename TIdx>
        struct IdxType<TIdx, std::enable_if_t<cuda::traits::isCudaBuiltInType<TIdx>>>
        {
            using type = std::size_t;
        };
    } // namespace traits
} // namespace alpaka

#    include <alpaka/core/UniformCudaHip.hpp>

#endif
